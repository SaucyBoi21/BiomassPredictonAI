{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 150\n",
    "LOSS_FUNCTION = nn.MSELoss()\n",
    "LOWEST_LOSS = 150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loss = {}\n",
    "y_loss['train'] = []\n",
    "y_loss['test'] = []\n",
    "\n",
    "X_epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_workflow(input_data, target_value):\n",
    "    X = input_data.drop([target_value], axis=1)\n",
    "    y = input_data[target_value]\n",
    "\n",
    "    #print(X)\n",
    "    #print(y)\n",
    "\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    X = torch.from_numpy(X)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    data = TensorDataset(X, y)\n",
    "\n",
    "    train_ds, test_ds = train_test_split(data, test_size=0.2, random_state=25)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f\"training data: {train_dl}\\n test data: {test_dl}\")\n",
    "    return train_dl, test_dl\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LFW_g</th>\n",
       "      <th>LDW_g</th>\n",
       "      <th>LA_mm2</th>\n",
       "      <th>length_mm</th>\n",
       "      <th>width_mm</th>\n",
       "      <th>height_mm</th>\n",
       "      <th>plant_area</th>\n",
       "      <th>plant_convex_hull_area</th>\n",
       "      <th>plant_solidity</th>\n",
       "      <th>plant_perimeter</th>\n",
       "      <th>plant_width</th>\n",
       "      <th>plant_height</th>\n",
       "      <th>plant_longest_path</th>\n",
       "      <th>plant_convex_hull_vertices</th>\n",
       "      <th>plant_ellipse_major_axis</th>\n",
       "      <th>plant_ellipse_minor_axis</th>\n",
       "      <th>plant_ellipse_angle</th>\n",
       "      <th>plant_ellipse_eccentricity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.30</td>\n",
       "      <td>0.078</td>\n",
       "      <td>31.95</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1211</td>\n",
       "      <td>1456.0</td>\n",
       "      <td>0.831731</td>\n",
       "      <td>189.923880</td>\n",
       "      <td>49</td>\n",
       "      <td>42</td>\n",
       "      <td>346</td>\n",
       "      <td>16</td>\n",
       "      <td>45.684494</td>\n",
       "      <td>40.965988</td>\n",
       "      <td>92.878510</td>\n",
       "      <td>0.442608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.10</td>\n",
       "      <td>0.148</td>\n",
       "      <td>44.10</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1412</td>\n",
       "      <td>1556.5</td>\n",
       "      <td>0.907164</td>\n",
       "      <td>181.338094</td>\n",
       "      <td>56</td>\n",
       "      <td>41</td>\n",
       "      <td>370</td>\n",
       "      <td>20</td>\n",
       "      <td>53.368065</td>\n",
       "      <td>37.484673</td>\n",
       "      <td>96.255425</td>\n",
       "      <td>0.711802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.36</td>\n",
       "      <td>0.196</td>\n",
       "      <td>67.61</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.5</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1303</td>\n",
       "      <td>1766.5</td>\n",
       "      <td>0.737617</td>\n",
       "      <td>247.865005</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>333</td>\n",
       "      <td>19</td>\n",
       "      <td>46.356819</td>\n",
       "      <td>43.059174</td>\n",
       "      <td>2.102176</td>\n",
       "      <td>0.370421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.07</td>\n",
       "      <td>0.184</td>\n",
       "      <td>66.98</td>\n",
       "      <td>5.8</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1601</td>\n",
       "      <td>1787.0</td>\n",
       "      <td>0.895915</td>\n",
       "      <td>203.480229</td>\n",
       "      <td>44</td>\n",
       "      <td>59</td>\n",
       "      <td>395</td>\n",
       "      <td>21</td>\n",
       "      <td>55.633369</td>\n",
       "      <td>38.548523</td>\n",
       "      <td>15.127802</td>\n",
       "      <td>0.721031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.17</td>\n",
       "      <td>0.187</td>\n",
       "      <td>68.74</td>\n",
       "      <td>7.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1919</td>\n",
       "      <td>2372.0</td>\n",
       "      <td>0.809022</td>\n",
       "      <td>263.421354</td>\n",
       "      <td>62</td>\n",
       "      <td>58</td>\n",
       "      <td>454</td>\n",
       "      <td>19</td>\n",
       "      <td>60.012882</td>\n",
       "      <td>48.875011</td>\n",
       "      <td>55.774792</td>\n",
       "      <td>0.580292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>57.96</td>\n",
       "      <td>3.010</td>\n",
       "      <td>726.46</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19084</td>\n",
       "      <td>22974.0</td>\n",
       "      <td>0.830678</td>\n",
       "      <td>767.938160</td>\n",
       "      <td>193</td>\n",
       "      <td>167</td>\n",
       "      <td>1367</td>\n",
       "      <td>33</td>\n",
       "      <td>190.580887</td>\n",
       "      <td>137.733093</td>\n",
       "      <td>62.138748</td>\n",
       "      <td>0.691160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>81.46</td>\n",
       "      <td>3.880</td>\n",
       "      <td>1001.79</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>14.8</td>\n",
       "      <td>18373</td>\n",
       "      <td>21556.5</td>\n",
       "      <td>0.852318</td>\n",
       "      <td>661.612260</td>\n",
       "      <td>164</td>\n",
       "      <td>189</td>\n",
       "      <td>1317</td>\n",
       "      <td>29</td>\n",
       "      <td>176.040924</td>\n",
       "      <td>141.141724</td>\n",
       "      <td>22.056726</td>\n",
       "      <td>0.597653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>140.84</td>\n",
       "      <td>6.380</td>\n",
       "      <td>1707.14</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>16.9</td>\n",
       "      <td>23374</td>\n",
       "      <td>26050.0</td>\n",
       "      <td>0.897274</td>\n",
       "      <td>683.754395</td>\n",
       "      <td>186</td>\n",
       "      <td>194</td>\n",
       "      <td>1427</td>\n",
       "      <td>31</td>\n",
       "      <td>191.379974</td>\n",
       "      <td>164.496170</td>\n",
       "      <td>175.959305</td>\n",
       "      <td>0.511091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>108.17</td>\n",
       "      <td>5.250</td>\n",
       "      <td>1364.18</td>\n",
       "      <td>31.4</td>\n",
       "      <td>20.5</td>\n",
       "      <td>16.6</td>\n",
       "      <td>23457</td>\n",
       "      <td>25678.5</td>\n",
       "      <td>0.913488</td>\n",
       "      <td>671.754395</td>\n",
       "      <td>198</td>\n",
       "      <td>179</td>\n",
       "      <td>1364</td>\n",
       "      <td>31</td>\n",
       "      <td>194.815384</td>\n",
       "      <td>159.870819</td>\n",
       "      <td>122.656914</td>\n",
       "      <td>0.571464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>64.40</td>\n",
       "      <td>3.350</td>\n",
       "      <td>812.24</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>18533</td>\n",
       "      <td>22886.5</td>\n",
       "      <td>0.809779</td>\n",
       "      <td>821.595015</td>\n",
       "      <td>189</td>\n",
       "      <td>168</td>\n",
       "      <td>1376</td>\n",
       "      <td>22</td>\n",
       "      <td>193.579941</td>\n",
       "      <td>133.306320</td>\n",
       "      <td>123.732269</td>\n",
       "      <td>0.725106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LFW_g  LDW_g   LA_mm2  length_mm  width_mm  height_mm  plant_area  \\\n",
       "0      1.30  0.078    31.95        4.3       5.2        5.8        1211   \n",
       "1      2.10  0.148    44.10        5.3       5.7        5.5        1412   \n",
       "2      3.36  0.196    67.61        7.3       6.5        8.9        1303   \n",
       "3      3.07  0.184    66.98        5.8       7.6        7.5        1601   \n",
       "4      3.17  0.187    68.74        7.2       8.0        7.1        1919   \n",
       "..      ...    ...      ...        ...       ...        ...         ...   \n",
       "160   57.96  3.010   726.46       18.5      18.5       13.0       19084   \n",
       "161   81.46  3.880  1001.79       21.0      21.5       14.8       18373   \n",
       "162  140.84  6.380  1707.14       23.0      22.5       16.9       23374   \n",
       "163  108.17  5.250  1364.18       31.4      20.5       16.6       23457   \n",
       "164   64.40  3.350   812.24       21.0      18.0       14.7       18533   \n",
       "\n",
       "     plant_convex_hull_area  plant_solidity  plant_perimeter  plant_width  \\\n",
       "0                    1456.0        0.831731       189.923880           49   \n",
       "1                    1556.5        0.907164       181.338094           56   \n",
       "2                    1766.5        0.737617       247.865005           49   \n",
       "3                    1787.0        0.895915       203.480229           44   \n",
       "4                    2372.0        0.809022       263.421354           62   \n",
       "..                      ...             ...              ...          ...   \n",
       "160                 22974.0        0.830678       767.938160          193   \n",
       "161                 21556.5        0.852318       661.612260          164   \n",
       "162                 26050.0        0.897274       683.754395          186   \n",
       "163                 25678.5        0.913488       671.754395          198   \n",
       "164                 22886.5        0.809779       821.595015          189   \n",
       "\n",
       "     plant_height  plant_longest_path  plant_convex_hull_vertices  \\\n",
       "0              42                 346                          16   \n",
       "1              41                 370                          20   \n",
       "2              54                 333                          19   \n",
       "3              59                 395                          21   \n",
       "4              58                 454                          19   \n",
       "..            ...                 ...                         ...   \n",
       "160           167                1367                          33   \n",
       "161           189                1317                          29   \n",
       "162           194                1427                          31   \n",
       "163           179                1364                          31   \n",
       "164           168                1376                          22   \n",
       "\n",
       "     plant_ellipse_major_axis  plant_ellipse_minor_axis  plant_ellipse_angle  \\\n",
       "0                   45.684494                 40.965988            92.878510   \n",
       "1                   53.368065                 37.484673            96.255425   \n",
       "2                   46.356819                 43.059174             2.102176   \n",
       "3                   55.633369                 38.548523            15.127802   \n",
       "4                   60.012882                 48.875011            55.774792   \n",
       "..                        ...                       ...                  ...   \n",
       "160                190.580887                137.733093            62.138748   \n",
       "161                176.040924                141.141724            22.056726   \n",
       "162                191.379974                164.496170           175.959305   \n",
       "163                194.815384                159.870819           122.656914   \n",
       "164                193.579941                133.306320           123.732269   \n",
       "\n",
       "     plant_ellipse_eccentricity  \n",
       "0                      0.442608  \n",
       "1                      0.711802  \n",
       "2                      0.370421  \n",
       "3                      0.721031  \n",
       "4                      0.580292  \n",
       "..                          ...  \n",
       "160                    0.691160  \n",
       "161                    0.597653  \n",
       "162                    0.511091  \n",
       "163                    0.571464  \n",
       "164                    0.725106  \n",
       "\n",
       "[165 rows x 18 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_filepath = \"./final_harvest_data.csv\"\n",
    "\n",
    "data = pd.read_csv(csv_filepath)\n",
    "DATA = data.drop(['date', 'index', 'plant_id', 'tray_id', 'row', 'column'], axis=1)\n",
    "\n",
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: <torch.utils.data.dataloader.DataLoader object at 0x7f7137cf7b20>\n",
      " test data: <torch.utils.data.dataloader.DataLoader object at 0x7f7137cf7fd0>\n"
     ]
    }
   ],
   "source": [
    "LFW_train, LFW_test = data_workflow(DATA, 'LFW_g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(17, 16),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            \n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits.double()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=16, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = DeepNeuralNetwork()\n",
    "model = model.double()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataset, model, loss_function, optimizer, filler):\n",
    "    model.train()\n",
    "    for (X, y) in dataset:\n",
    "        #X, y = X.to('cuda'), y.to('cuda')\n",
    "        y = y.view(-1,1) \n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(X)\n",
    "        loss = loss_function(prediction, y)\n",
    "\n",
    "        y_loss['train'].append(loss.item())\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        global LOWEST_LOSS\n",
    "        \n",
    "        print(f\"Loss: {loss}\")\n",
    "        if (loss < LOWEST_LOSS):\n",
    "            LOWEST_LOSS = loss\n",
    "        #print(f\"X: {X} \\n Y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataset, model, loss_function, optimizer, filler):\n",
    "    \n",
    "    for (X, y) in dataset:\n",
    "        #X, y = X.to('cuda'), y.to('cuda')\n",
    "        y = y.view(-1,1) \n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(X)\n",
    "        loss = loss_function(prediction, y)\n",
    "\n",
    "        y_loss['test'].append(loss.item())\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        global LOWEST_LOSS\n",
    "        \n",
    "        print(f\"Test Loss: {loss}\")\n",
    "        \n",
    "        #print(f\"X: {X} \\n Y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------------\n",
      "Loss: 10940267.715043351\n",
      "Loss: 23576113.397908762\n",
      "Loss: 10285104.421093319\n",
      "Loss: 16526880.075345013\n",
      "Loss: 13965866.45978802\n",
      "Test Loss: 9263583.81518221\n",
      "Test Loss: 12079547.61192582\n",
      "Epoch 2\n",
      "------------------------------------\n",
      "Loss: 7320812.8539527\n",
      "Loss: 5397397.455717239\n",
      "Loss: 12905049.088615363\n",
      "Loss: 6481196.117680663\n",
      "Loss: 3381311.797130401\n",
      "Test Loss: 4327644.78669898\n",
      "Test Loss: 5393463.865359005\n",
      "Epoch 3\n",
      "------------------------------------\n",
      "Loss: 5315352.764860764\n",
      "Loss: 3549784.883944831\n",
      "Loss: 3658448.012819658\n",
      "Loss: 1618307.3880160074\n",
      "Loss: 119451.60719737754\n",
      "Test Loss: 1493827.2302460554\n",
      "Test Loss: 1516461.3191315003\n",
      "Epoch 4\n",
      "------------------------------------\n",
      "Loss: 1984234.3669089426\n",
      "Loss: 1138353.927736024\n",
      "Loss: 486146.38210383436\n",
      "Loss: 400725.1051939209\n",
      "Loss: 694248.7257590373\n",
      "Test Loss: 298014.73991308035\n",
      "Test Loss: 87.37722417822656\n",
      "Epoch 5\n",
      "------------------------------------\n",
      "Loss: 208379.11079074247\n",
      "Loss: 110597.86883402415\n",
      "Loss: 57090.318156636225\n",
      "Loss: 35047.23866363551\n",
      "Loss: 19200.821576007416\n",
      "Test Loss: 3941.103433015728\n",
      "Test Loss: 412.3020380260095\n",
      "Epoch 6\n",
      "------------------------------------\n",
      "Loss: 5667.822024089732\n",
      "Loss: 8310.706009206744\n",
      "Loss: 16793.445035390156\n",
      "Loss: 38325.59356877751\n",
      "Loss: 32612.77880971386\n",
      "Test Loss: 46967.670807158735\n",
      "Test Loss: 3677.2508513441294\n",
      "Epoch 7\n",
      "------------------------------------\n",
      "Loss: 88166.93395871058\n",
      "Loss: 84605.8675013831\n",
      "Loss: 88860.44776990844\n",
      "Loss: 112466.07214845376\n",
      "Loss: 57249.53393450794\n",
      "Test Loss: 89934.84422684043\n",
      "Test Loss: 11706.156713701404\n",
      "Epoch 8\n",
      "------------------------------------\n",
      "Loss: 113127.36741902774\n",
      "Loss: 143657.58257141785\n",
      "Loss: 90403.87966290693\n",
      "Loss: 68037.39101607358\n",
      "Loss: 49592.75682709598\n",
      "Test Loss: 59204.78626479344\n",
      "Test Loss: 250256.25463281284\n",
      "Epoch 9\n",
      "------------------------------------\n",
      "Loss: 78234.11630197449\n",
      "Loss: 42780.76993286175\n",
      "Loss: 37420.44771207254\n",
      "Loss: 44193.75802300178\n",
      "Loss: 12513.125520003712\n",
      "Test Loss: 17587.59957088859\n",
      "Test Loss: 20462.068634533232\n",
      "Epoch 10\n",
      "------------------------------------\n",
      "Loss: 9924.223631789828\n",
      "Loss: 10993.942584202021\n",
      "Loss: 7152.770555396704\n",
      "Loss: 5170.595063573654\n",
      "Loss: 3709.9261496653335\n",
      "Test Loss: 1374.9778728192318\n",
      "Test Loss: 309.46053210377056\n",
      "Epoch 11\n",
      "------------------------------------\n",
      "Loss: 2181.6332379178857\n",
      "Loss: 1481.244232371955\n",
      "Loss: 6866.191055934159\n",
      "Loss: 2602.433928792046\n",
      "Loss: 629.2994971120422\n",
      "Test Loss: 2914.7593444543163\n",
      "Test Loss: 1351.6661046179402\n",
      "Epoch 12\n",
      "------------------------------------\n",
      "Loss: 3750.7132974924743\n",
      "Loss: 9237.945628960722\n",
      "Loss: 4564.343722908671\n",
      "Loss: 9902.814465277203\n",
      "Loss: 3987.8337418240367\n",
      "Test Loss: 4913.575488153791\n",
      "Test Loss: 18.54333379999282\n",
      "Epoch 13\n",
      "------------------------------------\n",
      "Loss: 11044.24698937076\n",
      "Loss: 4164.005862272573\n",
      "Loss: 3652.2421254468045\n",
      "Loss: 5049.033479673348\n",
      "Loss: 29653.07819695481\n",
      "Test Loss: 3121.5555138830778\n",
      "Test Loss: 3144.413773149126\n",
      "Epoch 14\n",
      "------------------------------------\n",
      "Loss: 3122.164411578289\n",
      "Loss: 5461.139332001032\n",
      "Loss: 3641.3965310831218\n",
      "Loss: 2140.825347417445\n",
      "Loss: 951.2094424857648\n",
      "Test Loss: 1070.10737426057\n",
      "Test Loss: 24.740414647134244\n",
      "Epoch 15\n",
      "------------------------------------\n",
      "Loss: 1955.8605316320907\n",
      "Loss: 3180.4717164786553\n",
      "Loss: 2409.192913525278\n",
      "Loss: 2594.956360459004\n",
      "Loss: 2302.395881636103\n",
      "Test Loss: 1104.8531403340196\n",
      "Test Loss: 173.30547379751295\n",
      "Epoch 16\n",
      "------------------------------------\n",
      "Loss: 3238.703333359588\n",
      "Loss: 2641.7434866343547\n",
      "Loss: 2639.712057578403\n",
      "Loss: 2397.4043432794624\n",
      "Loss: 5993.054327637811\n",
      "Test Loss: 1395.8559198989706\n",
      "Test Loss: 6.886458495358152\n",
      "Epoch 17\n",
      "------------------------------------\n",
      "Loss: 3637.2240744834207\n",
      "Loss: 2420.257505878285\n",
      "Loss: 3254.8776715697286\n",
      "Loss: 2678.833176001865\n",
      "Loss: 5709.997428123669\n",
      "Test Loss: 1400.908258011335\n",
      "Test Loss: 605.8092540024219\n",
      "Epoch 18\n",
      "------------------------------------\n",
      "Loss: 3383.922368156498\n",
      "Loss: 3230.416071977833\n",
      "Loss: 2901.6218248122923\n",
      "Loss: 1964.5786880405003\n",
      "Loss: 416.4794711263504\n",
      "Test Loss: 959.8601496331925\n",
      "Test Loss: 5131.937351146207\n",
      "Epoch 19\n",
      "------------------------------------\n",
      "Loss: 2934.8420977921874\n",
      "Loss: 2610.279231044253\n",
      "Loss: 3274.2255974639857\n",
      "Loss: 1543.6906887963132\n",
      "Loss: 2550.791436097099\n",
      "Test Loss: 945.4938900492699\n",
      "Test Loss: 2767.8007257705394\n",
      "Epoch 20\n",
      "------------------------------------\n",
      "Loss: 1659.3236733453787\n",
      "Loss: 3066.323620801489\n",
      "Loss: 2405.8898699320207\n",
      "Loss: 3129.868465245166\n",
      "Loss: 1115.3879424627055\n",
      "Test Loss: 971.5797706754503\n",
      "Test Loss: 30.150152795281013\n",
      "Epoch 21\n",
      "------------------------------------\n",
      "Loss: 1946.4063867173165\n",
      "Loss: 2154.672849103037\n",
      "Loss: 2252.9816611567016\n",
      "Loss: 3753.06448048486\n",
      "Loss: 1466.9659129460422\n",
      "Test Loss: 958.1322779070442\n",
      "Test Loss: 112.30358356733349\n",
      "Epoch 22\n",
      "------------------------------------\n",
      "Loss: 2907.613261450871\n",
      "Loss: 1418.997417415753\n",
      "Loss: 2716.353592633758\n",
      "Loss: 3117.054043516491\n",
      "Loss: 617.8517668522594\n",
      "Test Loss: 946.8497222890867\n",
      "Test Loss: 537.4007449166866\n",
      "Epoch 23\n",
      "------------------------------------\n",
      "Loss: 1688.7756325069645\n",
      "Loss: 2561.7728916900387\n",
      "Loss: 3275.805721632413\n",
      "Loss: 2185.413065731646\n",
      "Loss: 3719.8966542189023\n",
      "Test Loss: 936.6276471217319\n",
      "Test Loss: 688.78230156727\n",
      "Epoch 24\n",
      "------------------------------------\n",
      "Loss: 2822.416712915598\n",
      "Loss: 2164.4172708268948\n",
      "Loss: 3232.805426632221\n",
      "Loss: 1484.1599232405233\n",
      "Loss: 3319.749252695384\n",
      "Test Loss: 944.3058503387543\n",
      "Test Loss: 295.31367312748927\n",
      "Epoch 25\n",
      "------------------------------------\n",
      "Loss: 4146.88698696292\n",
      "Loss: 1251.6040632026804\n",
      "Loss: 1435.78109216389\n",
      "Loss: 3128.52603507664\n",
      "Loss: 1297.9990783461217\n",
      "Test Loss: 946.7534500017902\n",
      "Test Loss: 191.88055501894152\n",
      "Epoch 26\n",
      "------------------------------------\n",
      "Loss: 1371.295802170256\n",
      "Loss: 2873.5562961575015\n",
      "Loss: 2557.785600372944\n",
      "Loss: 3010.9953844044708\n",
      "Loss: 1766.9055556050375\n",
      "Test Loss: 931.6286146527345\n",
      "Test Loss: 527.0455312892253\n",
      "Epoch 27\n",
      "------------------------------------\n",
      "Loss: 2328.0402695267353\n",
      "Loss: 2522.9800338820105\n",
      "Loss: 1850.6354782552794\n",
      "Loss: 2986.8257757121582\n",
      "Loss: 2984.9764437910308\n",
      "Test Loss: 862.5335461005151\n",
      "Test Loss: 2987.2743238923545\n",
      "Epoch 28\n",
      "------------------------------------\n",
      "Loss: 3832.2087912503853\n",
      "Loss: 1176.7889515042073\n",
      "Loss: 3164.257689419483\n",
      "Loss: 1481.4257994311015\n",
      "Loss: 4174.106017780722\n",
      "Test Loss: 936.0518457460491\n",
      "Test Loss: 328.2700340300518\n",
      "Epoch 29\n",
      "------------------------------------\n",
      "Loss: 3156.5675894117812\n",
      "Loss: 3012.2889637830713\n",
      "Loss: 1802.0400713210004\n",
      "Loss: 1414.5247876351314\n",
      "Loss: 4528.839411243958\n",
      "Test Loss: 942.3062594831497\n",
      "Test Loss: 3.4887851237166108\n",
      "Epoch 30\n",
      "------------------------------------\n",
      "Loss: 3119.747982175063\n",
      "Loss: 2361.6649356556645\n",
      "Loss: 1950.6749266720838\n",
      "Loss: 2492.3032449222906\n",
      "Loss: 437.2397553544463\n",
      "Test Loss: 924.6586972003438\n",
      "Test Loss: 453.9431813769385\n",
      "Epoch 31\n",
      "------------------------------------\n",
      "Loss: 3867.9250255642482\n",
      "Loss: 1836.2485946272386\n",
      "Loss: 1737.9170048038309\n",
      "Loss: 2328.877833581568\n",
      "Loss: 1149.6522374229983\n",
      "Test Loss: 931.1805588080638\n",
      "Test Loss: 170.22568872293644\n",
      "Epoch 32\n",
      "------------------------------------\n",
      "Loss: 1852.9318275130342\n",
      "Loss: 2285.782960233575\n",
      "Loss: 2709.1953499095935\n",
      "Loss: 2046.5700990436928\n",
      "Loss: 8297.313649908974\n",
      "Test Loss: 932.7478547230996\n",
      "Test Loss: 405.6840336371253\n",
      "Epoch 33\n",
      "------------------------------------\n",
      "Loss: 2658.307722199434\n",
      "Loss: 1670.1388185576918\n",
      "Loss: 3291.4610112225546\n",
      "Loss: 1581.8509336836282\n",
      "Loss: 5608.225651959043\n",
      "Test Loss: 909.4695710620413\n",
      "Test Loss: 1085.3855079295413\n",
      "Epoch 34\n",
      "------------------------------------\n",
      "Loss: 2351.014260119171\n",
      "Loss: 2905.3590884854325\n",
      "Loss: 2904.8957234154154\n",
      "Loss: 1364.295350352284\n",
      "Loss: 2769.6617629568336\n",
      "Test Loss: 926.0741941827685\n",
      "Test Loss: 252.8562761423142\n",
      "Epoch 35\n",
      "------------------------------------\n",
      "Loss: 3115.2664487746088\n",
      "Loss: 3125.067517476244\n",
      "Loss: 1111.3219994987198\n",
      "Loss: 2334.941341963866\n",
      "Loss: 1025.58625034577\n",
      "Test Loss: 851.5285860255522\n",
      "Test Loss: 2378.7807366965712\n",
      "Epoch 36\n",
      "------------------------------------\n",
      "Loss: 2474.448657854443\n",
      "Loss: 1252.7842363855934\n",
      "Loss: 3336.8113404608403\n",
      "Loss: 2501.2425987692527\n",
      "Loss: 1429.3755451582667\n",
      "Test Loss: 917.0242705860924\n",
      "Test Loss: 173.25962998759053\n",
      "Epoch 37\n",
      "------------------------------------\n",
      "Loss: 3325.9007631398786\n",
      "Loss: 2728.6548849504316\n",
      "Loss: 1310.2191803239416\n",
      "Loss: 2376.5206221976587\n",
      "Loss: 560.6495673312003\n",
      "Test Loss: 934.7589473456806\n",
      "Test Loss: 120.39436380620343\n",
      "Epoch 38\n",
      "------------------------------------\n",
      "Loss: 1914.7294408557752\n",
      "Loss: 4091.5648135621223\n",
      "Loss: 1384.708919591502\n",
      "Loss: 2107.2873496923435\n",
      "Loss: 2453.7323341755205\n",
      "Test Loss: 912.2451912564695\n",
      "Test Loss: 686.1420588330597\n",
      "Epoch 39\n",
      "------------------------------------\n",
      "Loss: 2293.4012270469375\n",
      "Loss: 1492.6090365711484\n",
      "Loss: 2894.3960048595263\n",
      "Loss: 2931.403282396848\n",
      "Loss: 1517.2959790945229\n",
      "Test Loss: 931.6666338404062\n",
      "Test Loss: 204.10373140671334\n",
      "Epoch 40\n",
      "------------------------------------\n",
      "Loss: 1685.052339709945\n",
      "Loss: 2181.8770638375527\n",
      "Loss: 1868.7594605678325\n",
      "Loss: 2095.7903625650697\n",
      "Loss: 15248.263232616939\n",
      "Test Loss: 892.2990961078693\n",
      "Test Loss: 679.356586905919\n",
      "Epoch 41\n",
      "------------------------------------\n",
      "Loss: 2085.5479645331793\n",
      "Loss: 2550.3830773019113\n",
      "Loss: 2561.4802343519345\n",
      "Loss: 2356.1595727575304\n",
      "Loss: 2508.7619509076358\n",
      "Test Loss: 1130.838060080112\n",
      "Test Loss: 899.214102898638\n",
      "Epoch 42\n",
      "------------------------------------\n",
      "Loss: 2432.081849260172\n",
      "Loss: 2957.064231132608\n",
      "Loss: 2936.591508940356\n",
      "Loss: 2548.675325127691\n",
      "Loss: 1400.9871584140888\n",
      "Test Loss: 1133.8495893491656\n",
      "Test Loss: 265.5641379224396\n",
      "Epoch 43\n",
      "------------------------------------\n",
      "Loss: 2892.5449183741753\n",
      "Loss: 2741.4909956089355\n",
      "Loss: 2021.5872219892444\n",
      "Loss: 2486.4476509261176\n",
      "Loss: 825.1931474118992\n",
      "Test Loss: 916.7036465959975\n",
      "Test Loss: 49.910105526790474\n",
      "Epoch 44\n",
      "------------------------------------\n",
      "Loss: 2116.219260173747\n",
      "Loss: 3338.6635493160074\n",
      "Loss: 1976.0348108171365\n",
      "Loss: 1822.4579934123103\n",
      "Loss: 1254.8267846592596\n",
      "Test Loss: 875.8866406637844\n",
      "Test Loss: 1346.3796689785595\n",
      "Epoch 45\n",
      "------------------------------------\n",
      "Loss: 1356.5471960140974\n",
      "Loss: 2883.168963552422\n",
      "Loss: 3555.9716989852036\n",
      "Loss: 1912.112016375503\n",
      "Loss: 628.3243089104387\n",
      "Test Loss: 982.4776644061172\n",
      "Test Loss: 274.74146209354984\n",
      "Epoch 46\n",
      "------------------------------------\n",
      "Loss: 3745.236983631689\n",
      "Loss: 3211.2684560765415\n",
      "Loss: 1714.1032370743064\n",
      "Loss: 996.8057241428006\n",
      "Loss: 898.241004337227\n",
      "Test Loss: 928.9656084433386\n",
      "Test Loss: 54.41174020132975\n",
      "Epoch 47\n",
      "------------------------------------\n",
      "Loss: 3103.0592701987794\n",
      "Loss: 1856.7275127725331\n",
      "Loss: 1309.5903884029292\n",
      "Loss: 2527.2003143065917\n",
      "Loss: 5362.983457334035\n",
      "Test Loss: 848.9097818819794\n",
      "Test Loss: 1631.1381244273289\n",
      "Epoch 48\n",
      "------------------------------------\n",
      "Loss: 1825.735035801273\n",
      "Loss: 2998.8720602770627\n",
      "Loss: 2312.6733066002053\n",
      "Loss: 1913.175392286083\n",
      "Loss: 2952.6699323367743\n",
      "Test Loss: 888.5589243201712\n",
      "Test Loss: 579.2004580876466\n",
      "Epoch 49\n",
      "------------------------------------\n",
      "Loss: 2813.7370540030392\n",
      "Loss: 2008.283758052874\n",
      "Loss: 2006.6007939596893\n",
      "Loss: 2159.5936989305337\n",
      "Loss: 2529.6608820718225\n",
      "Test Loss: 845.2609801435204\n",
      "Test Loss: 1083.835089269619\n",
      "Epoch 50\n",
      "------------------------------------\n",
      "Loss: 2124.6299971636736\n",
      "Loss: 3139.2650258315643\n",
      "Loss: 2110.729566558797\n",
      "Loss: 1926.1106747516455\n",
      "Loss: 837.7359050829466\n",
      "Test Loss: 944.5608451768984\n",
      "Test Loss: 1736.090170618044\n",
      "Epoch 51\n",
      "------------------------------------\n",
      "Loss: 2151.3556272253236\n",
      "Loss: 2514.847995186275\n",
      "Loss: 3212.745635752657\n",
      "Loss: 1919.0586348715879\n",
      "Loss: 399.74357847595854\n",
      "Test Loss: 940.6675232384285\n",
      "Test Loss: 862.1467303391371\n",
      "Epoch 52\n",
      "------------------------------------\n",
      "Loss: 1787.732023925742\n",
      "Loss: 2025.5894095694857\n",
      "Loss: 2381.892740459887\n",
      "Loss: 2884.2100188626982\n",
      "Loss: 649.7017452941963\n",
      "Test Loss: 875.6476035305668\n",
      "Test Loss: 0.45874013672949254\n",
      "Epoch 53\n",
      "------------------------------------\n",
      "Loss: 2118.5306048181224\n",
      "Loss: 1832.9954523015178\n",
      "Loss: 2880.6501271868133\n",
      "Loss: 2137.072209931257\n",
      "Loss: 1170.1709324921922\n",
      "Test Loss: 818.4735653297362\n",
      "Test Loss: 2196.4038783535448\n",
      "Epoch 54\n",
      "------------------------------------\n",
      "Loss: 2339.506417971879\n",
      "Loss: 1597.824808672608\n",
      "Loss: 3621.3295630192915\n",
      "Loss: 1533.6107532078295\n",
      "Loss: 1385.8791984069721\n",
      "Test Loss: 895.5446290444622\n",
      "Test Loss: 852.9120130173394\n",
      "Epoch 55\n",
      "------------------------------------\n",
      "Loss: 1304.1783738415954\n",
      "Loss: 2922.709153400171\n",
      "Loss: 2432.1910588927512\n",
      "Loss: 2213.2807551945634\n",
      "Loss: 3032.7327456546827\n",
      "Test Loss: 867.360014514574\n",
      "Test Loss: 57.4630906215441\n",
      "Epoch 56\n",
      "------------------------------------\n",
      "Loss: 1438.164948231008\n",
      "Loss: 3139.0417758731674\n",
      "Loss: 3140.5397378421508\n",
      "Loss: 1190.19613573548\n",
      "Loss: 1056.6566907092852\n",
      "Test Loss: 852.5635205299532\n",
      "Test Loss: 476.4793046628232\n",
      "Epoch 57\n",
      "------------------------------------\n",
      "Loss: 2967.13505332819\n",
      "Loss: 2655.2583582912857\n",
      "Loss: 1477.9992294350122\n",
      "Loss: 1757.5064848711131\n",
      "Loss: 1233.9833443819816\n",
      "Test Loss: 870.936660574441\n",
      "Test Loss: 305.3743816679733\n",
      "Epoch 58\n",
      "------------------------------------\n",
      "Loss: 2116.6861094391925\n",
      "Loss: 2988.06895598578\n",
      "Loss: 1572.844807201883\n",
      "Loss: 1497.4358413396815\n",
      "Loss: 6910.353838440304\n",
      "Test Loss: 819.5830821728362\n",
      "Test Loss: 1358.951320228815\n",
      "Epoch 59\n",
      "------------------------------------\n",
      "Loss: 2475.3298477500502\n",
      "Loss: 3052.7727271641616\n",
      "Loss: 2287.0690879009267\n",
      "Loss: 1303.925805906472\n",
      "Loss: 1211.1594318356088\n",
      "Test Loss: 982.1144030414121\n",
      "Test Loss: 509.33660428649205\n",
      "Epoch 60\n",
      "------------------------------------\n",
      "Loss: 1932.6074473724723\n",
      "Loss: 2814.7555131265312\n",
      "Loss: 2300.165925390172\n",
      "Loss: 1965.5424471056967\n",
      "Loss: 4229.781250813201\n",
      "Test Loss: 890.4581924474205\n",
      "Test Loss: 220.48311571243076\n",
      "Epoch 61\n",
      "------------------------------------\n",
      "Loss: 2375.5718980482516\n",
      "Loss: 1806.9146942979175\n",
      "Loss: 2349.4221221726057\n",
      "Loss: 1977.1103205620025\n",
      "Loss: 1750.6225122080693\n",
      "Test Loss: 874.6927622113383\n",
      "Test Loss: 367.74070413786666\n",
      "Epoch 62\n",
      "------------------------------------\n",
      "Loss: 2815.3617321201036\n",
      "Loss: 1497.7672828080053\n",
      "Loss: 2564.7942772192514\n",
      "Loss: 2109.005193786904\n",
      "Loss: 1627.5225112687494\n",
      "Test Loss: 946.6507357577906\n",
      "Test Loss: 281.48296073458874\n",
      "Epoch 63\n",
      "------------------------------------\n",
      "Loss: 1040.5412369292917\n",
      "Loss: 3953.2508916922898\n",
      "Loss: 2065.607960157269\n",
      "Loss: 1663.0585085707999\n",
      "Loss: 3413.9721381216746\n",
      "Test Loss: 861.2152896720534\n",
      "Test Loss: 590.1438695610656\n",
      "Epoch 64\n",
      "------------------------------------\n",
      "Loss: 1024.361697777937\n",
      "Loss: 3030.262811280285\n",
      "Loss: 2500.082292218844\n",
      "Loss: 2185.976198552505\n",
      "Loss: 282.9675345313266\n",
      "Test Loss: 840.9188895640425\n",
      "Test Loss: 391.5159071094317\n",
      "Epoch 65\n",
      "------------------------------------\n",
      "Loss: 1401.3689340459348\n",
      "Loss: 2601.530245704707\n",
      "Loss: 1207.9972091017503\n",
      "Loss: 3463.4655495068628\n",
      "Loss: 270.24136868459647\n",
      "Test Loss: 750.5355422668204\n",
      "Test Loss: 3194.973161622654\n",
      "Epoch 66\n",
      "------------------------------------\n",
      "Loss: 2366.168515447691\n",
      "Loss: 3497.9685777657432\n",
      "Loss: 1679.035645782871\n",
      "Loss: 1042.7785939351634\n",
      "Loss: 1782.2591780741857\n",
      "Test Loss: 860.0991984329842\n",
      "Test Loss: 292.3544535919693\n",
      "Epoch 67\n",
      "------------------------------------\n",
      "Loss: 3274.0455305264686\n",
      "Loss: 2068.304551914039\n",
      "Loss: 1920.2843785739333\n",
      "Loss: 1667.7352833686111\n",
      "Loss: 1047.9971578999935\n",
      "Test Loss: 883.8435119276394\n",
      "Test Loss: 207.7512288658676\n",
      "Epoch 68\n",
      "------------------------------------\n",
      "Loss: 1841.953871794198\n",
      "Loss: 1921.204253689356\n",
      "Loss: 2688.2737731872585\n",
      "Loss: 2010.7681230508658\n",
      "Loss: 1807.8839068696038\n",
      "Test Loss: 790.1378158600858\n",
      "Test Loss: 1149.038882524902\n",
      "Epoch 69\n",
      "------------------------------------\n",
      "Loss: 2332.017965200956\n",
      "Loss: 1863.002777820287\n",
      "Loss: 2442.1042072533146\n",
      "Loss: 1761.41502775872\n",
      "Loss: 1906.877692601908\n",
      "Test Loss: 930.3963700440302\n",
      "Test Loss: 234.49856201966756\n",
      "Epoch 70\n",
      "------------------------------------\n",
      "Loss: 2356.6630409815875\n",
      "Loss: 1259.7211969082134\n",
      "Loss: 1878.3042494738163\n",
      "Loss: 2859.689205520299\n",
      "Loss: 5844.524855773266\n",
      "Test Loss: 872.7122866207167\n",
      "Test Loss: 1575.0022812450266\n",
      "Epoch 71\n",
      "------------------------------------\n",
      "Loss: 1615.4074739747575\n",
      "Loss: 2676.544542754588\n",
      "Loss: 2222.1254875198724\n",
      "Loss: 1864.2464852840478\n",
      "Loss: 2223.466454738202\n",
      "Test Loss: 791.3035476340532\n",
      "Test Loss: 977.0319493008309\n",
      "Epoch 72\n",
      "------------------------------------\n",
      "Loss: 2363.468033355915\n",
      "Loss: 2376.205788689024\n",
      "Loss: 855.7138558808833\n",
      "Loss: 1864.0877625245923\n",
      "Loss: 8213.837650432442\n",
      "Test Loss: 837.1505940164581\n",
      "Test Loss: 275.45772486890877\n",
      "Epoch 73\n",
      "------------------------------------\n",
      "Loss: 1499.571884976648\n",
      "Loss: 1542.0760063830235\n",
      "Loss: 2943.868112223916\n",
      "Loss: 2720.0783173862797\n",
      "Loss: 3062.892391467884\n",
      "Test Loss: 845.5840773101733\n",
      "Test Loss: 1853.836671904989\n",
      "Epoch 74\n",
      "------------------------------------\n",
      "Loss: 2250.2223710572925\n",
      "Loss: 2102.360613370186\n",
      "Loss: 1029.8601647134958\n",
      "Loss: 2871.4297426527755\n",
      "Loss: 731.2260715196414\n",
      "Test Loss: 800.8024445534677\n",
      "Test Loss: 327.7786904797694\n",
      "Epoch 75\n",
      "------------------------------------\n",
      "Loss: 1707.091334824871\n",
      "Loss: 3276.8854716346486\n",
      "Loss: 1211.6182555305452\n",
      "Loss: 2322.8249386817088\n",
      "Loss: 753.8286923907426\n",
      "Test Loss: 979.4598206431995\n",
      "Test Loss: 130.08009320836643\n",
      "Epoch 76\n",
      "------------------------------------\n",
      "Loss: 3745.4948789639448\n",
      "Loss: 1757.8306197016302\n",
      "Loss: 1379.2502558807796\n",
      "Loss: 2059.0892419238435\n",
      "Loss: 292.1664942716584\n",
      "Test Loss: 915.9559247341388\n",
      "Test Loss: 286.84125621754174\n",
      "Epoch 77\n",
      "------------------------------------\n",
      "Loss: 906.3919949773878\n",
      "Loss: 2851.183705463967\n",
      "Loss: 2410.731199614317\n",
      "Loss: 1666.2423062153964\n",
      "Loss: 4071.842081226001\n",
      "Test Loss: 803.3703726551205\n",
      "Test Loss: 260.38901232396023\n",
      "Epoch 78\n",
      "------------------------------------\n",
      "Loss: 2492.272999243438\n",
      "Loss: 1830.4363283207188\n",
      "Loss: 1954.0321166751373\n",
      "Loss: 1961.3214481265445\n",
      "Loss: 478.3009839495277\n",
      "Test Loss: 803.9286880332825\n",
      "Test Loss: 360.63395415174165\n",
      "Epoch 79\n",
      "------------------------------------\n",
      "Loss: 2215.2529314514845\n",
      "Loss: 2126.5897070970586\n",
      "Loss: 2240.4646679050466\n",
      "Loss: 1608.173476115413\n",
      "Loss: 741.618146020349\n",
      "Test Loss: 791.1029556059497\n",
      "Test Loss: 235.82883513079713\n",
      "Epoch 80\n",
      "------------------------------------\n",
      "Loss: 1329.770717002462\n",
      "Loss: 1232.4023052050807\n",
      "Loss: 3166.81299705084\n",
      "Loss: 2428.674939894426\n",
      "Loss: 271.5057879688948\n",
      "Test Loss: 766.2037301674778\n",
      "Test Loss: 1492.2235768166609\n",
      "Epoch 81\n",
      "------------------------------------\n",
      "Loss: 1815.324630792707\n",
      "Loss: 2428.9135225217897\n",
      "Loss: 2002.9808919672569\n",
      "Loss: 1882.517877987908\n",
      "Loss: 376.36912632452504\n",
      "Test Loss: 808.3585438052831\n",
      "Test Loss: 49.22680131826853\n",
      "Epoch 82\n",
      "------------------------------------\n",
      "Loss: 1569.9957497316686\n",
      "Loss: 981.0954055483294\n",
      "Loss: 1980.495352782406\n",
      "Loss: 2758.3345328974524\n",
      "Loss: 6166.698242579712\n",
      "Test Loss: 769.500365808453\n",
      "Test Loss: 910.922020880061\n",
      "Epoch 83\n",
      "------------------------------------\n",
      "Loss: 1769.7018161444098\n",
      "Loss: 1126.919502779617\n",
      "Loss: 1469.7426477571391\n",
      "Loss: 3544.5741244721007\n",
      "Loss: 4097.275060680529\n",
      "Test Loss: 826.2415427966575\n",
      "Test Loss: 438.0320024962015\n",
      "Epoch 84\n",
      "------------------------------------\n",
      "Loss: 1877.5093932758873\n",
      "Loss: 1310.3116198225941\n",
      "Loss: 3600.470229256429\n",
      "Loss: 1155.2056023967905\n",
      "Loss: 1184.6543510798274\n",
      "Test Loss: 587.4001502184569\n",
      "Test Loss: 7341.465826892349\n",
      "Epoch 85\n",
      "------------------------------------\n",
      "Loss: 2308.2394880419088\n",
      "Loss: 1134.2450342996997\n",
      "Loss: 1281.0606064498545\n",
      "Loss: 2820.9608358923792\n",
      "Loss: 818.186475430119\n",
      "Test Loss: 774.180688089969\n",
      "Test Loss: 989.5683489668517\n",
      "Epoch 86\n",
      "------------------------------------\n",
      "Loss: 2065.4031431880353\n",
      "Loss: 2629.398942794043\n",
      "Loss: 1248.959846212062\n",
      "Loss: 2146.871213791086\n",
      "Loss: 4267.6873984513595\n",
      "Test Loss: 888.4402412185261\n",
      "Test Loss: 31.626596573460816\n",
      "Epoch 87\n",
      "------------------------------------\n",
      "Loss: 2005.8804054333407\n",
      "Loss: 1761.3874302869162\n",
      "Loss: 868.1180510666046\n",
      "Loss: 2810.8004995222877\n",
      "Loss: 5012.290430676834\n",
      "Test Loss: 767.0120630222135\n",
      "Test Loss: 354.66874059650866\n",
      "Epoch 88\n",
      "------------------------------------\n",
      "Loss: 2642.74559194329\n",
      "Loss: 1357.2038381803188\n",
      "Loss: 2452.599229887954\n",
      "Loss: 1165.2293177798458\n",
      "Loss: 2618.5043914477837\n",
      "Test Loss: 807.6843413857309\n",
      "Test Loss: 2673.1316806939285\n",
      "Epoch 89\n",
      "------------------------------------\n",
      "Loss: 2863.15457455797\n",
      "Loss: 1963.9574181244302\n",
      "Loss: 1440.814392456832\n",
      "Loss: 1570.9943886721196\n",
      "Loss: 1233.429999644653\n",
      "Test Loss: 738.7159069278657\n",
      "Test Loss: 1090.2677718881193\n",
      "Epoch 90\n",
      "------------------------------------\n",
      "Loss: 1728.0117173368185\n",
      "Loss: 1123.654416045451\n",
      "Loss: 2948.578944078317\n",
      "Loss: 1669.201869587248\n",
      "Loss: 3710.504575687161\n",
      "Test Loss: 764.6984473625156\n",
      "Test Loss: 197.9826201149703\n",
      "Epoch 91\n",
      "------------------------------------\n",
      "Loss: 1776.1084204904814\n",
      "Loss: 1631.4101866457618\n",
      "Loss: 2497.400044948533\n",
      "Loss: 1671.1684122974398\n",
      "Loss: 1499.282834595378\n",
      "Test Loss: 776.3038570919887\n",
      "Test Loss: 337.93917844780043\n",
      "Epoch 92\n",
      "------------------------------------\n",
      "Loss: 1760.0669478726636\n",
      "Loss: 1256.855103106122\n",
      "Loss: 1251.523084250821\n",
      "Loss: 2662.901298602897\n",
      "Loss: 7683.151041413378\n",
      "Test Loss: 785.3606741049713\n",
      "Test Loss: 276.28108010961336\n",
      "Epoch 93\n",
      "------------------------------------\n",
      "Loss: 1264.3750697493579\n",
      "Loss: 2054.8907273496557\n",
      "Loss: 1217.7488678951045\n",
      "Loss: 2851.554813191379\n",
      "Loss: 2186.4905505567676\n",
      "Test Loss: 912.3633793155892\n",
      "Test Loss: 40.057989908839644\n",
      "Epoch 94\n",
      "------------------------------------\n",
      "Loss: 2185.329952461217\n",
      "Loss: 2483.4022136637295\n",
      "Loss: 2148.1638186815608\n",
      "Loss: 2641.5724116035494\n",
      "Loss: 3880.695271205022\n",
      "Test Loss: 1062.0365026822374\n",
      "Test Loss: 156.30011458416402\n",
      "Epoch 95\n",
      "------------------------------------\n",
      "Loss: 1391.1771137296505\n",
      "Loss: 3604.31861836958\n",
      "Loss: 1275.9611305458607\n",
      "Loss: 2115.2032449262497\n",
      "Loss: 765.8149751085928\n",
      "Test Loss: 729.7581513074297\n",
      "Test Loss: 574.6311432436062\n",
      "Epoch 96\n",
      "------------------------------------\n",
      "Loss: 1660.8288072276007\n",
      "Loss: 2168.249965354335\n",
      "Loss: 1946.8700491814668\n",
      "Loss: 2029.0337128764468\n",
      "Loss: 817.0461837780712\n",
      "Test Loss: 978.5361785851419\n",
      "Test Loss: 316.1438517178324\n",
      "Epoch 97\n",
      "------------------------------------\n",
      "Loss: 1450.525973891074\n",
      "Loss: 1877.4637007964361\n",
      "Loss: 3509.4078147952982\n",
      "Loss: 1143.132337479285\n",
      "Loss: 5443.39103009025\n",
      "Test Loss: 837.4741112110632\n",
      "Test Loss: 1990.9158515881063\n",
      "Epoch 98\n",
      "------------------------------------\n",
      "Loss: 1846.4720779414706\n",
      "Loss: 1934.6003692494587\n",
      "Loss: 2375.0847700073223\n",
      "Loss: 1303.7060631112945\n",
      "Loss: 1193.274079235018\n",
      "Test Loss: 901.7419988212135\n",
      "Test Loss: 62.638745344000284\n",
      "Epoch 99\n",
      "------------------------------------\n",
      "Loss: 1820.5863317099418\n",
      "Loss: 2220.9843617718057\n",
      "Loss: 2655.3650912060302\n",
      "Loss: 1770.4013243994214\n",
      "Loss: 2069.4693687332765\n",
      "Test Loss: 790.6727834901561\n",
      "Test Loss: 275.0976182064307\n",
      "Epoch 100\n",
      "------------------------------------\n",
      "Loss: 1740.6728109702233\n",
      "Loss: 1503.8010519241798\n",
      "Loss: 1359.2612973992677\n",
      "Loss: 2835.689532568387\n",
      "Loss: 756.4148168658522\n",
      "Test Loss: 807.9253926845894\n",
      "Test Loss: 322.86320398598326\n",
      "Epoch 101\n",
      "------------------------------------\n",
      "Loss: 1374.6883419340502\n",
      "Loss: 2142.704790826291\n",
      "Loss: 1704.0661107468125\n",
      "Loss: 2412.924837233902\n",
      "Loss: 302.11363793823546\n",
      "Test Loss: 856.0716916856029\n",
      "Test Loss: 9.376690349665498\n",
      "Epoch 102\n",
      "------------------------------------\n",
      "Loss: 2454.1882366045634\n",
      "Loss: 1792.906347990052\n",
      "Loss: 813.9022931876168\n",
      "Loss: 2000.7565987316138\n",
      "Loss: 2980.906336238184\n",
      "Test Loss: 743.4160834461177\n",
      "Test Loss: 76.78475318030604\n",
      "Epoch 103\n",
      "------------------------------------\n",
      "Loss: 2041.236197612383\n",
      "Loss: 1163.8545314474895\n",
      "Loss: 1877.621152524746\n",
      "Loss: 2088.072447559781\n",
      "Loss: 736.1836257600243\n",
      "Test Loss: 723.2543504991762\n",
      "Test Loss: 118.8926917410727\n",
      "Epoch 104\n",
      "------------------------------------\n",
      "Loss: 1196.1471429692642\n",
      "Loss: 1184.0045684637425\n",
      "Loss: 2302.3475929761694\n",
      "Loss: 2459.514563318581\n",
      "Loss: 927.536096530074\n",
      "Test Loss: 721.661040116211\n",
      "Test Loss: 98.80976117184588\n",
      "Epoch 105\n",
      "------------------------------------\n",
      "Loss: 1700.8611265234133\n",
      "Loss: 2052.0431341604385\n",
      "Loss: 1938.2203343542674\n",
      "Loss: 1404.043822882883\n",
      "Loss: 548.7443084881363\n",
      "Test Loss: 725.3753278046077\n",
      "Test Loss: 83.97058769530734\n",
      "Epoch 106\n",
      "------------------------------------\n",
      "Loss: 1348.3573758973864\n",
      "Loss: 1691.3452253871385\n",
      "Loss: 1000.8072527167174\n",
      "Loss: 2468.0590140278146\n",
      "Loss: 5184.220761761524\n",
      "Test Loss: 727.2253126873593\n",
      "Test Loss: 225.29231594308163\n",
      "Epoch 107\n",
      "------------------------------------\n",
      "Loss: 1217.8779309679098\n",
      "Loss: 1634.3095249906532\n",
      "Loss: 1711.7618064762896\n",
      "Loss: 2602.101497960538\n",
      "Loss: 894.7765480247652\n",
      "Test Loss: 677.8707581199031\n",
      "Test Loss: 1768.2259484026108\n",
      "Epoch 108\n",
      "------------------------------------\n",
      "Loss: 1798.427138397269\n",
      "Loss: 1923.3496327465314\n",
      "Loss: 1399.561301832452\n",
      "Loss: 2087.701162025797\n",
      "Loss: 180.6940977123416\n",
      "Test Loss: 660.8521443959589\n",
      "Test Loss: 1804.2382816365616\n",
      "Epoch 109\n",
      "------------------------------------\n",
      "Loss: 2024.393922550108\n",
      "Loss: 1024.4224107757573\n",
      "Loss: 3018.3887323321474\n",
      "Loss: 923.8732095679732\n",
      "Loss: 339.10727563178443\n",
      "Test Loss: 701.9314406499287\n",
      "Test Loss: 294.0465560359366\n",
      "Epoch 110\n",
      "------------------------------------\n",
      "Loss: 2104.9748126066556\n",
      "Loss: 2221.8727228613707\n",
      "Loss: 1199.2651264248736\n",
      "Loss: 1431.1240559470314\n",
      "Loss: 821.2726938334769\n",
      "Test Loss: 702.530786573484\n",
      "Test Loss: 571.7306363117913\n",
      "Epoch 111\n",
      "------------------------------------\n",
      "Loss: 1917.6214615884198\n",
      "Loss: 2110.2147321757993\n",
      "Loss: 688.7617813913386\n",
      "Loss: 1316.7187246988883\n",
      "Loss: 7300.358730103422\n",
      "Test Loss: 663.771470065097\n",
      "Test Loss: 2072.1793752217914\n",
      "Epoch 112\n",
      "------------------------------------\n",
      "Loss: 2335.428971507736\n",
      "Loss: 1633.166118894608\n",
      "Loss: 1751.034486370736\n",
      "Loss: 1296.9575376228595\n",
      "Loss: 844.817525636121\n",
      "Test Loss: 728.2653402179736\n",
      "Test Loss: 0.3824147534381308\n",
      "Epoch 113\n",
      "------------------------------------\n",
      "Loss: 2051.717770352314\n",
      "Loss: 1352.760380673711\n",
      "Loss: 1909.4821607670494\n",
      "Loss: 1575.9239982859235\n",
      "Loss: 2421.037902719605\n",
      "Test Loss: 699.6673353323978\n",
      "Test Loss: 105.56730806471911\n",
      "Epoch 114\n",
      "------------------------------------\n",
      "Loss: 2060.483651894996\n",
      "Loss: 1414.6308072332868\n",
      "Loss: 1682.4404840351865\n",
      "Loss: 1854.386591993606\n",
      "Loss: 670.2507142174275\n",
      "Test Loss: 746.0190688877751\n",
      "Test Loss: 191.6535481630657\n",
      "Epoch 115\n",
      "------------------------------------\n",
      "Loss: 1096.0413408616819\n",
      "Loss: 1931.0508897595696\n",
      "Loss: 2171.526892277584\n",
      "Loss: 1443.597346280425\n",
      "Loss: 3509.356044837177\n",
      "Test Loss: 782.4778281347718\n",
      "Test Loss: 5.271162763869042\n",
      "Epoch 116\n",
      "------------------------------------\n",
      "Loss: 1725.5820242694408\n",
      "Loss: 2218.458978449029\n",
      "Loss: 1364.5812968114572\n",
      "Loss: 1087.0394771177585\n",
      "Loss: 3777.4034976908506\n",
      "Test Loss: 694.8008275929751\n",
      "Test Loss: 193.42411819787432\n",
      "Epoch 117\n",
      "------------------------------------\n",
      "Loss: 1398.132241278243\n",
      "Loss: 1672.8645764756443\n",
      "Loss: 2113.7023522969503\n",
      "Loss: 1350.6224794895652\n",
      "Loss: 2158.172104388716\n",
      "Test Loss: 608.7225744223236\n",
      "Test Loss: 3827.3634190460475\n",
      "Epoch 118\n",
      "------------------------------------\n",
      "Loss: 1645.8294934330556\n",
      "Loss: 1848.1581157248522\n",
      "Loss: 1982.612199046644\n",
      "Loss: 1430.4983003539264\n",
      "Loss: 814.0293526382991\n",
      "Test Loss: 744.260005387467\n",
      "Test Loss: 0.6768197830168329\n",
      "Epoch 119\n",
      "------------------------------------\n",
      "Loss: 1614.606558828879\n",
      "Loss: 1905.9218631566096\n",
      "Loss: 1692.5052348245324\n",
      "Loss: 1707.988095115492\n",
      "Loss: 2908.056094272277\n",
      "Test Loss: 662.8888985435422\n",
      "Test Loss: 1171.1240705382925\n",
      "Epoch 120\n",
      "------------------------------------\n",
      "Loss: 1366.3092904848136\n",
      "Loss: 2250.2346466485824\n",
      "Loss: 1377.0518138926377\n",
      "Loss: 1713.5298681976012\n",
      "Loss: 161.25550662532956\n",
      "Test Loss: 668.0828409904302\n",
      "Test Loss: 1064.6501281368603\n",
      "Epoch 121\n",
      "------------------------------------\n",
      "Loss: 1312.4079180672184\n",
      "Loss: 1777.6817444355897\n",
      "Loss: 2102.7497117478665\n",
      "Loss: 944.1645575927542\n",
      "Loss: 4550.603891506105\n",
      "Test Loss: 525.7940988272566\n",
      "Test Loss: 5780.81780698539\n",
      "Epoch 122\n",
      "------------------------------------\n",
      "Loss: 1790.757088403522\n",
      "Loss: 1308.0497755852284\n",
      "Loss: 1825.109300391758\n",
      "Loss: 1395.0068845682704\n",
      "Loss: 7242.259777233853\n",
      "Test Loss: 846.7288223890146\n",
      "Test Loss: 623.2102881963558\n",
      "Epoch 123\n",
      "------------------------------------\n",
      "Loss: 2359.1576333486823\n",
      "Loss: 2834.3685606324693\n",
      "Loss: 1504.220037716816\n",
      "Loss: 1218.151107907416\n",
      "Loss: 1334.9617437238685\n",
      "Test Loss: 724.7971515400675\n",
      "Test Loss: 76.21334446162611\n",
      "Epoch 124\n",
      "------------------------------------\n",
      "Loss: 1462.2305580911154\n",
      "Loss: 2543.620065390133\n",
      "Loss: 1260.1970462855784\n",
      "Loss: 1299.8093718821558\n",
      "Loss: 246.51087591550072\n",
      "Test Loss: 749.4580201992671\n",
      "Test Loss: 342.8638562953141\n",
      "Epoch 125\n",
      "------------------------------------\n",
      "Loss: 1118.40601873015\n",
      "Loss: 1604.7157026341247\n",
      "Loss: 1416.72385281799\n",
      "Loss: 2035.8834271195592\n",
      "Loss: 4482.708278195998\n",
      "Test Loss: 709.9097975096751\n",
      "Test Loss: 96.20423584722197\n",
      "Epoch 126\n",
      "------------------------------------\n",
      "Loss: 1425.8411808939632\n",
      "Loss: 2352.260681751379\n",
      "Loss: 1446.076728207491\n",
      "Loss: 1034.8746734821705\n",
      "Loss: 2093.1919123091493\n",
      "Test Loss: 673.728286751215\n",
      "Test Loss: 100.59922118415268\n",
      "Epoch 127\n",
      "------------------------------------\n",
      "Loss: 1707.0703949854433\n",
      "Loss: 1790.8681596883057\n",
      "Loss: 1243.2153272868447\n",
      "Loss: 1607.7610747894448\n",
      "Loss: 3317.3082894764625\n",
      "Test Loss: 665.4013048327754\n",
      "Test Loss: 24.162670707429506\n",
      "Epoch 128\n",
      "------------------------------------\n",
      "Loss: 1078.0608652911278\n",
      "Loss: 1920.7134081399174\n",
      "Loss: 1986.3521518428595\n",
      "Loss: 1622.721805478149\n",
      "Loss: 292.4218428119185\n",
      "Test Loss: 769.4068872970631\n",
      "Test Loss: 438.6695079618919\n",
      "Epoch 129\n",
      "------------------------------------\n",
      "Loss: 3415.9337862486077\n",
      "Loss: 1070.5225761204913\n",
      "Loss: 1292.8662232044303\n",
      "Loss: 844.9115622623117\n",
      "Loss: 166.494229587121\n",
      "Test Loss: 680.0918684166348\n",
      "Test Loss: 115.847375057534\n",
      "Epoch 130\n",
      "------------------------------------\n",
      "Loss: 804.5947948908238\n",
      "Loss: 1874.525560785046\n",
      "Loss: 1906.0288086994415\n",
      "Loss: 1876.7929699282859\n",
      "Loss: 318.1048060069185\n",
      "Test Loss: 634.6904048949049\n",
      "Test Loss: 775.6854287806775\n",
      "Epoch 131\n",
      "------------------------------------\n",
      "Loss: 1590.4249427349118\n",
      "Loss: 1386.5237418981405\n",
      "Loss: 2357.7865784805354\n",
      "Loss: 803.7364795815492\n",
      "Loss: 2284.5081600333465\n",
      "Test Loss: 729.9345747566682\n",
      "Test Loss: 839.7692761064525\n",
      "Epoch 132\n",
      "------------------------------------\n",
      "Loss: 1541.3197604329202\n",
      "Loss: 1603.9075223692278\n",
      "Loss: 1701.2273404067382\n",
      "Loss: 1235.3432453444366\n",
      "Loss: 3712.2296839248265\n",
      "Test Loss: 679.6722800404648\n",
      "Test Loss: 221.27408613737106\n",
      "Epoch 133\n",
      "------------------------------------\n",
      "Loss: 1178.2267758133894\n",
      "Loss: 1753.83168525778\n",
      "Loss: 2213.7646063334305\n",
      "Loss: 1201.0782796298843\n",
      "Loss: 490.5907057735046\n",
      "Test Loss: 651.6032838670399\n",
      "Test Loss: 48.60459044148778\n",
      "Epoch 134\n",
      "------------------------------------\n",
      "Loss: 1873.6785410561301\n",
      "Loss: 711.5719666399273\n",
      "Loss: 1943.2257684491542\n",
      "Loss: 1819.875483922843\n",
      "Loss: 482.50035388517455\n",
      "Test Loss: 650.0257856998023\n",
      "Test Loss: 18.45620309353729\n",
      "Epoch 135\n",
      "------------------------------------\n",
      "Loss: 1882.2994607907171\n",
      "Loss: 714.9602810006118\n",
      "Loss: 1147.392107373571\n",
      "Loss: 2417.554100890454\n",
      "Loss: 677.4668799443739\n",
      "Test Loss: 672.8856476674723\n",
      "Test Loss: 17.80295945551385\n",
      "Epoch 136\n",
      "------------------------------------\n",
      "Loss: 1739.333779280093\n",
      "Loss: 1320.2777639634535\n",
      "Loss: 1860.6351878428432\n",
      "Loss: 1085.1443722006954\n",
      "Loss: 2469.544077997006\n",
      "Test Loss: 719.6921052847175\n",
      "Test Loss: 152.60045013570397\n",
      "Epoch 137\n",
      "------------------------------------\n",
      "Loss: 1424.3986499601363\n",
      "Loss: 883.942445118282\n",
      "Loss: 1646.451098332534\n",
      "Loss: 2069.6920197590543\n",
      "Loss: 885.749561905346\n",
      "Test Loss: 644.5345605282567\n",
      "Test Loss: 57.104811145415525\n",
      "Epoch 138\n",
      "------------------------------------\n",
      "Loss: 1118.2964042566523\n",
      "Loss: 1425.441534876307\n",
      "Loss: 1887.2041881876148\n",
      "Loss: 1865.5359151513308\n",
      "Loss: 854.3400068094088\n",
      "Test Loss: 639.3181775486406\n",
      "Test Loss: 169.53613518513075\n",
      "Epoch 139\n",
      "------------------------------------\n",
      "Loss: 1126.3754804649657\n",
      "Loss: 1344.2681804001318\n",
      "Loss: 1395.6391029966144\n",
      "Loss: 2123.9221559661783\n",
      "Loss: 1626.9246110191452\n",
      "Test Loss: 646.0244986666162\n",
      "Test Loss: 336.3618808916992\n",
      "Epoch 140\n",
      "------------------------------------\n",
      "Loss: 1709.6345063656556\n",
      "Loss: 1574.0167564703934\n",
      "Loss: 1384.122916255386\n",
      "Loss: 1627.3942712029677\n",
      "Loss: 919.2031586425676\n",
      "Test Loss: 631.8367332254102\n",
      "Test Loss: 228.8332863289023\n",
      "Epoch 141\n",
      "------------------------------------\n",
      "Loss: 1932.332019007209\n",
      "Loss: 1466.4330416341863\n",
      "Loss: 1409.9008071519452\n",
      "Loss: 871.287082004483\n",
      "Loss: 3229.0426440433143\n",
      "Test Loss: 682.9143034764913\n",
      "Test Loss: 122.77929988502166\n",
      "Epoch 142\n",
      "------------------------------------\n",
      "Loss: 1776.8624530721304\n",
      "Loss: 1877.5820908411697\n",
      "Loss: 1223.9137005025914\n",
      "Loss: 1438.3103745450733\n",
      "Loss: 1159.119794111766\n",
      "Test Loss: 839.8176504256676\n",
      "Test Loss: 136.37415211066397\n",
      "Epoch 143\n",
      "------------------------------------\n",
      "Loss: 1668.9900162590868\n",
      "Loss: 2065.7313276477385\n",
      "Loss: 1320.4946476142145\n",
      "Loss: 1540.1728412105872\n",
      "Loss: 415.9011782581368\n",
      "Test Loss: 680.061955920161\n",
      "Test Loss: 2108.417697993452\n",
      "Epoch 144\n",
      "------------------------------------\n",
      "Loss: 1704.645430780929\n",
      "Loss: 1728.7620393085992\n",
      "Loss: 1241.2186965872697\n",
      "Loss: 1093.931596403075\n",
      "Loss: 2900.5576491871434\n",
      "Test Loss: 618.0736613306567\n",
      "Test Loss: 1503.3566161980648\n",
      "Epoch 145\n",
      "------------------------------------\n",
      "Loss: 1901.493550404402\n",
      "Loss: 1258.4574622760867\n",
      "Loss: 1988.8220444445456\n",
      "Loss: 1049.155165830368\n",
      "Loss: 789.3115790595268\n",
      "Test Loss: 628.6251031837545\n",
      "Test Loss: 100.6545328676069\n",
      "Epoch 146\n",
      "------------------------------------\n",
      "Loss: 1881.170796069056\n",
      "Loss: 1238.9714196888306\n",
      "Loss: 1924.09991979471\n",
      "Loss: 868.9246198033533\n",
      "Loss: 1020.320034009169\n",
      "Test Loss: 682.527388033326\n",
      "Test Loss: 234.8612545106183\n",
      "Epoch 147\n",
      "------------------------------------\n",
      "Loss: 1159.8320144926051\n",
      "Loss: 1559.1578231417975\n",
      "Loss: 1652.0957674056367\n",
      "Loss: 1571.3952348174455\n",
      "Loss: 524.5100912101303\n",
      "Test Loss: 654.143819802955\n",
      "Test Loss: 9.00642362952365\n",
      "Epoch 148\n",
      "------------------------------------\n",
      "Loss: 1138.938705431969\n",
      "Loss: 1333.7959840647063\n",
      "Loss: 1809.1044957508323\n",
      "Loss: 1625.8706839631127\n",
      "Loss: 81.89953782260287\n",
      "Test Loss: 616.1073591425287\n",
      "Test Loss: 156.01621688000876\n",
      "Epoch 149\n",
      "------------------------------------\n",
      "Loss: 1757.1610402804204\n",
      "Loss: 1453.6635898592667\n",
      "Loss: 1672.2899436906284\n",
      "Loss: 1138.0277291419618\n",
      "Loss: 1492.8713746239207\n",
      "Test Loss: 619.63083293964\n",
      "Test Loss: 196.84407915504153\n",
      "Epoch 150\n",
      "------------------------------------\n",
      "Loss: 1105.3454838930475\n",
      "Loss: 1473.2639253880925\n",
      "Loss: 1945.1211770183997\n",
      "Loss: 1496.6515370005948\n",
      "Loss: 299.07251889936526\n",
      "Test Loss: 622.0178189363037\n",
      "Test Loss: 9.698904505981904\n",
      "Training complete!\n",
      "tensor(81.8995, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n------------------------------------\")\n",
    "    train_loop(LFW_train, model, LOSS_FUNCTION, optimizer, LOWEST_LOSS)\n",
    "    test_loop(LFW_test, model, LOSS_FUNCTION, optimizer, LOWEST_LOSS)\n",
    "    X_epochs.append(t)\n",
    "print(\"Training complete!\")\n",
    "print(LOWEST_LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10940267.715043351, 23576113.397908762, 10285104.421093319, 16526880.075345013, 13965866.45978802, 7320812.8539527, 5397397.455717239, 12905049.088615363, 6481196.117680663, 3381311.797130401, 5315352.764860764, 3549784.883944831, 3658448.012819658, 1618307.3880160074, 119451.60719737754, 1984234.3669089426, 1138353.927736024, 486146.38210383436, 400725.1051939209, 694248.7257590373, 208379.11079074247, 110597.86883402415, 57090.318156636225, 35047.23866363551, 19200.821576007416, 5667.822024089732, 8310.706009206744, 16793.445035390156, 38325.59356877751, 32612.77880971386, 88166.93395871058, 84605.8675013831, 88860.44776990844, 112466.07214845376, 57249.53393450794, 113127.36741902774, 143657.58257141785, 90403.87966290693, 68037.39101607358, 49592.75682709598, 78234.11630197449, 42780.76993286175, 37420.44771207254, 44193.75802300178, 12513.125520003712, 9924.223631789828, 10993.942584202021, 7152.770555396704, 5170.595063573654, 3709.9261496653335, 2181.6332379178857, 1481.244232371955, 6866.191055934159, 2602.433928792046, 629.2994971120422, 3750.7132974924743, 9237.945628960722, 4564.343722908671, 9902.814465277203, 3987.8337418240367, 11044.24698937076, 4164.005862272573, 3652.2421254468045, 5049.033479673348, 29653.07819695481, 3122.164411578289, 5461.139332001032, 3641.3965310831218, 2140.825347417445, 951.2094424857648, 1955.8605316320907, 3180.4717164786553, 2409.192913525278, 2594.956360459004, 2302.395881636103, 3238.703333359588, 2641.7434866343547, 2639.712057578403, 2397.4043432794624, 5993.054327637811, 3637.2240744834207, 2420.257505878285, 3254.8776715697286, 2678.833176001865, 5709.997428123669, 3383.922368156498, 3230.416071977833, 2901.6218248122923, 1964.5786880405003, 416.4794711263504, 2934.8420977921874, 2610.279231044253, 3274.2255974639857, 1543.6906887963132, 2550.791436097099, 1659.3236733453787, 3066.323620801489, 2405.8898699320207, 3129.868465245166, 1115.3879424627055, 1946.4063867173165, 2154.672849103037, 2252.9816611567016, 3753.06448048486, 1466.9659129460422, 2907.613261450871, 1418.997417415753, 2716.353592633758, 3117.054043516491, 617.8517668522594, 1688.7756325069645, 2561.7728916900387, 3275.805721632413, 2185.413065731646, 3719.8966542189023, 2822.416712915598, 2164.4172708268948, 3232.805426632221, 1484.1599232405233, 3319.749252695384, 4146.88698696292, 1251.6040632026804, 1435.78109216389, 3128.52603507664, 1297.9990783461217, 1371.295802170256, 2873.5562961575015, 2557.785600372944, 3010.9953844044708, 1766.9055556050375, 2328.0402695267353, 2522.9800338820105, 1850.6354782552794, 2986.8257757121582, 2984.9764437910308, 3832.2087912503853, 1176.7889515042073, 3164.257689419483, 1481.4257994311015, 4174.106017780722, 3156.5675894117812, 3012.2889637830713, 1802.0400713210004, 1414.5247876351314, 4528.839411243958, 3119.747982175063, 2361.6649356556645, 1950.6749266720838, 2492.3032449222906, 437.2397553544463, 3867.9250255642482, 1836.2485946272386, 1737.9170048038309, 2328.877833581568, 1149.6522374229983, 1852.9318275130342, 2285.782960233575, 2709.1953499095935, 2046.5700990436928, 8297.313649908974, 2658.307722199434, 1670.1388185576918, 3291.4610112225546, 1581.8509336836282, 5608.225651959043, 2351.014260119171, 2905.3590884854325, 2904.8957234154154, 1364.295350352284, 2769.6617629568336, 3115.2664487746088, 3125.067517476244, 1111.3219994987198, 2334.941341963866, 1025.58625034577, 2474.448657854443, 1252.7842363855934, 3336.8113404608403, 2501.2425987692527, 1429.3755451582667, 3325.9007631398786, 2728.6548849504316, 1310.2191803239416, 2376.5206221976587, 560.6495673312003, 1914.7294408557752, 4091.5648135621223, 1384.708919591502, 2107.2873496923435, 2453.7323341755205, 2293.4012270469375, 1492.6090365711484, 2894.3960048595263, 2931.403282396848, 1517.2959790945229, 1685.052339709945, 2181.8770638375527, 1868.7594605678325, 2095.7903625650697, 15248.263232616939, 2085.5479645331793, 2550.3830773019113, 2561.4802343519345, 2356.1595727575304, 2508.7619509076358, 2432.081849260172, 2957.064231132608, 2936.591508940356, 2548.675325127691, 1400.9871584140888, 2892.5449183741753, 2741.4909956089355, 2021.5872219892444, 2486.4476509261176, 825.1931474118992, 2116.219260173747, 3338.6635493160074, 1976.0348108171365, 1822.4579934123103, 1254.8267846592596, 1356.5471960140974, 2883.168963552422, 3555.9716989852036, 1912.112016375503, 628.3243089104387, 3745.236983631689, 3211.2684560765415, 1714.1032370743064, 996.8057241428006, 898.241004337227, 3103.0592701987794, 1856.7275127725331, 1309.5903884029292, 2527.2003143065917, 5362.983457334035, 1825.735035801273, 2998.8720602770627, 2312.6733066002053, 1913.175392286083, 2952.6699323367743, 2813.7370540030392, 2008.283758052874, 2006.6007939596893, 2159.5936989305337, 2529.6608820718225, 2124.6299971636736, 3139.2650258315643, 2110.729566558797, 1926.1106747516455, 837.7359050829466, 2151.3556272253236, 2514.847995186275, 3212.745635752657, 1919.0586348715879, 399.74357847595854, 1787.732023925742, 2025.5894095694857, 2381.892740459887, 2884.2100188626982, 649.7017452941963, 2118.5306048181224, 1832.9954523015178, 2880.6501271868133, 2137.072209931257, 1170.1709324921922, 2339.506417971879, 1597.824808672608, 3621.3295630192915, 1533.6107532078295, 1385.8791984069721, 1304.1783738415954, 2922.709153400171, 2432.1910588927512, 2213.2807551945634, 3032.7327456546827, 1438.164948231008, 3139.0417758731674, 3140.5397378421508, 1190.19613573548, 1056.6566907092852, 2967.13505332819, 2655.2583582912857, 1477.9992294350122, 1757.5064848711131, 1233.9833443819816, 2116.6861094391925, 2988.06895598578, 1572.844807201883, 1497.4358413396815, 6910.353838440304, 2475.3298477500502, 3052.7727271641616, 2287.0690879009267, 1303.925805906472, 1211.1594318356088, 1932.6074473724723, 2814.7555131265312, 2300.165925390172, 1965.5424471056967, 4229.781250813201, 2375.5718980482516, 1806.9146942979175, 2349.4221221726057, 1977.1103205620025, 1750.6225122080693, 2815.3617321201036, 1497.7672828080053, 2564.7942772192514, 2109.005193786904, 1627.5225112687494, 1040.5412369292917, 3953.2508916922898, 2065.607960157269, 1663.0585085707999, 3413.9721381216746, 1024.361697777937, 3030.262811280285, 2500.082292218844, 2185.976198552505, 282.9675345313266, 1401.3689340459348, 2601.530245704707, 1207.9972091017503, 3463.4655495068628, 270.24136868459647, 2366.168515447691, 3497.9685777657432, 1679.035645782871, 1042.7785939351634, 1782.2591780741857, 3274.0455305264686, 2068.304551914039, 1920.2843785739333, 1667.7352833686111, 1047.9971578999935, 1841.953871794198, 1921.204253689356, 2688.2737731872585, 2010.7681230508658, 1807.8839068696038, 2332.017965200956, 1863.002777820287, 2442.1042072533146, 1761.41502775872, 1906.877692601908, 2356.6630409815875, 1259.7211969082134, 1878.3042494738163, 2859.689205520299, 5844.524855773266, 1615.4074739747575, 2676.544542754588, 2222.1254875198724, 1864.2464852840478, 2223.466454738202, 2363.468033355915, 2376.205788689024, 855.7138558808833, 1864.0877625245923, 8213.837650432442, 1499.571884976648, 1542.0760063830235, 2943.868112223916, 2720.0783173862797, 3062.892391467884, 2250.2223710572925, 2102.360613370186, 1029.8601647134958, 2871.4297426527755, 731.2260715196414, 1707.091334824871, 3276.8854716346486, 1211.6182555305452, 2322.8249386817088, 753.8286923907426, 3745.4948789639448, 1757.8306197016302, 1379.2502558807796, 2059.0892419238435, 292.1664942716584, 906.3919949773878, 2851.183705463967, 2410.731199614317, 1666.2423062153964, 4071.842081226001, 2492.272999243438, 1830.4363283207188, 1954.0321166751373, 1961.3214481265445, 478.3009839495277, 2215.2529314514845, 2126.5897070970586, 2240.4646679050466, 1608.173476115413, 741.618146020349, 1329.770717002462, 1232.4023052050807, 3166.81299705084, 2428.674939894426, 271.5057879688948, 1815.324630792707, 2428.9135225217897, 2002.9808919672569, 1882.517877987908, 376.36912632452504, 1569.9957497316686, 981.0954055483294, 1980.495352782406, 2758.3345328974524, 6166.698242579712, 1769.7018161444098, 1126.919502779617, 1469.7426477571391, 3544.5741244721007, 4097.275060680529, 1877.5093932758873, 1310.3116198225941, 3600.470229256429, 1155.2056023967905, 1184.6543510798274, 2308.2394880419088, 1134.2450342996997, 1281.0606064498545, 2820.9608358923792, 818.186475430119, 2065.4031431880353, 2629.398942794043, 1248.959846212062, 2146.871213791086, 4267.6873984513595, 2005.8804054333407, 1761.3874302869162, 868.1180510666046, 2810.8004995222877, 5012.290430676834, 2642.74559194329, 1357.2038381803188, 2452.599229887954, 1165.2293177798458, 2618.5043914477837, 2863.15457455797, 1963.9574181244302, 1440.814392456832, 1570.9943886721196, 1233.429999644653, 1728.0117173368185, 1123.654416045451, 2948.578944078317, 1669.201869587248, 3710.504575687161, 1776.1084204904814, 1631.4101866457618, 2497.400044948533, 1671.1684122974398, 1499.282834595378, 1760.0669478726636, 1256.855103106122, 1251.523084250821, 2662.901298602897, 7683.151041413378, 1264.3750697493579, 2054.8907273496557, 1217.7488678951045, 2851.554813191379, 2186.4905505567676, 2185.329952461217, 2483.4022136637295, 2148.1638186815608, 2641.5724116035494, 3880.695271205022, 1391.1771137296505, 3604.31861836958, 1275.9611305458607, 2115.2032449262497, 765.8149751085928, 1660.8288072276007, 2168.249965354335, 1946.8700491814668, 2029.0337128764468, 817.0461837780712, 1450.525973891074, 1877.4637007964361, 3509.4078147952982, 1143.132337479285, 5443.39103009025, 1846.4720779414706, 1934.6003692494587, 2375.0847700073223, 1303.7060631112945, 1193.274079235018, 1820.5863317099418, 2220.9843617718057, 2655.3650912060302, 1770.4013243994214, 2069.4693687332765, 1740.6728109702233, 1503.8010519241798, 1359.2612973992677, 2835.689532568387, 756.4148168658522, 1374.6883419340502, 2142.704790826291, 1704.0661107468125, 2412.924837233902, 302.11363793823546, 2454.1882366045634, 1792.906347990052, 813.9022931876168, 2000.7565987316138, 2980.906336238184, 2041.236197612383, 1163.8545314474895, 1877.621152524746, 2088.072447559781, 736.1836257600243, 1196.1471429692642, 1184.0045684637425, 2302.3475929761694, 2459.514563318581, 927.536096530074, 1700.8611265234133, 2052.0431341604385, 1938.2203343542674, 1404.043822882883, 548.7443084881363, 1348.3573758973864, 1691.3452253871385, 1000.8072527167174, 2468.0590140278146, 5184.220761761524, 1217.8779309679098, 1634.3095249906532, 1711.7618064762896, 2602.101497960538, 894.7765480247652, 1798.427138397269, 1923.3496327465314, 1399.561301832452, 2087.701162025797, 180.6940977123416, 2024.393922550108, 1024.4224107757573, 3018.3887323321474, 923.8732095679732, 339.10727563178443, 2104.9748126066556, 2221.8727228613707, 1199.2651264248736, 1431.1240559470314, 821.2726938334769, 1917.6214615884198, 2110.2147321757993, 688.7617813913386, 1316.7187246988883, 7300.358730103422, 2335.428971507736, 1633.166118894608, 1751.034486370736, 1296.9575376228595, 844.817525636121, 2051.717770352314, 1352.760380673711, 1909.4821607670494, 1575.9239982859235, 2421.037902719605, 2060.483651894996, 1414.6308072332868, 1682.4404840351865, 1854.386591993606, 670.2507142174275, 1096.0413408616819, 1931.0508897595696, 2171.526892277584, 1443.597346280425, 3509.356044837177, 1725.5820242694408, 2218.458978449029, 1364.5812968114572, 1087.0394771177585, 3777.4034976908506, 1398.132241278243, 1672.8645764756443, 2113.7023522969503, 1350.6224794895652, 2158.172104388716, 1645.8294934330556, 1848.1581157248522, 1982.612199046644, 1430.4983003539264, 814.0293526382991, 1614.606558828879, 1905.9218631566096, 1692.5052348245324, 1707.988095115492, 2908.056094272277, 1366.3092904848136, 2250.2346466485824, 1377.0518138926377, 1713.5298681976012, 161.25550662532956, 1312.4079180672184, 1777.6817444355897, 2102.7497117478665, 944.1645575927542, 4550.603891506105, 1790.757088403522, 1308.0497755852284, 1825.109300391758, 1395.0068845682704, 7242.259777233853, 2359.1576333486823, 2834.3685606324693, 1504.220037716816, 1218.151107907416, 1334.9617437238685, 1462.2305580911154, 2543.620065390133, 1260.1970462855784, 1299.8093718821558, 246.51087591550072, 1118.40601873015, 1604.7157026341247, 1416.72385281799, 2035.8834271195592, 4482.708278195998, 1425.8411808939632, 2352.260681751379, 1446.076728207491, 1034.8746734821705, 2093.1919123091493, 1707.0703949854433, 1790.8681596883057, 1243.2153272868447, 1607.7610747894448, 3317.3082894764625, 1078.0608652911278, 1920.7134081399174, 1986.3521518428595, 1622.721805478149, 292.4218428119185, 3415.9337862486077, 1070.5225761204913, 1292.8662232044303, 844.9115622623117, 166.494229587121, 804.5947948908238, 1874.525560785046, 1906.0288086994415, 1876.7929699282859, 318.1048060069185, 1590.4249427349118, 1386.5237418981405, 2357.7865784805354, 803.7364795815492, 2284.5081600333465, 1541.3197604329202, 1603.9075223692278, 1701.2273404067382, 1235.3432453444366, 3712.2296839248265, 1178.2267758133894, 1753.83168525778, 2213.7646063334305, 1201.0782796298843, 490.5907057735046, 1873.6785410561301, 711.5719666399273, 1943.2257684491542, 1819.875483922843, 482.50035388517455, 1882.2994607907171, 714.9602810006118, 1147.392107373571, 2417.554100890454, 677.4668799443739, 1739.333779280093, 1320.2777639634535, 1860.6351878428432, 1085.1443722006954, 2469.544077997006, 1424.3986499601363, 883.942445118282, 1646.451098332534, 2069.6920197590543, 885.749561905346, 1118.2964042566523, 1425.441534876307, 1887.2041881876148, 1865.5359151513308, 854.3400068094088, 1126.3754804649657, 1344.2681804001318, 1395.6391029966144, 2123.9221559661783, 1626.9246110191452, 1709.6345063656556, 1574.0167564703934, 1384.122916255386, 1627.3942712029677, 919.2031586425676, 1932.332019007209, 1466.4330416341863, 1409.9008071519452, 871.287082004483, 3229.0426440433143, 1776.8624530721304, 1877.5820908411697, 1223.9137005025914, 1438.3103745450733, 1159.119794111766, 1668.9900162590868, 2065.7313276477385, 1320.4946476142145, 1540.1728412105872, 415.9011782581368, 1704.645430780929, 1728.7620393085992, 1241.2186965872697, 1093.931596403075, 2900.5576491871434, 1901.493550404402, 1258.4574622760867, 1988.8220444445456, 1049.155165830368, 789.3115790595268, 1881.170796069056, 1238.9714196888306, 1924.09991979471, 868.9246198033533, 1020.320034009169, 1159.8320144926051, 1559.1578231417975, 1652.0957674056367, 1571.3952348174455, 524.5100912101303, 1138.938705431969, 1333.7959840647063, 1809.1044957508323, 1625.8706839631127, 81.89953782260287, 1757.1610402804204, 1453.6635898592667, 1672.2899436906284, 1138.0277291419618, 1492.8713746239207, 1105.3454838930475, 1473.2639253880925, 1945.1211770183997, 1496.6515370005948, 299.07251889936526]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y_loss['train'])\n",
    "plt.plot(y_loss['train'])\n",
    "plt.plot(y_loss['test'])\n",
    "#plt.plot(X_epochs)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05dbbb7414389032baa654308b5b2368ed4754b5c0531661864b7939633c6eac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

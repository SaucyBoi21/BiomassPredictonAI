---
title: "Model Selection"
author: "Jonathan CÃ¡rdenas"
date: "2023-03-12"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(readxl)
library(caret)
library(ggridges)
library(reshape2)
library(lubridate)
library(RColorBrewer)
library(corrplot)
library(GGally)
library(party)
library(MASS)
library(randomForest)
```
# 1. Dataset upload and data set splitting 
Uploading the original dataset (raw data): 
```{r}
raw_data<- read.csv("harvest_complete.csv")
raw_data$treatment <-as.factor(with(raw_data, ifelse(tray_id == '1', '280 ppm',
                                                     ifelse(tray_id == 2, '160 ppm', '80 ppm'))))
```

## 1.1 Separate dataset to use image derived features as predictors. 
Create 6 data frames based on the raw data: 
Source: Metadata, predictors and all response variables (raw_data).  
1) Image derived predictors and all response variables. 
2) Manual predictors and all response variables. 
3) Image derived predictors and LFW
4) Image derived predictors and LDW
5) Image derived predictors and LA

```{r}
img_predictors <- raw_data %>% dplyr::select(12:24) #1
manual_predictors <- raw_data %>% dplyr::select(1,3,7,9:12,25) #2
img_LFW <- raw_data %>% dplyr::select(7,12:23) #3
img_LDW <- raw_data %>% dplyr::select(8,12:23) #4
img_LA <- raw_data %>% dplyr::select(7,8,9,12:23) #5
```

# 2. Model Selection and Performance 

Fit different models to estimate 3 biomass variables (LFW,LDW,LA). Our baseline comparison (Model 0) will be a multiple linear regression model chose by using AIC and BIC criteria. 


## Model 0: Multiple Linear Regression. 
### Estimation of Leaf Fresh Weight (LFW)
```{r}
# Fitting null & full model to use step AIC in order to select the best variables (we wabt to optimize model fit and model complexity). 

full_LFW<- lm(LFW_g~., data = img_LFW)
null_LFW<- lm(LFW_g~1, data = img_LFW)

linear_mod<-stepAIC(null_LFW,direction = 'forward', scope = list(upper=full_LFW,lower = null_LFW))

# Here, we are storing the optimized linear model in a variable called linear_mod. 
```
```{r}
summary(linear_mod)
```
After checking the coefficients p-value,looks like is possible to discard more predictors that are not so significant (plant_ellipse_minor_axis, plant_convex_hull_area). The model  shown here discards three variables that are not significant according to the coefficient p-value. However, our models performance gets worse. 
```{r}
opt_LFW_linear <- lm(LFW_g ~ height_mm + plant_area + 
    plant_ellipse_major_axis, 
    data = img_LFW)

summary(opt_LFW_linear)
```

### Estimation of Leaf Dry Weight (LDW)
```{r}
# Fitting null & full model to use step AIC in order to select the best variables (we wabt to optimize model fit and model complexity). 

full_LDW<- lm(LDW_g~., data = img_LDW)
null_LDW<- lm(LDW_g~1, data = img_LDW)
linear_LDW<-stepAIC(null_LDW,direction = 'forward', scope = list(upper=full_LDW,lower = null_LDW))

# Here, we are storing the optimized linear model in a variable called linear_mod. 
```
```{r}
summary(linear_LDW)
```

### Estimation of Leaf Area (LA)
Notice that for Leaf Area(LA) we are including both LFW and LDW as predictors. 
```{r}
# Fitting null & full model to use step AIC in order to select the best variables (we wabt to optimize model fit and model complexity). 

full_LA<- lm(LA_mm2~., data = img_LA)
null_LA<- lm(LA_mm2~1, data = img_LA)
linear_LA<-stepAIC(null_LA,direction = 'forward', scope = list(upper=full_LA,lower = null_LA))

# Here, we are storing the optimized linear model in a variable called linear_mod. 
```
```{r}
summary(linear_LA)
```

## Model 1: Random Forest (Holistic Model)
From here, we will try to improve our Random Forest model and give graphic and numeric explanation of the models accuracy. We will split our dataset into a training and test set using a 70/30 split (meaning that 70% of our data is used for training while 30% will be used for testing). 

Why holistic model? It will predict biomass accummulation regardless of the date the harvest was performed. 

### Estimation Leaf Fresh Weight (LFW)
```{r}
# Use a random forest model to predict LFW, this time splitting the dataset. 
set.seed(123)
train_id<- sample(1:nrow(img_LFW),0.7*nrow(img_LFW))
train_set<- img_LFW[train_id,]
test_set<- img_LFW[-train_id,]
```
Once we split the whole dataset we can run the Random Forest model using our train_data. 

```{r}
LFW_rf <- randomForest(LFW_g~., data = train_set, ntree = 500)
```

Now all the predictions will be made on our test set. 

```{r}
model_pred <- predict(LFW_rf, newdata = test_set)
```

#### Model 1 Performance for LFW
We start with a variable importance plot that is showing us which of the predictors used (variables) were most important in predicting the response variable. With this we can identify the key features that are driving the model's predictions. 

The following chart shows the models summary. Is important to mention here that the models summary give us an out-of-bag MSE (for values used to train the model). 


```{r}
varImpPlot(LFW_rf)

LFW_rf
sqrt(LFW_rf$mse[which.min(LFW_rf$mse)]) 

ggplot(data.frame(actual = test_set$LFW_g, predicted = model_pred), aes(x = actual, y = predicted)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Actual", y = "Predicted", title = "Random Forest Regression Model Accuracy")
```
Notice that the R-squared value obtained below is not the same as the R-quared given by the model's summary. For this R-squared we are correlating the predicted values and the labels from the test set.

MSE displayed is not the same as the summary MSE neither. The MSE we are calculating here is mean squared error of the model's prediction on the testing data. This metric measures how well the model generalizes to new, unseen data. 

While the two MSE values are related, they are not the same. The out-of-bag MSE gives an estimate of the model's generalization performance based on the training data, while the MSE on the test set provides an actual measure of how well the model performs on new, unseen data. 
```{r}
# Evaluate the model performance: On new, unseen data. 
MAE <- mean(abs(model_pred - test_set$LFW_g))
MSE <- mean((model_pred - test_set$LFW_g)^2)
R_squared <- cor(model_pred,test_set$LFW_g)^2
RMSE<- RMSE(model_pred,test_set$LFW_g)

# Print the performance metrics
cat("Mean Absolute Error (MAE): ", MAE, "\n")
cat("Mean Squared Error (MSE): ", MSE, "\n")
cat("R-squared: ", R_squared, "\n")
cat("RMSE (Prediction): ", RMSE, "\n")
```
Partial dependence plots show how each variable or predictor affects the model's predictions.However, Partial dependence plots will have problems with highly correlated variables (like the ones we are including in our models).

A solution to deal with that correlation is using an Accumulated Local Effect (ALE) plot. 


Relative importance of features

```{r}
LFW_importance <- importance(LFW_rf, type = 1)
varImpPlot(LFW_rf, main = "Variable Importance Plot")
print(LFW_importance)
```
#### LFW Selecting Variables
```{r}
# Use a random forest model to predict LFW, this time splitting the dataset. 
set.seed(123)
train_id<- sample(1:nrow(img_LFW),0.7*nrow(img_LFW))
train_set<- img_LFW[train_id,]
test_set<- img_LFW[-train_id,]
```



### Estimation Leaf Dry Weight (LDW)
```{r}
# Use a random forest model to predict LFW, this time splitting the dataset. 
set.seed(123)
train_id_LDW<- sample(1:nrow(img_LDW),0.7*nrow(img_LDW))
train_set_LDW<- img_LDW[train_id_LDW,]
test_set_LDW<- img_LDW[-train_id_LDW,]
```
Once we split the whole dataset we can run the Random Forest model using our train_data. 

```{r}
LDW_rf <- randomForest(LDW_g~., data = train_set_LDW, ntree = 500)
```

Now all the predictions will be made on our test set. 

```{r}
model_LDW <- predict(LDW_rf, newdata = test_set_LDW)
```

#### Model 1 Performance for LDW
The following chart shows the models summary 

```{r}
varImpPlot(LDW_rf)

LDW_rf
sqrt(LDW_rf$mse[which.min(LDW_rf$mse)]) 

ggplot(data.frame(actual = test_set_LDW$LDW_g, predicted = model_LDW), aes(x = actual, y = predicted)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Actual", y = "Predicted", title = "Random Forest Regression Model Accuracy")
```
```{r}
# Evaluate the model performance
MAE <- mean(abs(model_LDW - test_set_LDW$LDW_g))
MSE <- mean((model_LDW - test_set_LDW$LDW_g)^2)
R_squared <- cor(model_LDW,test_set_LDW$LDW_g)^2

# Print the performance metrics
cat("Mean Absolute Error (MAE): ", MAE, "\n")
cat("Mean Squared Error (MSE): ", MSE, "\n")
cat("R-squared: ", R_squared, "\n")
```

### Estimation Leaf Area (LA)
```{r}
# Use a random forest model to predict LFW, this time splitting the dataset. 
set.seed(123)
train_id_LA<- sample(1:nrow(img_LA),0.7*nrow(img_LA))
train_set_LA<- img_LA[train_id_LA,]
test_set_LA<- img_LA[-train_id_LA,]
```
Once we split the whole dataset we can run the Random Forest model using our train_data. 

```{r}
LA_rf <- randomForest(LA_mm2~., data = train_set_LA, ntree = 500)
```

Now all the predictions will be made on our test set. 

```{r}
model_LA <- predict(LA_rf, newdata = test_set_LA)
```

The following chart shows the models summary 


#### Model 1 Performance for LA
```{r}
varImpPlot(LA_rf, main = "Variable importance in Predicting Leaf Area")

LA_rf
sqrt(LA_rf$mse[which.min(LA_rf$mse)]) # Raw calculation of RMSE
RMSE(model_LA,test_set_LA$LA_mm2) # using caret functions
R2(model_LA,test_set_LA$LA_mm2)

ggplot(data.frame(actual = test_set_LA$LA_mm2, predicted = model_LA), aes(x = actual, y = predicted)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Actual", y = "Predicted", title = "Random Forest Regression Model Accuracy")
```

```{r}
# Evaluate the model performance
MAE <- mean(abs(model_LA - test_set_LA$LA_mm2))
MSE <- mean((model_LA - test_set_LA$LA_mm2)^2)
R_squared <- cor(model_LA,test_set_LA$LA_mm2)^2

# Print the performance metrics
cat("Mean Absolute Error (MAE): ", MAE, "\n")
cat("Mean Squared Error (MSE): ", MSE, "\n")
cat("R-squared: ", R_squared, "\n")
```
## Predictions Filtered By Harvest Date
From here, I'll filter the information of our entire dataset by date to assess the model's performance in each harvest date. In this scenario, the predict function will take as parameter the data set with an specific date. Lets start with the first harvest date

```{r}
LFW_h1 <- raw_data %>% filter(date == "02/02/2023") %>% dplyr::select(7,12:23) #3
```
Predicition of LFW for first harvest date using random forest model: 

```{r}
h1_pred <- predict(LFW_rf, newdata = LFW_h1)
```

```{r}
# Evaluate the model performance
mean_h1 <- mean(h1_pred)
MAE <- mean(abs(h1_pred - LFW_h1$LFW_g))
MSE <- mean((h1_pred - LFW_h1$LFW_g)^2)
R_squared <- cor(h1_pred,LFW_h1$LFW_g)^2
RMSE<-RMSE(h1_pred,LFW_h1$LFW_g) # using caret functions
R_sqrd<- R2(h1_pred,LFW_h1$LFW_g) # same as cor(h3_pred,LFW_h3$LFW_g)^2
mean_LFW_h1 <- mean(LFW_h1$LFW_g)
sd_LFW_h1 <- sd(LFW_h1$LFW_g)

# Print the performance metrics
cat("Mean Absolute Error (MAE): ", MAE, "\n")
cat("Mean Squared Error (MSE): ", MSE, "\n")
cat("R-squared: ", R_sqrd, "\n")
cat("RMSE (Prediction): ", RMSE, "\n") # same as sqrt(MSE)
cat("Average H2 (LFW): ", mean_LFW_h1, "\n")
cat("Standard deviation H2 (LFW)): ", sd_LFW_h1, "\n")

ggplot(data.frame(actual = LFW_h1$LFW_g, predicted = h1_pred), aes(x = actual, y = predicted)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Actual", y = "Predicted", title = "Random Forest Regression Model Accuracy")
```

Now, we will do the same for our second harvest date. 

```{r}
LFW_h2 <- raw_data %>% filter(date == "09/02/2023") %>% dplyr::select(7,12:23) #3
```
Prediction of LFW for first harvest date using random forest model: 

```{r}
h2_pred <- predict(LFW_rf, newdata = LFW_h2)
```

Lets check the performance of the random forest model for the data obtained for our second harvest:

```{r}
# Evaluate the model performance
MAE <- mean(abs(h2_pred - LFW_h2$LFW_g))
MSE <- mean((h2_pred - LFW_h2$LFW_g)^2)
RMSE<-RMSE(h2_pred,LFW_h2$LFW_g) # using caret functions
R_sqrd<- R2(h2_pred,LFW_h2$LFW_g) # same as cor(h3_pred,LFW_h3$LFW_g)^2
mean_LFW_h2 <- mean(LFW_h2$LFW_g)
sd_LFW_h2 <- sd(LFW_h2$LFW_g)

# Print the performance metrics
cat("Mean Absolute Error (MAE): ", MAE, "\n")
cat("Mean Squared Error (MSE): ", MSE, "\n")
cat("R-squared: ", R_sqrd, "\n")
cat("RMSE (Prediction): ", RMSE, "\n") # same as sqrt(MSE)
cat("Average H2 (LFW): ", mean_LFW_h2, "\n")
cat("Standard deviation H2 (LFW)): ", sd_LFW_h2, "\n")

ggplot(data.frame(actual = LFW_h2$LFW_g, predicted = h2_pred), aes(x = actual, y = predicted)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Actual", y = "Predicted", title = "Random Forest Regression Model Accuracy")


```
Apparently, outliers (2 points in this case) seems to avoid a good accuracy at this time point. What if we erase those two outliers. 

```{r}
LFW_h3 <- raw_data %>% filter(date == "23/02/2023") %>% dplyr::select(7,12:23) #3
```
Predicition of LFW for first harvest date using random forest model: 

```{r}
h3_pred <- predict(LFW_rf, newdata = LFW_h3)
```

Lets check the performance of the random forest model for the data obtained for our second harvest:

```{r}
# Evaluate the model performance
MAE <- mean(abs(h3_pred - LFW_h3$LFW_g))
MSE <- mean((h3_pred - LFW_h3$LFW_g)^2)
RMSE<-RMSE(h3_pred,LFW_h3$LFW_g) # using caret functions
R_sqrd<- R2(h3_pred,LFW_h3$LFW_g) # same as cor(h3_pred,LFW_h3$LFW_g)^2
mean_LFW <- mean(LFW_h3$LFW_g)
sd_LFW <- sd(LFW_h3$LFW_g)

# Print the performance metrics
cat("Mean Absolute Error (MAE): ", MAE, "\n")
cat("Mean Squared Error (MSE): ", MSE, "\n")
cat("R-squared: ", R_sqrd, "\n")
cat("RMSE (Prediction): ", RMSE, "\n") # same as sqrt(MSE)
cat("Average (LFW): ", mean_LFW, "\n")
cat("Standard deviation (LFW)): ", sd_LFW, "\n")

ggplot(data.frame(actual = LFW_h3$LFW_g, predicted = h3_pred), aes(x = actual, y = predicted)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Actual", y = "Predicted", title = "Random Forest Regression Model Accuracy")


```


```{r}
LA_h3 <- raw_data %>% filter(date == "23/02/2023") %>% dplyr::select(7,8,9,12:23) #3
```
Predicition of LFW for first harvest date using random forest model: 

```{r}
LAh3_pred <- predict(LA_rf, newdata = LA_h3)
```

Lets check the performance of the random forest model for the data obtained for our second harvest:

```{r}
# Evaluate the model performance
MAE <- mean(abs(LAh3_pred - LA_h3$LA_mm2))
MSE <- mean((LAh3_pred - LA_h3$LA_mm2)^2)
R_squared <- cor(LAh3_pred, LA_h3$LA_mm2)^2

# Print the performance metrics
cat("Mean Absolute Error (MAE): ", MAE, "\n")
cat("Mean Squared Error (MSE): ", MSE, "\n")
cat("R-squared: ", R_squared, "\n")

ggplot(data.frame(actual = LA_h3$LA_mm2, predicted = LAh3_pred), aes(x = actual, y = predicted)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Actual", y = "Predicted", title = "Random Forest Regression Model Accuracy")

sqrt(MSE)
print(mean(LA_h3$LA_mm2))
print(sd(LA_h3$LA_mm2))
```
Now lets apply the same strategy for Leaf Dry Weight (LDW). We will split dataset by date to check how our Random Forest model performs in the last harvest date. 

```{r}
LDW_h3 <- raw_data %>% filter(date == "23/02/2023") %>% dplyr::select(8,12:23) #3
```
Predicition of LDW for first harvest date using random forest model: 

```{r}
LDWh3_pred <- predict(LDW_rf, newdata = LDW_h3)
```

Lets check the performance of the random forest model for the data obtained for our second harvest:

```{r}
# Evaluate the model performance: Don't forget to display the mean of the observed values (labels). 
MAE <- mean(abs(LDWh3_pred - LDW_h3$LDW_g))
MSE <- mean((LDWh3_pred - LDW_h3$LDW_g)^2)
R_pearson <- cor(LDWh3_pred, LDW_h3$LDW_g)
R_squared <- cor(LDWh3_pred, LDW_h3$LDW_g)^2
RMSE<- RMSE(LDWh3_pred,LDW_h3$LDW_g)


# Print the performance metrics
cat("Mean Absolute Error (MAE): ", MAE, "\n")
cat("Mean Squared Error (MSE): ", MSE, "\n")
cat("Pearson Correlation: ", R_pearson, "\n")
cat("R-squared: ", R_squared, "\n")
cat("RMSE ", RMSE, "\n")

ggplot(data.frame(actual = LDW_h3$LDW_g, predicted = LDWh3_pred), aes(x = actual, y = predicted)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Actual", y = "Predicted", title = "Random Forest Regression Model Accuracy")

print(sqrt(MSE))
print(mean(LDW_h3$LDW_g))
print(sd(LDW_h3$LDW_g))
RMSE(LDWh3_pred,LDW_h3$LDW_g)
R2(LDWh3_pred,LDW_h3$LDW_g)
```


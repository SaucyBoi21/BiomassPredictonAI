---
title: "Features Selection for Prediction Models"
author: "Jonathan CÃ¡rdenas"
date: "2023-03-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(BBmisc)
library(tidyverse)
library(dplyr)
library(readxl)
library(caret)
library(ggridges)
library(reshape2)
library(lubridate)
library(RColorBrewer)
library(corrplot)
library(GGally)
library(party)
library(MASS)
library(randomForest)

```

Uploading the raw dataset that contains individual feature value for a single plant in each row and feature names in columns. Response variables LFW,LDW and LA all included:
```{r}
raw_data<- read.csv("harvest_complete.csv")
raw_data$treatment <-as.factor(with(raw_data, ifelse(tray_id == '1', '280 ppm',
                                                     ifelse(tray_id == 2, '160 ppm', '80 ppm'))))
```

## 1.1 Separate dataset to use image derived features as predictors. 
Create 6 data frames based on the raw data: 
Source: Metadata, predictors and all response variables (raw_data).  
1) Image derived predictors and all response variables. 
2) Manual predictors and all response variables. 
3) Image derived predictors and LFW
4) Image derived predictors and LDW
5) Image derived predictors and LA

```{r}
img_predictors <- raw_data %>% dplyr::select(12:24) #1
manual_predictors <- raw_data %>% dplyr::select(1,3,7,9:12,25) #2
img_LFW <- raw_data %>% dplyr::select(7,12:24) #3
img_LDW <- raw_data %>% dplyr::select(8,12:24) #4
img_LA <- raw_data %>% dplyr::select(7,8,9,12:2) #5
```

# Correlations and multicolinearity
## 1) Correlation analysis
We want to avoid redundant features that correlates excessively. The objective here is to implement a methodology that selects just the optimal set of explanatory variables. 

To begin with the analysis, choose the response variable we are interested in predict. For practical purposes we will use the same response variable along the whole report. 

Response variable to predict = Leaf Fresh Weight (LFW).

We want to start by checking the correlations between all image predictors and after that check the correlations between the response variable and each predictor. 

After creating a matrix with all the image derived features we will find the features that are highly correlated. For this we need to define a cutoff (Pearson correlation coefficient of 0.8 for example):
```{r}
# img predictors just includes 
matrix_features <- cor(img_predictors)
corrplot(matrix_features, type = "lower",  addCoef.col = "black")
matrix_features
```
Alternatively, we can reshape our features matrix to create a correlation plot using ggplot. This option will adapt better to the screen size. 

Remember that here we are just correlating the geometrical image-derived features. We are not correlating each feature with our response variable. The intention of this step is to avoid features overffitting. 

The following data in reshaping our correlation matrix so we can use ggplot to plot a better representation of a correlation matrix
```{r}
melted_matrix_feat %>% ggplot(aes(Var1,Var2, fill = value)) +
    geom_tile() +
  scale_fill_gradient2(midpoint = 0.5, mid ="grey70", 
                       limits = c(-1, +1)) +
  labs(title = "Correlation Matrix \n for Image Derived Features \n", 
       x = "", y = "", fill = "Correlation \n Measure") +
  theme(plot.title = element_text(hjust = 0.5, colour = "black", face = "bold"), 
        axis.title.x = element_text(face="bold", colour="darkgreen", size = 12),
        axis.title.y = element_text(face="bold", colour="darkgreen", size = 12),
        legend.title = element_text(face="bold", colour="brown", size = 10),
        axis.text.x = element_text(angle = 90)) +
  geom_text(aes(x = Var1, y = Var2, label = round(value, 2)), color = "black", 
            fontface = "bold", size = 5)
```
To find which are the features that are highly correlate we use the findcorrelation function from caret. The problem with using this function is that we lose accuracy (higher RMSE) in our model. However, we are improving the model when we choose the top 4 variables according to variable importance function after running our random forest model. 

Backward regression to solve this issue. Do not reject variables yet.
```{r}
high_cor <- sort(findCorrelation(matrix_features, cutoff = 0.99))
print(high_cor)

img_predictors_cor <- img_predictors[, -high_cor]
```
# 2) Subset Selection
```{r}
library(leaps)
LFW_all=regsubsets(LFW_g~.,data=img_LFW,nvmax=13)
summary(LFW_all)
```
```{r}
print(summary(LFW_all)$adjr2)
plot(1:13,summary(LFW_all)$adjr2,pch =18, type = "b", col = "red")
```
# Best subset using BIC
We would keep just the following 4 variables by using this approach: 
1)height_mm
2)plant_area
3)plant_ellipse_eccentricity
4)plant_ellipse_major_axis
```{r}
summary(LFW_all)$bic
plot(1:13,summary(LFW_all)$bic,type="b",pch=19,col="blue")
```


# 3) Feature Scaling - Normalization
After deleting the variables that show a high level of correlation between each other we can go ahead and use the new data set with our new group of predictors and perform a rescaling. We want to avoid all our numerical features to have very different scales. 

We have two options here:
Normalization (min - max scaling)
Standardization: Much less affected  by outliers.


Option 2 use normalize function from bbmisc library:
```{r}
norm_bbmisc <- normalize(img_predictors_cor, method = 'range', range = c(0,1))

stand_bbmisc <- normalize(img_predictors_cor, method = 'standardize')

new_dataset<- stand_bbmisc %>%
  mutate(LFW_g = img_LFW$LFW_g)
```



Here, we are using Random Forest as a variable selector instead of a regression model. 

```{r}
# Use a random forest model to predict LFW, this time splitting the dataset. 
set.seed(123)
train_id<- sample(1:nrow(new_dataset),0.7*nrow(new_dataset))
train_set<- new_dataset[train_id,]
test_set<- new_dataset[-train_id,]
```
Once we split the whole dataset we can run the Random Forest model using our train_data. 

```{r}
model <- randomForest(LFW_g~., data = train_set, ntree = 500, importance= TRUE)
varImpPlot(model, sort = TRUE, n.var = 6, main = "Variable Importance Plot")
```
```{r}
selected_variables <- names(model$importanceSD[order(model$importanceSD, decreasing = TRUE)[1:5]])
selected_variables
```

Now all the predictions will be made on our test set. 

```{r}
new_model <- randomForest(LFW_g ~. , data = new_dataset, ntree = 500)
predictions <- predict(new_model,test_set)

new_model
sqrt(new_model$mse[which.min(new_model$mse)])
```






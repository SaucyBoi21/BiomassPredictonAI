{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs.get('mean_squared_error') < -100: #TODO:find target threshold\n",
    "            print(\"\\nReached desired accuracy so cancelling training\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>index</th>\n",
       "      <th>plant_id</th>\n",
       "      <th>tray_id</th>\n",
       "      <th>row</th>\n",
       "      <th>column</th>\n",
       "      <th>LFW_g</th>\n",
       "      <th>LDW_g</th>\n",
       "      <th>LA_mm2</th>\n",
       "      <th>length_mm</th>\n",
       "      <th>...</th>\n",
       "      <th>plant_solidity</th>\n",
       "      <th>plant_perimeter</th>\n",
       "      <th>plant_width</th>\n",
       "      <th>plant_height</th>\n",
       "      <th>plant_longest_path</th>\n",
       "      <th>plant_convex_hull_vertices</th>\n",
       "      <th>plant_ellipse_major_axis</th>\n",
       "      <th>plant_ellipse_minor_axis</th>\n",
       "      <th>plant_ellipse_angle</th>\n",
       "      <th>plant_ellipse_eccentricity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>23/02/2023</td>\n",
       "      <td>77</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>57.96</td>\n",
       "      <td>3.01</td>\n",
       "      <td>726.46</td>\n",
       "      <td>18.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830678</td>\n",
       "      <td>767.938160</td>\n",
       "      <td>193</td>\n",
       "      <td>167</td>\n",
       "      <td>1367</td>\n",
       "      <td>33</td>\n",
       "      <td>190.580887</td>\n",
       "      <td>137.733093</td>\n",
       "      <td>62.138748</td>\n",
       "      <td>0.691160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>23/02/2023</td>\n",
       "      <td>78</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>81.46</td>\n",
       "      <td>3.88</td>\n",
       "      <td>1001.79</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852318</td>\n",
       "      <td>661.612260</td>\n",
       "      <td>164</td>\n",
       "      <td>189</td>\n",
       "      <td>1317</td>\n",
       "      <td>29</td>\n",
       "      <td>176.040924</td>\n",
       "      <td>141.141724</td>\n",
       "      <td>22.056726</td>\n",
       "      <td>0.597653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>23/02/2023</td>\n",
       "      <td>79</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>140.84</td>\n",
       "      <td>6.38</td>\n",
       "      <td>1707.14</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.897274</td>\n",
       "      <td>683.754395</td>\n",
       "      <td>186</td>\n",
       "      <td>194</td>\n",
       "      <td>1427</td>\n",
       "      <td>31</td>\n",
       "      <td>191.379974</td>\n",
       "      <td>164.496170</td>\n",
       "      <td>175.959305</td>\n",
       "      <td>0.511091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>23/02/2023</td>\n",
       "      <td>80</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>108.17</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1364.18</td>\n",
       "      <td>31.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913488</td>\n",
       "      <td>671.754395</td>\n",
       "      <td>198</td>\n",
       "      <td>179</td>\n",
       "      <td>1364</td>\n",
       "      <td>31</td>\n",
       "      <td>194.815384</td>\n",
       "      <td>159.870819</td>\n",
       "      <td>122.656914</td>\n",
       "      <td>0.571464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>23/02/2023</td>\n",
       "      <td>81</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>64.40</td>\n",
       "      <td>3.35</td>\n",
       "      <td>812.24</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809779</td>\n",
       "      <td>821.595015</td>\n",
       "      <td>189</td>\n",
       "      <td>168</td>\n",
       "      <td>1376</td>\n",
       "      <td>22</td>\n",
       "      <td>193.579941</td>\n",
       "      <td>133.306320</td>\n",
       "      <td>123.732269</td>\n",
       "      <td>0.725106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  index  plant_id  tray_id  row  column   LFW_g  LDW_g  \\\n",
       "160  23/02/2023     77        46        3    5       2   57.96   3.01   \n",
       "161  23/02/2023     78        48        3    5       4   81.46   3.88   \n",
       "162  23/02/2023     79        50        3    5       6  140.84   6.38   \n",
       "163  23/02/2023     80        52        3    5       8  108.17   5.25   \n",
       "164  23/02/2023     81        54        3    5      10   64.40   3.35   \n",
       "\n",
       "      LA_mm2  length_mm  ...  plant_solidity  plant_perimeter  plant_width  \\\n",
       "160   726.46       18.5  ...        0.830678       767.938160          193   \n",
       "161  1001.79       21.0  ...        0.852318       661.612260          164   \n",
       "162  1707.14       23.0  ...        0.897274       683.754395          186   \n",
       "163  1364.18       31.4  ...        0.913488       671.754395          198   \n",
       "164   812.24       21.0  ...        0.809779       821.595015          189   \n",
       "\n",
       "     plant_height  plant_longest_path  plant_convex_hull_vertices  \\\n",
       "160           167                1367                          33   \n",
       "161           189                1317                          29   \n",
       "162           194                1427                          31   \n",
       "163           179                1364                          31   \n",
       "164           168                1376                          22   \n",
       "\n",
       "     plant_ellipse_major_axis  plant_ellipse_minor_axis  plant_ellipse_angle  \\\n",
       "160                190.580887                137.733093            62.138748   \n",
       "161                176.040924                141.141724            22.056726   \n",
       "162                191.379974                164.496170           175.959305   \n",
       "163                194.815384                159.870819           122.656914   \n",
       "164                193.579941                133.306320           123.732269   \n",
       "\n",
       "     plant_ellipse_eccentricity  \n",
       "160                    0.691160  \n",
       "161                    0.597653  \n",
       "162                    0.511091  \n",
       "163                    0.571464  \n",
       "164                    0.725106  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = \"../BiomassPredictonAI/final_harvest_data.csv\"\n",
    "dataset = pd.read_csv(df)\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height_mm</th>\n",
       "      <th>plant_area</th>\n",
       "      <th>plant_convex_hull_area</th>\n",
       "      <th>plant_solidity</th>\n",
       "      <th>plant_perimeter</th>\n",
       "      <th>plant_width</th>\n",
       "      <th>plant_height</th>\n",
       "      <th>plant_longest_path</th>\n",
       "      <th>plant_convex_hull_vertices</th>\n",
       "      <th>plant_ellipse_major_axis</th>\n",
       "      <th>plant_ellipse_minor_axis</th>\n",
       "      <th>plant_ellipse_angle</th>\n",
       "      <th>plant_ellipse_eccentricity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>13.0</td>\n",
       "      <td>19084</td>\n",
       "      <td>22974.0</td>\n",
       "      <td>0.830678</td>\n",
       "      <td>767.938160</td>\n",
       "      <td>193</td>\n",
       "      <td>167</td>\n",
       "      <td>1367</td>\n",
       "      <td>33</td>\n",
       "      <td>190.580887</td>\n",
       "      <td>137.733093</td>\n",
       "      <td>62.138748</td>\n",
       "      <td>0.691160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>14.8</td>\n",
       "      <td>18373</td>\n",
       "      <td>21556.5</td>\n",
       "      <td>0.852318</td>\n",
       "      <td>661.612260</td>\n",
       "      <td>164</td>\n",
       "      <td>189</td>\n",
       "      <td>1317</td>\n",
       "      <td>29</td>\n",
       "      <td>176.040924</td>\n",
       "      <td>141.141724</td>\n",
       "      <td>22.056726</td>\n",
       "      <td>0.597653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>16.9</td>\n",
       "      <td>23374</td>\n",
       "      <td>26050.0</td>\n",
       "      <td>0.897274</td>\n",
       "      <td>683.754395</td>\n",
       "      <td>186</td>\n",
       "      <td>194</td>\n",
       "      <td>1427</td>\n",
       "      <td>31</td>\n",
       "      <td>191.379974</td>\n",
       "      <td>164.496170</td>\n",
       "      <td>175.959305</td>\n",
       "      <td>0.511091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>16.6</td>\n",
       "      <td>23457</td>\n",
       "      <td>25678.5</td>\n",
       "      <td>0.913488</td>\n",
       "      <td>671.754395</td>\n",
       "      <td>198</td>\n",
       "      <td>179</td>\n",
       "      <td>1364</td>\n",
       "      <td>31</td>\n",
       "      <td>194.815384</td>\n",
       "      <td>159.870819</td>\n",
       "      <td>122.656914</td>\n",
       "      <td>0.571464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>14.7</td>\n",
       "      <td>18533</td>\n",
       "      <td>22886.5</td>\n",
       "      <td>0.809779</td>\n",
       "      <td>821.595015</td>\n",
       "      <td>189</td>\n",
       "      <td>168</td>\n",
       "      <td>1376</td>\n",
       "      <td>22</td>\n",
       "      <td>193.579941</td>\n",
       "      <td>133.306320</td>\n",
       "      <td>123.732269</td>\n",
       "      <td>0.725106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     height_mm  plant_area  plant_convex_hull_area  plant_solidity  \\\n",
       "160       13.0       19084                 22974.0        0.830678   \n",
       "161       14.8       18373                 21556.5        0.852318   \n",
       "162       16.9       23374                 26050.0        0.897274   \n",
       "163       16.6       23457                 25678.5        0.913488   \n",
       "164       14.7       18533                 22886.5        0.809779   \n",
       "\n",
       "     plant_perimeter  plant_width  plant_height  plant_longest_path  \\\n",
       "160       767.938160          193           167                1367   \n",
       "161       661.612260          164           189                1317   \n",
       "162       683.754395          186           194                1427   \n",
       "163       671.754395          198           179                1364   \n",
       "164       821.595015          189           168                1376   \n",
       "\n",
       "     plant_convex_hull_vertices  plant_ellipse_major_axis  \\\n",
       "160                          33                190.580887   \n",
       "161                          29                176.040924   \n",
       "162                          31                191.379974   \n",
       "163                          31                194.815384   \n",
       "164                          22                193.579941   \n",
       "\n",
       "     plant_ellipse_minor_axis  plant_ellipse_angle  plant_ellipse_eccentricity  \n",
       "160                137.733093            62.138748                    0.691160  \n",
       "161                141.141724            22.056726                    0.597653  \n",
       "162                164.496170           175.959305                    0.511091  \n",
       "163                159.870819           122.656914                    0.571464  \n",
       "164                133.306320           123.732269                    0.725106  "
      ]
     },
     "execution_count": 890,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: feature engineering\n",
    "def drop_axis(keyword):\n",
    "    return dataset.drop(keyword, axis=1)\n",
    "\n",
    "dataset = drop_axis(\"date\")\n",
    "dataset = drop_axis(\"index\")\n",
    "dataset = drop_axis(\"plant_id\")\n",
    "dataset = drop_axis(\"tray_id\")\n",
    "dataset = drop_axis(\"row\")\n",
    "dataset = drop_axis(\"column\")\n",
    "dataset = drop_axis(\"LFW_g\")\n",
    "dataset = drop_axis(\"LDW_g\")\n",
    "dataset = drop_axis(\"LA_mm2\")\n",
    "dataset = drop_axis(\"width_mm\")\n",
    "dataset = drop_axis(\"length_mm\")\n",
    "dataset.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>height_mm</th>\n",
       "      <td>12.322424</td>\n",
       "      <td>4.501536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_area</th>\n",
       "      <td>14338.569697</td>\n",
       "      <td>12415.053296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_convex_hull_area</th>\n",
       "      <td>16600.530303</td>\n",
       "      <td>14801.909088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_solidity</th>\n",
       "      <td>0.855192</td>\n",
       "      <td>0.059827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_perimeter</th>\n",
       "      <td>570.369749</td>\n",
       "      <td>326.288761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_width</th>\n",
       "      <td>143.115152</td>\n",
       "      <td>72.628306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_height</th>\n",
       "      <td>129.575758</td>\n",
       "      <td>65.897292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_longest_path</th>\n",
       "      <td>1041.090909</td>\n",
       "      <td>537.448385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_convex_hull_vertices</th>\n",
       "      <td>27.418182</td>\n",
       "      <td>6.922104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_ellipse_major_axis</th>\n",
       "      <td>142.149979</td>\n",
       "      <td>74.902063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_ellipse_minor_axis</th>\n",
       "      <td>111.665869</td>\n",
       "      <td>56.159437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_ellipse_angle</th>\n",
       "      <td>91.036919</td>\n",
       "      <td>40.530153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plant_ellipse_eccentricity</th>\n",
       "      <td>0.590003</td>\n",
       "      <td>0.152607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    mean           std\n",
       "height_mm                      12.322424      4.501536\n",
       "plant_area                  14338.569697  12415.053296\n",
       "plant_convex_hull_area      16600.530303  14801.909088\n",
       "plant_solidity                  0.855192      0.059827\n",
       "plant_perimeter               570.369749    326.288761\n",
       "plant_width                   143.115152     72.628306\n",
       "plant_height                  129.575758     65.897292\n",
       "plant_longest_path           1041.090909    537.448385\n",
       "plant_convex_hull_vertices     27.418182      6.922104\n",
       "plant_ellipse_major_axis      142.149979     74.902063\n",
       "plant_ellipse_minor_axis      111.665869     56.159437\n",
       "plant_ellipse_angle            91.036919     40.530153\n",
       "plant_ellipse_eccentricity      0.590003      0.152607"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe().transpose()[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.80000000e+00, 1.21100000e+03, 1.45600000e+03, ...,\n",
       "        4.09659882e+01, 9.28785095e+01, 4.42607656e-01],\n",
       "       [5.50000000e+00, 1.41200000e+03, 1.55650000e+03, ...,\n",
       "        3.74846725e+01, 9.62554245e+01, 7.11801969e-01],\n",
       "       [8.90000000e+00, 1.30300000e+03, 1.76650000e+03, ...,\n",
       "        4.30591736e+01, 2.10217595e+00, 3.70421308e-01],\n",
       "       ...,\n",
       "       [1.69000000e+01, 2.33740000e+04, 2.60500000e+04, ...,\n",
       "        1.64496170e+02, 1.75959305e+02, 5.11091089e-01],\n",
       "       [1.66000000e+01, 2.34570000e+04, 2.56785000e+04, ...,\n",
       "        1.59870819e+02, 1.22656914e+02, 5.71463790e-01],\n",
       "       [1.47000000e+01, 1.85330000e+04, 2.28865000e+04, ...,\n",
       "        1.33306320e+02, 1.23732269e+02, 7.25106204e-01]])"
      ]
     },
     "execution_count": 892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(dataset)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2322424e+01 1.4338570e+04 1.6600531e+04 8.5519230e-01 5.7036981e+02\n",
      "  1.4311516e+02 1.2957578e+02 1.0410909e+03 2.7418180e+01 1.4214999e+02\n",
      "  1.1166587e+02 9.1036919e+01 5.9000260e-01]]\n"
     ]
    }
   ],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(X)\n",
    "print(normalizer.mean.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.3 ,   2.1 ,   3.36,   3.07,   3.17,   1.82,   3.12,   1.38,\n",
       "         2.03,   2.15,   3.54,   2.02,   1.19,   3.66,   1.47,   2.46,\n",
       "         1.79,   2.23,   2.54,   3.7 ,   3.  ,   2.14,   3.3 ,   2.29,\n",
       "         2.44,   3.23,   2.64,   3.46,   2.91,   3.34,   2.51,   2.58,\n",
       "         3.69,  10.95,   9.7 ,   9.2 ,  15.06,  12.86,  15.27,  11.26,\n",
       "        15.29,  12.11,  14.64,   6.3 ,  11.32,   8.74,   9.59,   3.95,\n",
       "         1.82,   8.66,  12.3 ,  12.4 ,  10.47,   8.98,   9.6 ,  13.69,\n",
       "        14.39,  13.44,  13.76,  13.33,   6.82,  12.42,   6.76,  10.29,\n",
       "         5.54,  16.44,   9.46,  12.24,   8.75,  15.56,  18.81,   9.89,\n",
       "        19.94,  11.04,   9.87,   9.98,   4.92,   5.49,   6.23,   7.21,\n",
       "         6.5 ,   6.22,   3.27,   7.94,  75.17, 137.06, 132.66, 140.07,\n",
       "       134.19, 107.92, 160.7 , 129.16, 176.79, 147.64, 134.12, 103.44,\n",
       "        99.95, 105.17, 100.88, 127.07, 107.53, 106.28, 110.52, 107.43,\n",
       "        88.11, 136.21,  65.15,  96.16,  87.82, 108.08,  68.2 , 133.8 ,\n",
       "       124.48,  52.92,  90.13, 102.17, 130.46, 158.31,  80.86, 110.57,\n",
       "       139.  , 127.61, 127.5 ,  82.4 , 143.88,  90.09, 156.82, 121.68,\n",
       "       101.91,  77.56, 158.65, 118.44, 131.86,  68.37, 101.73, 113.93,\n",
       "        59.36,  81.36,  99.42, 108.82, 119.75, 136.97, 144.34, 139.75,\n",
       "       146.06, 119.49, 116.  , 118.46, 120.81, 108.6 , 111.97,  84.39,\n",
       "       115.08, 111.65, 157.4 , 137.99, 112.16, 187.64, 109.96, 142.47,\n",
       "        57.96,  81.46, 140.84, 108.17,  64.4 ])"
      ]
     },
     "execution_count": 894,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(df)\n",
    "y = (y['LFW_g'])\n",
    "y.tail()\n",
    "y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.Sequential([\n",
    "        normalizer,\n",
    "        #tf.keras.layers.Dense(16, activation=\"linear\"),\n",
    "        #tf.keras.layers.Dense(64, activation='relu'),\n",
    "        #tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example: [[    5.8   1211.    1456.   ...    40.97    92.88     0.44]\n",
      " [    5.5   1412.    1556.5  ...    37.48    96.26     0.71]\n",
      " [    8.9   1303.    1766.5  ...    43.06     2.1      0.37]\n",
      " ...\n",
      " [   16.9  23374.   26050.   ...   164.5    175.96     0.51]\n",
      " [   16.6  23457.   25678.5  ...   159.87   122.66     0.57]\n",
      " [   14.7  18533.   22886.5  ...   133.31   123.73     0.73]]\n",
      "\n",
      "Normalized: [[-1.45 -1.06 -1.03 ... -1.26  0.05 -0.97]\n",
      " [-1.52 -1.04 -1.02 ... -1.32  0.13  0.8 ]\n",
      " [-0.76 -1.05 -1.01 ... -1.23 -2.2  -1.44]\n",
      " ...\n",
      " [ 1.02  0.73  0.64 ...  0.94  2.1  -0.52]\n",
      " [ 0.95  0.74  0.62 ...  0.86  0.78 -0.12]\n",
      " [ 0.53  0.34  0.43 ...  0.39  0.81  0.89]]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print('First example:', X)\n",
    "    print()\n",
    "    print('Normalized:', normalizer(X).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_70 (Normaliza  (None, 13)               27        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 1)                 14        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 14\n",
      "Non-trainable params: 27\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "5/5 [==============================] - 2s 41ms/step - loss: 5763.9111 - val_loss: 13086.5068\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 5297.5317 - val_loss: 12108.4072\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 4876.7974 - val_loss: 11189.3350\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4508.6997 - val_loss: 10335.9609\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4184.2334 - val_loss: 9560.5605\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3917.9929 - val_loss: 8859.0664\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3716.2632 - val_loss: 8217.7969\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3530.2695 - val_loss: 7665.5254\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3386.6353 - val_loss: 7184.6533\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3274.8535 - val_loss: 6764.9121\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3173.1858 - val_loss: 6434.1045\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3096.5520 - val_loss: 6151.5586\n",
      "Epoch 13/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3031.1965 - val_loss: 5903.8633\n",
      "Epoch 14/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2968.2419 - val_loss: 5702.3516\n",
      "Epoch 15/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2914.9250 - val_loss: 5521.9834\n",
      "Epoch 16/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2863.2678 - val_loss: 5368.8159\n",
      "Epoch 17/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2811.0029 - val_loss: 5248.6108\n",
      "Epoch 18/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2760.7612 - val_loss: 5158.5210\n",
      "Epoch 19/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2712.7166 - val_loss: 5065.6143\n",
      "Epoch 20/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2667.3752 - val_loss: 4977.1943\n",
      "Epoch 21/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2620.7517 - val_loss: 4919.0210\n",
      "Epoch 22/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2575.5022 - val_loss: 4823.0449\n",
      "Epoch 23/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2530.8774 - val_loss: 4772.5488\n",
      "Epoch 24/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2485.4656 - val_loss: 4698.0977\n",
      "Epoch 25/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2441.8525 - val_loss: 4612.1470\n",
      "Epoch 26/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2399.1328 - val_loss: 4524.4746\n",
      "Epoch 27/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2357.1831 - val_loss: 4442.7090\n",
      "Epoch 28/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2315.8577 - val_loss: 4387.5820\n",
      "Epoch 29/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2275.1367 - val_loss: 4339.6982\n",
      "Epoch 30/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2235.0874 - val_loss: 4292.9565\n",
      "Epoch 31/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2195.0103 - val_loss: 4236.4414\n",
      "Epoch 32/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2156.2991 - val_loss: 4194.1147\n",
      "Epoch 33/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2118.3689 - val_loss: 4148.5898\n",
      "Epoch 34/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2080.4348 - val_loss: 4077.3147\n",
      "Epoch 35/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2043.3857 - val_loss: 4019.2798\n",
      "Epoch 36/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2007.3721 - val_loss: 3949.4583\n",
      "Epoch 37/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1971.2561 - val_loss: 3901.2676\n",
      "Epoch 38/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1936.8090 - val_loss: 3812.7935\n",
      "Epoch 39/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1901.2356 - val_loss: 3771.3997\n",
      "Epoch 40/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1867.2078 - val_loss: 3698.0178\n",
      "Epoch 41/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1833.8473 - val_loss: 3636.1868\n",
      "Epoch 42/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1800.3817 - val_loss: 3608.3171\n",
      "Epoch 43/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1767.5066 - val_loss: 3557.8293\n",
      "Epoch 44/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1735.6798 - val_loss: 3499.6379\n",
      "Epoch 45/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1704.2850 - val_loss: 3467.2012\n",
      "Epoch 46/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1673.3057 - val_loss: 3440.3494\n",
      "Epoch 47/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1642.7098 - val_loss: 3390.6362\n",
      "Epoch 48/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1612.4236 - val_loss: 3333.7632\n",
      "Epoch 49/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1583.3521 - val_loss: 3276.4565\n",
      "Epoch 50/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1554.8018 - val_loss: 3244.8911\n",
      "Epoch 51/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1526.2113 - val_loss: 3200.0659\n",
      "Epoch 52/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1498.0591 - val_loss: 3158.4297\n",
      "Epoch 53/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1470.6516 - val_loss: 3111.8450\n",
      "Epoch 54/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1443.7795 - val_loss: 3048.0903\n",
      "Epoch 55/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1417.2126 - val_loss: 3006.1521\n",
      "Epoch 56/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1391.1362 - val_loss: 2955.9189\n",
      "Epoch 57/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1365.2532 - val_loss: 2913.4053\n",
      "Epoch 58/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1340.6033 - val_loss: 2890.7986\n",
      "Epoch 59/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1315.8789 - val_loss: 2854.7161\n",
      "Epoch 60/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1291.2418 - val_loss: 2802.8604\n",
      "Epoch 61/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1267.1173 - val_loss: 2757.6431\n",
      "Epoch 62/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1243.4604 - val_loss: 2726.0908\n",
      "Epoch 63/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1221.3489 - val_loss: 2695.3228\n",
      "Epoch 64/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1198.2417 - val_loss: 2664.1006\n",
      "Epoch 65/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1175.5452 - val_loss: 2619.3435\n",
      "Epoch 66/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1153.1501 - val_loss: 2565.9551\n",
      "Epoch 67/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1131.7166 - val_loss: 2517.9060\n",
      "Epoch 68/500\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1111.7031 - val_loss: 2453.8811\n",
      "Epoch 69/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1092.1909 - val_loss: 2387.6558\n",
      "Epoch 70/500\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1072.0797 - val_loss: 2347.4099\n",
      "Epoch 71/500\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 1050.4395 - val_loss: 2339.9290\n",
      "Epoch 72/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1030.1630 - val_loss: 2337.8057\n",
      "Epoch 73/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1010.9210 - val_loss: 2332.0984\n",
      "Epoch 74/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 993.3708 - val_loss: 2323.4824\n",
      "Epoch 75/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 973.8931 - val_loss: 2287.7559\n",
      "Epoch 76/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 955.3618 - val_loss: 2225.7615\n",
      "Epoch 77/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 938.0840 - val_loss: 2177.6501\n",
      "Epoch 78/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 920.0184 - val_loss: 2142.6230\n",
      "Epoch 79/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 902.5933 - val_loss: 2112.8042\n",
      "Epoch 80/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 886.0765 - val_loss: 2082.2183\n",
      "Epoch 81/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 870.5241 - val_loss: 2041.8536\n",
      "Epoch 82/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 854.3151 - val_loss: 2009.4795\n",
      "Epoch 83/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 838.4222 - val_loss: 2002.4349\n",
      "Epoch 84/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 822.4520 - val_loss: 1980.7139\n",
      "Epoch 85/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 808.1892 - val_loss: 1967.5795\n",
      "Epoch 86/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 793.4751 - val_loss: 1928.9982\n",
      "Epoch 87/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 778.9022 - val_loss: 1908.2628\n",
      "Epoch 88/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 764.1607 - val_loss: 1874.1995\n",
      "Epoch 89/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 750.4142 - val_loss: 1840.9929\n",
      "Epoch 90/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 736.6647 - val_loss: 1818.4952\n",
      "Epoch 91/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 723.7053 - val_loss: 1796.8007\n",
      "Epoch 92/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 710.5759 - val_loss: 1784.2023\n",
      "Epoch 93/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 698.1292 - val_loss: 1763.8466\n",
      "Epoch 94/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 685.5834 - val_loss: 1749.5090\n",
      "Epoch 95/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 674.4124 - val_loss: 1739.8325\n",
      "Epoch 96/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 661.8135 - val_loss: 1707.1904\n",
      "Epoch 97/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 649.5927 - val_loss: 1671.5912\n",
      "Epoch 98/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 637.7868 - val_loss: 1631.9746\n",
      "Epoch 99/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 627.0729 - val_loss: 1600.3927\n",
      "Epoch 100/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 615.8167 - val_loss: 1572.4541\n",
      "Epoch 101/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 605.0041 - val_loss: 1538.2277\n",
      "Epoch 102/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 594.5757 - val_loss: 1523.8867\n",
      "Epoch 103/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 585.2849 - val_loss: 1494.0532\n",
      "Epoch 104/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 574.2325 - val_loss: 1494.4862\n",
      "Epoch 105/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 565.2335 - val_loss: 1493.8528\n",
      "Epoch 106/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 554.0225 - val_loss: 1465.2505\n",
      "Epoch 107/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 544.4922 - val_loss: 1437.4617\n",
      "Epoch 108/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 536.9595 - val_loss: 1402.5339\n",
      "Epoch 109/500\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 526.4775 - val_loss: 1385.3896\n",
      "Epoch 110/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 517.7892 - val_loss: 1384.5634\n",
      "Epoch 111/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 508.9624 - val_loss: 1368.7310\n",
      "Epoch 112/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 500.9412 - val_loss: 1356.3240\n",
      "Epoch 113/500\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 492.2375 - val_loss: 1341.4965\n",
      "Epoch 114/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 484.6468 - val_loss: 1327.2856\n",
      "Epoch 115/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 476.7399 - val_loss: 1294.7886\n",
      "Epoch 116/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 468.7699 - val_loss: 1279.7268\n",
      "Epoch 117/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 461.0818 - val_loss: 1261.4390\n",
      "Epoch 118/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 454.1000 - val_loss: 1231.5940\n",
      "Epoch 119/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 446.9800 - val_loss: 1218.0649\n",
      "Epoch 120/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 440.4352 - val_loss: 1201.5004\n",
      "Epoch 121/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 432.5928 - val_loss: 1197.7137\n",
      "Epoch 122/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 425.8040 - val_loss: 1189.3744\n",
      "Epoch 123/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 419.2458 - val_loss: 1189.0388\n",
      "Epoch 124/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 412.6387 - val_loss: 1185.7307\n",
      "Epoch 125/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 406.9839 - val_loss: 1175.2523\n",
      "Epoch 126/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 400.9747 - val_loss: 1161.5811\n",
      "Epoch 127/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 394.9096 - val_loss: 1132.0812\n",
      "Epoch 128/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 388.7437 - val_loss: 1108.2994\n",
      "Epoch 129/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 383.4011 - val_loss: 1092.7321\n",
      "Epoch 130/500\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 377.8107 - val_loss: 1061.3861\n",
      "Epoch 131/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 372.1241 - val_loss: 1048.3701\n",
      "Epoch 132/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 367.8058 - val_loss: 1048.4573\n",
      "Epoch 133/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 361.7173 - val_loss: 1033.7351\n",
      "Epoch 134/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 356.4279 - val_loss: 1020.9243\n",
      "Epoch 135/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 351.7796 - val_loss: 1004.2362\n",
      "Epoch 136/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 347.0906 - val_loss: 992.1343\n",
      "Epoch 137/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 341.9991 - val_loss: 993.7878\n",
      "Epoch 138/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 337.3366 - val_loss: 995.9798\n",
      "Epoch 139/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 333.4427 - val_loss: 996.1000\n",
      "Epoch 140/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 329.0546 - val_loss: 988.7319\n",
      "Epoch 141/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 324.8249 - val_loss: 978.7719\n",
      "Epoch 142/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 321.0699 - val_loss: 964.5056\n",
      "Epoch 143/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 316.9778 - val_loss: 951.2617\n",
      "Epoch 144/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 313.0755 - val_loss: 929.9483\n",
      "Epoch 145/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 309.5641 - val_loss: 911.6967\n",
      "Epoch 146/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 306.0310 - val_loss: 898.3501\n",
      "Epoch 147/500\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 302.2955 - val_loss: 892.4807\n",
      "Epoch 148/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 298.5819 - val_loss: 894.7993\n",
      "Epoch 149/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 295.5461 - val_loss: 882.5845\n",
      "Epoch 150/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 292.0445 - val_loss: 882.5660\n",
      "Epoch 151/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 288.5931 - val_loss: 880.4233\n",
      "Epoch 152/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 285.5673 - val_loss: 866.3083\n",
      "Epoch 153/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 282.4296 - val_loss: 858.4592\n",
      "Epoch 154/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 279.2702 - val_loss: 844.8191\n",
      "Epoch 155/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 276.2847 - val_loss: 831.2985\n",
      "Epoch 156/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 273.2021 - val_loss: 821.4980\n",
      "Epoch 157/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 270.7462 - val_loss: 807.7720\n",
      "Epoch 158/500\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 267.7653 - val_loss: 804.5601\n",
      "Epoch 159/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 265.3641 - val_loss: 799.2554\n",
      "Epoch 160/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 262.7497 - val_loss: 792.7627\n",
      "Epoch 161/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 260.3945 - val_loss: 789.4273\n",
      "Epoch 162/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 257.9452 - val_loss: 773.4758\n",
      "Epoch 163/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 255.1799 - val_loss: 769.5588\n",
      "Epoch 164/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 253.0740 - val_loss: 770.3572\n",
      "Epoch 165/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 250.6275 - val_loss: 761.0863\n",
      "Epoch 166/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 248.2981 - val_loss: 758.0461\n",
      "Epoch 167/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 246.3718 - val_loss: 750.2535\n",
      "Epoch 168/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 244.4836 - val_loss: 740.2322\n",
      "Epoch 169/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 241.9339 - val_loss: 741.8908\n",
      "Epoch 170/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 240.1673 - val_loss: 741.4348\n",
      "Epoch 171/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 238.2052 - val_loss: 737.6831\n",
      "Epoch 172/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 236.4262 - val_loss: 729.9791\n",
      "Epoch 173/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 234.9486 - val_loss: 712.8795\n",
      "Epoch 174/500\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 232.5728 - val_loss: 706.7176\n",
      "Epoch 175/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 230.9537 - val_loss: 700.2621\n",
      "Epoch 176/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 229.9900 - val_loss: 682.3292\n",
      "Epoch 177/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 227.5106 - val_loss: 680.1778\n",
      "Epoch 178/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 226.0803 - val_loss: 674.1562\n",
      "Epoch 179/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 224.2019 - val_loss: 675.6802\n",
      "Epoch 180/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 223.1135 - val_loss: 681.5338\n",
      "Epoch 181/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 221.5486 - val_loss: 678.8561\n",
      "Epoch 182/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 219.9349 - val_loss: 674.2272\n",
      "Epoch 183/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 218.4129 - val_loss: 666.9788\n",
      "Epoch 184/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 216.9439 - val_loss: 658.4924\n",
      "Epoch 185/500\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 215.4465 - val_loss: 647.1393\n",
      "Epoch 186/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 214.0978 - val_loss: 640.8555\n",
      "Epoch 187/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 213.1965 - val_loss: 629.4167\n",
      "Epoch 188/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 211.7284 - val_loss: 627.1938\n",
      "Epoch 189/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 210.5142 - val_loss: 623.8253\n",
      "Epoch 190/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 209.3232 - val_loss: 622.3763\n",
      "Epoch 191/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 208.6796 - val_loss: 627.4557\n",
      "Epoch 192/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 207.2718 - val_loss: 623.1290\n",
      "Epoch 193/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 207.0434 - val_loss: 625.5461\n",
      "Epoch 194/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 204.8999 - val_loss: 617.0102\n",
      "Epoch 195/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 203.8238 - val_loss: 609.8395\n",
      "Epoch 196/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 202.7752 - val_loss: 603.5067\n",
      "Epoch 197/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 201.6674 - val_loss: 595.9312\n",
      "Epoch 198/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 200.7739 - val_loss: 592.8833\n",
      "Epoch 199/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 200.3377 - val_loss: 581.2103\n",
      "Epoch 200/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 199.0040 - val_loss: 580.0522\n",
      "Epoch 201/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 198.2560 - val_loss: 580.2573\n",
      "Epoch 202/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 197.3437 - val_loss: 574.4094\n",
      "Epoch 203/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 196.3257 - val_loss: 577.7949\n",
      "Epoch 204/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 195.1697 - val_loss: 576.0174\n",
      "Epoch 205/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 194.3270 - val_loss: 571.6556\n",
      "Epoch 206/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 193.4986 - val_loss: 567.2133\n",
      "Epoch 207/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 192.7077 - val_loss: 563.6967\n",
      "Epoch 208/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 192.1463 - val_loss: 558.9324\n",
      "Epoch 209/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 191.0759 - val_loss: 557.8999\n",
      "Epoch 210/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 191.0078 - val_loss: 561.8091\n",
      "Epoch 211/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 189.7684 - val_loss: 552.8394\n",
      "Epoch 212/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 188.9869 - val_loss: 547.1974\n",
      "Epoch 213/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 188.1997 - val_loss: 542.1499\n",
      "Epoch 214/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 187.5007 - val_loss: 538.5861\n",
      "Epoch 215/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 186.6669 - val_loss: 539.0516\n",
      "Epoch 216/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 186.0514 - val_loss: 540.1487\n",
      "Epoch 217/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 185.5019 - val_loss: 536.7556\n",
      "Epoch 218/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 184.7236 - val_loss: 537.4498\n",
      "Epoch 219/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 184.1666 - val_loss: 537.7352\n",
      "Epoch 220/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 183.8459 - val_loss: 541.5378\n",
      "Epoch 221/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 183.2867 - val_loss: 538.4966\n",
      "Epoch 222/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 182.4105 - val_loss: 531.0209\n",
      "Epoch 223/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 181.6399 - val_loss: 525.8657\n",
      "Epoch 224/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 180.9425 - val_loss: 519.4922\n",
      "Epoch 225/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 180.4780 - val_loss: 513.9401\n",
      "Epoch 226/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 179.9257 - val_loss: 512.8193\n",
      "Epoch 227/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 179.2808 - val_loss: 508.2197\n",
      "Epoch 228/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 178.9539 - val_loss: 508.1352\n",
      "Epoch 229/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 178.2351 - val_loss: 505.7369\n",
      "Epoch 230/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 177.6887 - val_loss: 504.3730\n",
      "Epoch 231/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 177.2434 - val_loss: 500.6935\n",
      "Epoch 232/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 176.9298 - val_loss: 504.1420\n",
      "Epoch 233/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 176.2784 - val_loss: 504.3651\n",
      "Epoch 234/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 175.7491 - val_loss: 497.1497\n",
      "Epoch 235/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 175.1510 - val_loss: 492.7213\n",
      "Epoch 236/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 174.7390 - val_loss: 489.1715\n",
      "Epoch 237/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 174.2986 - val_loss: 488.1662\n",
      "Epoch 238/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 173.7186 - val_loss: 488.9978\n",
      "Epoch 239/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 173.7304 - val_loss: 496.0397\n",
      "Epoch 240/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 173.2646 - val_loss: 496.8870\n",
      "Epoch 241/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 173.1286 - val_loss: 485.7943\n",
      "Epoch 242/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 172.0844 - val_loss: 481.2201\n",
      "Epoch 243/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 171.4962 - val_loss: 483.7655\n",
      "Epoch 244/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 171.1100 - val_loss: 484.6508\n",
      "Epoch 245/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 170.7761 - val_loss: 478.0136\n",
      "Epoch 246/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 170.4317 - val_loss: 472.8741\n",
      "Epoch 247/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 169.7407 - val_loss: 472.1554\n",
      "Epoch 248/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 169.4063 - val_loss: 473.6132\n",
      "Epoch 249/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 169.1254 - val_loss: 469.7503\n",
      "Epoch 250/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 168.5153 - val_loss: 468.4902\n",
      "Epoch 251/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 168.1401 - val_loss: 466.8227\n",
      "Epoch 252/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 168.0139 - val_loss: 464.6299\n",
      "Epoch 253/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 167.4257 - val_loss: 469.1708\n",
      "Epoch 254/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 167.0473 - val_loss: 466.7937\n",
      "Epoch 255/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 166.6115 - val_loss: 464.9728\n",
      "Epoch 256/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 166.3757 - val_loss: 464.8698\n",
      "Epoch 257/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 165.8679 - val_loss: 460.1576\n",
      "Epoch 258/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 166.3001 - val_loss: 452.3851\n",
      "Epoch 259/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 164.9826 - val_loss: 453.6907\n",
      "Epoch 260/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 164.7491 - val_loss: 456.5229\n",
      "Epoch 261/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 164.7615 - val_loss: 458.4269\n",
      "Epoch 262/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 164.1391 - val_loss: 455.6052\n",
      "Epoch 263/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 163.9805 - val_loss: 448.6418\n",
      "Epoch 264/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 163.3757 - val_loss: 449.3165\n",
      "Epoch 265/500\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 163.2189 - val_loss: 445.0826\n",
      "Epoch 266/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 162.6685 - val_loss: 444.3079\n",
      "Epoch 267/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 162.2924 - val_loss: 444.6366\n",
      "Epoch 268/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 162.0185 - val_loss: 443.1711\n",
      "Epoch 269/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 161.5915 - val_loss: 443.8936\n",
      "Epoch 270/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 161.7643 - val_loss: 441.2097\n",
      "Epoch 271/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 161.1323 - val_loss: 444.8960\n",
      "Epoch 272/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 160.8561 - val_loss: 442.8845\n",
      "Epoch 273/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 160.2621 - val_loss: 438.5310\n",
      "Epoch 274/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 159.9699 - val_loss: 432.6670\n",
      "Epoch 275/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 160.0730 - val_loss: 428.1846\n",
      "Epoch 276/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 159.6530 - val_loss: 430.7957\n",
      "Epoch 277/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 159.2375 - val_loss: 435.9110\n",
      "Epoch 278/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 158.8898 - val_loss: 437.5185\n",
      "Epoch 279/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 158.9702 - val_loss: 439.3627\n",
      "Epoch 280/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 158.4076 - val_loss: 432.6498\n",
      "Epoch 281/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 158.9181 - val_loss: 422.6211\n",
      "Epoch 282/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 157.7318 - val_loss: 422.8394\n",
      "Epoch 283/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 157.3095 - val_loss: 425.1512\n",
      "Epoch 284/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 157.2783 - val_loss: 427.3179\n",
      "Epoch 285/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 157.3201 - val_loss: 433.0794\n",
      "Epoch 286/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 156.6310 - val_loss: 428.4675\n",
      "Epoch 287/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 156.2933 - val_loss: 424.9721\n",
      "Epoch 288/500\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 155.8211 - val_loss: 417.2380\n",
      "Epoch 289/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 155.7229 - val_loss: 411.6896\n",
      "Epoch 290/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 155.8218 - val_loss: 408.7080\n",
      "Epoch 291/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 155.4669 - val_loss: 411.8074\n",
      "Epoch 292/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 154.7345 - val_loss: 412.1391\n",
      "Epoch 293/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 154.4543 - val_loss: 413.9697\n",
      "Epoch 294/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 154.1084 - val_loss: 415.9419\n",
      "Epoch 295/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 153.9894 - val_loss: 414.9072\n",
      "Epoch 296/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 153.4533 - val_loss: 417.7666\n",
      "Epoch 297/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 154.2853 - val_loss: 422.1350\n",
      "Epoch 298/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 153.1539 - val_loss: 414.2415\n",
      "Epoch 299/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 152.7180 - val_loss: 407.9836\n",
      "Epoch 300/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 152.9701 - val_loss: 402.6186\n",
      "Epoch 301/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 152.3179 - val_loss: 401.6743\n",
      "Epoch 302/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 152.0400 - val_loss: 404.0011\n",
      "Epoch 303/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 153.1421 - val_loss: 413.8367\n",
      "Epoch 304/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 151.4899 - val_loss: 409.2585\n",
      "Epoch 305/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 151.0818 - val_loss: 406.5032\n",
      "Epoch 306/500\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 150.8886 - val_loss: 401.8445\n",
      "Epoch 307/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 150.9192 - val_loss: 398.5341\n",
      "Epoch 308/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 150.2734 - val_loss: 402.8464\n",
      "Epoch 309/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 150.2465 - val_loss: 408.1180\n",
      "Epoch 310/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 150.0827 - val_loss: 403.5789\n",
      "Epoch 311/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 149.6096 - val_loss: 403.1114\n",
      "Epoch 312/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 149.3233 - val_loss: 404.5979\n",
      "Epoch 313/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 149.0989 - val_loss: 402.3852\n",
      "Epoch 314/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 148.9002 - val_loss: 399.2637\n",
      "Epoch 315/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 148.6559 - val_loss: 399.0239\n",
      "Epoch 316/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 148.5647 - val_loss: 392.8436\n",
      "Epoch 317/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 148.1112 - val_loss: 394.4129\n",
      "Epoch 318/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 148.2386 - val_loss: 390.9732\n",
      "Epoch 319/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 147.4515 - val_loss: 396.9389\n",
      "Epoch 320/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 147.4883 - val_loss: 399.9294\n",
      "Epoch 321/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 147.1388 - val_loss: 397.4672\n",
      "Epoch 322/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 146.8234 - val_loss: 393.1032\n",
      "Epoch 323/500\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 146.5664 - val_loss: 390.4794\n",
      "Epoch 324/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 146.4516 - val_loss: 392.6115\n",
      "Epoch 325/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 146.1047 - val_loss: 392.4798\n",
      "Epoch 326/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 145.7774 - val_loss: 388.4477\n",
      "Epoch 327/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 145.8783 - val_loss: 382.8783\n",
      "Epoch 328/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 145.4889 - val_loss: 385.2960\n",
      "Epoch 329/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 145.2167 - val_loss: 384.9715\n",
      "Epoch 330/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 145.2073 - val_loss: 389.1925\n",
      "Epoch 331/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 144.7373 - val_loss: 387.6938\n",
      "Epoch 332/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 144.4528 - val_loss: 386.4958\n",
      "Epoch 333/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 144.1932 - val_loss: 387.1398\n",
      "Epoch 334/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 144.8524 - val_loss: 382.3477\n",
      "Epoch 335/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 143.5797 - val_loss: 386.7391\n",
      "Epoch 336/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 143.5191 - val_loss: 387.0192\n",
      "Epoch 337/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 143.5249 - val_loss: 386.1737\n",
      "Epoch 338/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 143.4049 - val_loss: 380.7176\n",
      "Epoch 339/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 142.8810 - val_loss: 381.6175\n",
      "Epoch 340/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 142.6732 - val_loss: 378.4580\n",
      "Epoch 341/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 142.3994 - val_loss: 378.7793\n",
      "Epoch 342/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 142.1210 - val_loss: 378.8250\n",
      "Epoch 343/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 141.9459 - val_loss: 379.5815\n",
      "Epoch 344/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 141.7223 - val_loss: 378.8743\n",
      "Epoch 345/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 141.5289 - val_loss: 379.3302\n",
      "Epoch 346/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 141.3952 - val_loss: 379.0216\n",
      "Epoch 347/500\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 141.1551 - val_loss: 371.9452\n",
      "Epoch 348/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 140.8599 - val_loss: 370.2420\n",
      "Epoch 349/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 141.0854 - val_loss: 367.0825\n",
      "Epoch 350/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 140.6065 - val_loss: 368.2509\n",
      "Epoch 351/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 140.2188 - val_loss: 371.5243\n",
      "Epoch 352/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 140.0114 - val_loss: 376.9222\n",
      "Epoch 353/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 140.1304 - val_loss: 378.4202\n",
      "Epoch 354/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 139.6033 - val_loss: 375.7160\n",
      "Epoch 355/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 139.2714 - val_loss: 371.6395\n",
      "Epoch 356/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 139.4326 - val_loss: 366.1420\n",
      "Epoch 357/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 138.9774 - val_loss: 368.1850\n",
      "Epoch 358/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 139.6007 - val_loss: 373.9768\n",
      "Epoch 359/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 138.5246 - val_loss: 367.6422\n",
      "Epoch 360/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 138.4805 - val_loss: 363.5464\n",
      "Epoch 361/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 138.2417 - val_loss: 361.2246\n",
      "Epoch 362/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 138.0870 - val_loss: 360.5437\n",
      "Epoch 363/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 137.9719 - val_loss: 366.4807\n",
      "Epoch 364/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 138.0072 - val_loss: 369.0992\n",
      "Epoch 365/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 137.1536 - val_loss: 364.9192\n",
      "Epoch 366/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 136.9304 - val_loss: 359.8058\n",
      "Epoch 367/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 136.9910 - val_loss: 356.3216\n",
      "Epoch 368/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 137.1083 - val_loss: 355.3857\n",
      "Epoch 369/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 137.1787 - val_loss: 362.8314\n",
      "Epoch 370/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 136.2592 - val_loss: 362.1121\n",
      "Epoch 371/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 136.5264 - val_loss: 366.7324\n",
      "Epoch 372/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 136.0295 - val_loss: 364.1926\n",
      "Epoch 373/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 135.7642 - val_loss: 360.1936\n",
      "Epoch 374/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 135.9035 - val_loss: 355.4589\n",
      "Epoch 375/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 135.3770 - val_loss: 359.6882\n",
      "Epoch 376/500\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 135.2384 - val_loss: 361.1180\n",
      "Epoch 377/500\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 134.9731 - val_loss: 357.9908\n",
      "Epoch 378/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 134.8080 - val_loss: 355.4912\n",
      "Epoch 379/500\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 134.6156 - val_loss: 354.8246\n",
      "Epoch 380/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 134.4071 - val_loss: 357.1229\n",
      "Epoch 381/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 134.4887 - val_loss: 355.0711\n",
      "Epoch 382/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 134.0392 - val_loss: 359.5851\n",
      "Epoch 383/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 133.8699 - val_loss: 357.9467\n",
      "Epoch 384/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 133.7370 - val_loss: 357.2818\n",
      "Epoch 385/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 133.9412 - val_loss: 361.4131\n",
      "Epoch 386/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 133.5116 - val_loss: 356.0452\n",
      "Epoch 387/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 133.2457 - val_loss: 351.6180\n",
      "Epoch 388/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 133.5703 - val_loss: 345.4362\n",
      "Epoch 389/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 132.9585 - val_loss: 351.9130\n",
      "Epoch 390/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 132.4668 - val_loss: 352.8497\n",
      "Epoch 391/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 132.3749 - val_loss: 354.5485\n",
      "Epoch 392/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 133.3018 - val_loss: 363.1200\n",
      "Epoch 393/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 132.5681 - val_loss: 356.6663\n",
      "Epoch 394/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 131.8372 - val_loss: 352.6386\n",
      "Epoch 395/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 131.6031 - val_loss: 351.4770\n",
      "Epoch 396/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 131.5321 - val_loss: 349.3377\n",
      "Epoch 397/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 131.3057 - val_loss: 345.4829\n",
      "Epoch 398/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 131.5984 - val_loss: 339.7479\n",
      "Epoch 399/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 131.0280 - val_loss: 343.1250\n",
      "Epoch 400/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 130.7957 - val_loss: 347.7824\n",
      "Epoch 401/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 130.6441 - val_loss: 347.4628\n",
      "Epoch 402/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 130.4936 - val_loss: 348.0440\n",
      "Epoch 403/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 130.3037 - val_loss: 346.8153\n",
      "Epoch 404/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 130.1834 - val_loss: 342.1340\n",
      "Epoch 405/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 130.1178 - val_loss: 339.7648\n",
      "Epoch 406/500\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 129.9323 - val_loss: 342.9426\n",
      "Epoch 407/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 129.6903 - val_loss: 342.1241\n",
      "Epoch 408/500\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 129.3489 - val_loss: 339.1938\n",
      "Epoch 409/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 129.2154 - val_loss: 338.7476\n",
      "Epoch 410/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 129.1267 - val_loss: 339.6419\n",
      "Epoch 411/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 128.8683 - val_loss: 343.2801\n",
      "Epoch 412/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 128.7152 - val_loss: 342.8579\n",
      "Epoch 413/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 128.5556 - val_loss: 341.8685\n",
      "Epoch 414/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 128.2143 - val_loss: 344.4688\n",
      "Epoch 415/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 128.4192 - val_loss: 345.0094\n",
      "Epoch 416/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 128.0519 - val_loss: 345.1521\n",
      "Epoch 417/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 127.9560 - val_loss: 342.9276\n",
      "Epoch 418/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 127.7982 - val_loss: 340.6594\n",
      "Epoch 419/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 127.6675 - val_loss: 337.5620\n",
      "Epoch 420/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 127.6942 - val_loss: 336.4423\n",
      "Epoch 421/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 127.3046 - val_loss: 340.0764\n",
      "Epoch 422/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 127.1937 - val_loss: 339.0965\n",
      "Epoch 423/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 126.9992 - val_loss: 339.7091\n",
      "Epoch 424/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 127.3041 - val_loss: 344.5461\n",
      "Epoch 425/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 126.7829 - val_loss: 338.4097\n",
      "Epoch 426/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 127.1815 - val_loss: 332.1221\n",
      "Epoch 427/500\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 126.2382 - val_loss: 333.9308\n",
      "Epoch 428/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 126.0235 - val_loss: 336.9552\n",
      "Epoch 429/500\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 126.2756 - val_loss: 342.2228\n",
      "Epoch 430/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 126.1130 - val_loss: 338.5175\n",
      "Epoch 431/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 126.6228 - val_loss: 330.0465\n",
      "Epoch 432/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 125.4944 - val_loss: 332.6178\n",
      "Epoch 433/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 125.2695 - val_loss: 335.6615\n",
      "Epoch 434/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 125.1844 - val_loss: 337.1159\n",
      "Epoch 435/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 125.0415 - val_loss: 337.3075\n",
      "Epoch 436/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 125.1857 - val_loss: 338.4913\n",
      "Epoch 437/500\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 124.8536 - val_loss: 334.6975\n",
      "Epoch 438/500\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 124.5147 - val_loss: 328.5582\n",
      "Epoch 439/500\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 124.8719 - val_loss: 325.6051\n",
      "Epoch 440/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 124.3150 - val_loss: 330.6009\n",
      "Epoch 441/500\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 124.2006 - val_loss: 335.9834\n",
      "Epoch 442/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 123.9576 - val_loss: 337.1580\n",
      "Epoch 443/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 124.1237 - val_loss: 337.2479\n",
      "Epoch 444/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 123.6018 - val_loss: 330.1286\n",
      "Epoch 445/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 123.4737 - val_loss: 325.0268\n",
      "Epoch 446/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 123.7041 - val_loss: 323.7457\n",
      "Epoch 447/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 123.3783 - val_loss: 325.7397\n",
      "Epoch 448/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 123.0824 - val_loss: 332.8049\n",
      "Epoch 449/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 123.2021 - val_loss: 337.4383\n",
      "Epoch 450/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 122.9125 - val_loss: 332.5790\n",
      "Epoch 451/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 122.5949 - val_loss: 328.8423\n",
      "Epoch 452/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 122.4780 - val_loss: 328.2730\n",
      "Epoch 453/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 122.4432 - val_loss: 325.7224\n",
      "Epoch 454/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 122.1818 - val_loss: 325.1670\n",
      "Epoch 455/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 122.2741 - val_loss: 328.2708\n",
      "Epoch 456/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 121.9866 - val_loss: 326.6937\n",
      "Epoch 457/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 121.7322 - val_loss: 324.8165\n",
      "Epoch 458/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 121.6557 - val_loss: 325.4316\n",
      "Epoch 459/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 121.4414 - val_loss: 323.7580\n",
      "Epoch 460/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 121.7981 - val_loss: 326.3151\n",
      "Epoch 461/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 121.2506 - val_loss: 322.3523\n",
      "Epoch 462/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 121.3486 - val_loss: 321.8933\n",
      "Epoch 463/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 120.9262 - val_loss: 326.3176\n",
      "Epoch 464/500\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 120.7054 - val_loss: 328.1476\n",
      "Epoch 465/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 120.7853 - val_loss: 331.5901\n",
      "Epoch 466/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 120.7194 - val_loss: 328.9435\n",
      "Epoch 467/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 120.3040 - val_loss: 322.6628\n",
      "Epoch 468/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 120.3374 - val_loss: 318.3408\n",
      "Epoch 469/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 120.2538 - val_loss: 320.7418\n",
      "Epoch 470/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 120.7305 - val_loss: 318.0640\n",
      "Epoch 471/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 120.5088 - val_loss: 327.2990\n",
      "Epoch 472/500\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 119.7705 - val_loss: 325.6253\n",
      "Epoch 473/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 119.6454 - val_loss: 326.0421\n",
      "Epoch 474/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 119.6323 - val_loss: 322.8918\n",
      "Epoch 475/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 119.3862 - val_loss: 322.0667\n",
      "Epoch 476/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 119.5524 - val_loss: 325.9745\n",
      "Epoch 477/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 119.2653 - val_loss: 322.7937\n",
      "Epoch 478/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 118.9676 - val_loss: 322.5015\n",
      "Epoch 479/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 118.9637 - val_loss: 322.8845\n",
      "Epoch 480/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 118.6363 - val_loss: 318.8221\n",
      "Epoch 481/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 118.5792 - val_loss: 317.3358\n",
      "Epoch 482/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 118.4239 - val_loss: 319.8770\n",
      "Epoch 483/500\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 118.2700 - val_loss: 321.2130\n",
      "Epoch 484/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 118.1014 - val_loss: 323.3524\n",
      "Epoch 485/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 118.2211 - val_loss: 323.7267\n",
      "Epoch 486/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 117.9140 - val_loss: 319.3997\n",
      "Epoch 487/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 118.0054 - val_loss: 313.3355\n",
      "Epoch 488/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 117.7601 - val_loss: 315.3594\n",
      "Epoch 489/500\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 117.8493 - val_loss: 313.3587\n",
      "Epoch 490/500\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 117.6894 - val_loss: 319.4336\n",
      "Epoch 491/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 117.2478 - val_loss: 318.9642\n",
      "Epoch 492/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 117.1132 - val_loss: 319.7942\n",
      "Epoch 493/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 117.2646 - val_loss: 314.7414\n",
      "Epoch 494/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 117.1137 - val_loss: 318.0351\n",
      "Epoch 495/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 116.8214 - val_loss: 316.2502\n",
      "Epoch 496/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 116.6244 - val_loss: 313.8354\n",
      "Epoch 497/500\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 116.6427 - val_loss: 313.5725\n",
      "Epoch 498/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 116.3895 - val_loss: 314.0929\n",
      "Epoch 499/500\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 116.2662 - val_loss: 315.4399\n",
      "Epoch 500/500\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 116.2302 - val_loss: 315.4066\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs=500,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>116.624352</td>\n",
       "      <td>313.835449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>116.642738</td>\n",
       "      <td>313.572479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>116.389519</td>\n",
       "      <td>314.092865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>116.266167</td>\n",
       "      <td>315.439911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>116.230217</td>\n",
       "      <td>315.406586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           loss    val_loss\n",
       "495  116.624352  313.835449\n",
       "496  116.642738  313.572479\n",
       "497  116.389519  314.092865\n",
       "498  116.266167  315.439911\n",
       "499  116.230217  315.406586"
      ]
     },
     "execution_count": 900,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.DataFrame(history.history)\n",
    "history.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

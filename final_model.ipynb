{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 150\n",
    "LOSS_FUNCTION = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loss = []\n",
    "X_epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_workflow(input_data, target_value):\n",
    "    X = input_data.drop([target_value], axis=1)\n",
    "    y = input_data[target_value]\n",
    "\n",
    "    #print(X)\n",
    "    #print(y)\n",
    "\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    X = torch.from_numpy(X)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    data = TensorDataset(X, y)\n",
    "\n",
    "    train_ds, test_ds = train_test_split(data, test_size=0.2, random_state=25)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f\"training data: {train_dl}\\n test data: {test_dl}\")\n",
    "    return train_dl, test_dl\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LFW_g</th>\n",
       "      <th>LDW_g</th>\n",
       "      <th>LA_mm2</th>\n",
       "      <th>length_mm</th>\n",
       "      <th>width_mm</th>\n",
       "      <th>height_mm</th>\n",
       "      <th>plant_area</th>\n",
       "      <th>plant_convex_hull_area</th>\n",
       "      <th>plant_solidity</th>\n",
       "      <th>plant_perimeter</th>\n",
       "      <th>plant_width</th>\n",
       "      <th>plant_height</th>\n",
       "      <th>plant_longest_path</th>\n",
       "      <th>plant_convex_hull_vertices</th>\n",
       "      <th>plant_ellipse_major_axis</th>\n",
       "      <th>plant_ellipse_minor_axis</th>\n",
       "      <th>plant_ellipse_angle</th>\n",
       "      <th>plant_ellipse_eccentricity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.30</td>\n",
       "      <td>0.078</td>\n",
       "      <td>31.95</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1211</td>\n",
       "      <td>1456.0</td>\n",
       "      <td>0.831731</td>\n",
       "      <td>189.923880</td>\n",
       "      <td>49</td>\n",
       "      <td>42</td>\n",
       "      <td>346</td>\n",
       "      <td>16</td>\n",
       "      <td>45.684494</td>\n",
       "      <td>40.965988</td>\n",
       "      <td>92.878510</td>\n",
       "      <td>0.442608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.10</td>\n",
       "      <td>0.148</td>\n",
       "      <td>44.10</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1412</td>\n",
       "      <td>1556.5</td>\n",
       "      <td>0.907164</td>\n",
       "      <td>181.338094</td>\n",
       "      <td>56</td>\n",
       "      <td>41</td>\n",
       "      <td>370</td>\n",
       "      <td>20</td>\n",
       "      <td>53.368065</td>\n",
       "      <td>37.484673</td>\n",
       "      <td>96.255425</td>\n",
       "      <td>0.711802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.36</td>\n",
       "      <td>0.196</td>\n",
       "      <td>67.61</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.5</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1303</td>\n",
       "      <td>1766.5</td>\n",
       "      <td>0.737617</td>\n",
       "      <td>247.865005</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>333</td>\n",
       "      <td>19</td>\n",
       "      <td>46.356819</td>\n",
       "      <td>43.059174</td>\n",
       "      <td>2.102176</td>\n",
       "      <td>0.370421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.07</td>\n",
       "      <td>0.184</td>\n",
       "      <td>66.98</td>\n",
       "      <td>5.8</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1601</td>\n",
       "      <td>1787.0</td>\n",
       "      <td>0.895915</td>\n",
       "      <td>203.480229</td>\n",
       "      <td>44</td>\n",
       "      <td>59</td>\n",
       "      <td>395</td>\n",
       "      <td>21</td>\n",
       "      <td>55.633369</td>\n",
       "      <td>38.548523</td>\n",
       "      <td>15.127802</td>\n",
       "      <td>0.721031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.17</td>\n",
       "      <td>0.187</td>\n",
       "      <td>68.74</td>\n",
       "      <td>7.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1919</td>\n",
       "      <td>2372.0</td>\n",
       "      <td>0.809022</td>\n",
       "      <td>263.421354</td>\n",
       "      <td>62</td>\n",
       "      <td>58</td>\n",
       "      <td>454</td>\n",
       "      <td>19</td>\n",
       "      <td>60.012882</td>\n",
       "      <td>48.875011</td>\n",
       "      <td>55.774792</td>\n",
       "      <td>0.580292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>57.96</td>\n",
       "      <td>3.010</td>\n",
       "      <td>726.46</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19084</td>\n",
       "      <td>22974.0</td>\n",
       "      <td>0.830678</td>\n",
       "      <td>767.938160</td>\n",
       "      <td>193</td>\n",
       "      <td>167</td>\n",
       "      <td>1367</td>\n",
       "      <td>33</td>\n",
       "      <td>190.580887</td>\n",
       "      <td>137.733093</td>\n",
       "      <td>62.138748</td>\n",
       "      <td>0.691160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>81.46</td>\n",
       "      <td>3.880</td>\n",
       "      <td>1001.79</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>14.8</td>\n",
       "      <td>18373</td>\n",
       "      <td>21556.5</td>\n",
       "      <td>0.852318</td>\n",
       "      <td>661.612260</td>\n",
       "      <td>164</td>\n",
       "      <td>189</td>\n",
       "      <td>1317</td>\n",
       "      <td>29</td>\n",
       "      <td>176.040924</td>\n",
       "      <td>141.141724</td>\n",
       "      <td>22.056726</td>\n",
       "      <td>0.597653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>140.84</td>\n",
       "      <td>6.380</td>\n",
       "      <td>1707.14</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>16.9</td>\n",
       "      <td>23374</td>\n",
       "      <td>26050.0</td>\n",
       "      <td>0.897274</td>\n",
       "      <td>683.754395</td>\n",
       "      <td>186</td>\n",
       "      <td>194</td>\n",
       "      <td>1427</td>\n",
       "      <td>31</td>\n",
       "      <td>191.379974</td>\n",
       "      <td>164.496170</td>\n",
       "      <td>175.959305</td>\n",
       "      <td>0.511091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>108.17</td>\n",
       "      <td>5.250</td>\n",
       "      <td>1364.18</td>\n",
       "      <td>31.4</td>\n",
       "      <td>20.5</td>\n",
       "      <td>16.6</td>\n",
       "      <td>23457</td>\n",
       "      <td>25678.5</td>\n",
       "      <td>0.913488</td>\n",
       "      <td>671.754395</td>\n",
       "      <td>198</td>\n",
       "      <td>179</td>\n",
       "      <td>1364</td>\n",
       "      <td>31</td>\n",
       "      <td>194.815384</td>\n",
       "      <td>159.870819</td>\n",
       "      <td>122.656914</td>\n",
       "      <td>0.571464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>64.40</td>\n",
       "      <td>3.350</td>\n",
       "      <td>812.24</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>18533</td>\n",
       "      <td>22886.5</td>\n",
       "      <td>0.809779</td>\n",
       "      <td>821.595015</td>\n",
       "      <td>189</td>\n",
       "      <td>168</td>\n",
       "      <td>1376</td>\n",
       "      <td>22</td>\n",
       "      <td>193.579941</td>\n",
       "      <td>133.306320</td>\n",
       "      <td>123.732269</td>\n",
       "      <td>0.725106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LFW_g  LDW_g   LA_mm2  length_mm  width_mm  height_mm  plant_area  \\\n",
       "0      1.30  0.078    31.95        4.3       5.2        5.8        1211   \n",
       "1      2.10  0.148    44.10        5.3       5.7        5.5        1412   \n",
       "2      3.36  0.196    67.61        7.3       6.5        8.9        1303   \n",
       "3      3.07  0.184    66.98        5.8       7.6        7.5        1601   \n",
       "4      3.17  0.187    68.74        7.2       8.0        7.1        1919   \n",
       "..      ...    ...      ...        ...       ...        ...         ...   \n",
       "160   57.96  3.010   726.46       18.5      18.5       13.0       19084   \n",
       "161   81.46  3.880  1001.79       21.0      21.5       14.8       18373   \n",
       "162  140.84  6.380  1707.14       23.0      22.5       16.9       23374   \n",
       "163  108.17  5.250  1364.18       31.4      20.5       16.6       23457   \n",
       "164   64.40  3.350   812.24       21.0      18.0       14.7       18533   \n",
       "\n",
       "     plant_convex_hull_area  plant_solidity  plant_perimeter  plant_width  \\\n",
       "0                    1456.0        0.831731       189.923880           49   \n",
       "1                    1556.5        0.907164       181.338094           56   \n",
       "2                    1766.5        0.737617       247.865005           49   \n",
       "3                    1787.0        0.895915       203.480229           44   \n",
       "4                    2372.0        0.809022       263.421354           62   \n",
       "..                      ...             ...              ...          ...   \n",
       "160                 22974.0        0.830678       767.938160          193   \n",
       "161                 21556.5        0.852318       661.612260          164   \n",
       "162                 26050.0        0.897274       683.754395          186   \n",
       "163                 25678.5        0.913488       671.754395          198   \n",
       "164                 22886.5        0.809779       821.595015          189   \n",
       "\n",
       "     plant_height  plant_longest_path  plant_convex_hull_vertices  \\\n",
       "0              42                 346                          16   \n",
       "1              41                 370                          20   \n",
       "2              54                 333                          19   \n",
       "3              59                 395                          21   \n",
       "4              58                 454                          19   \n",
       "..            ...                 ...                         ...   \n",
       "160           167                1367                          33   \n",
       "161           189                1317                          29   \n",
       "162           194                1427                          31   \n",
       "163           179                1364                          31   \n",
       "164           168                1376                          22   \n",
       "\n",
       "     plant_ellipse_major_axis  plant_ellipse_minor_axis  plant_ellipse_angle  \\\n",
       "0                   45.684494                 40.965988            92.878510   \n",
       "1                   53.368065                 37.484673            96.255425   \n",
       "2                   46.356819                 43.059174             2.102176   \n",
       "3                   55.633369                 38.548523            15.127802   \n",
       "4                   60.012882                 48.875011            55.774792   \n",
       "..                        ...                       ...                  ...   \n",
       "160                190.580887                137.733093            62.138748   \n",
       "161                176.040924                141.141724            22.056726   \n",
       "162                191.379974                164.496170           175.959305   \n",
       "163                194.815384                159.870819           122.656914   \n",
       "164                193.579941                133.306320           123.732269   \n",
       "\n",
       "     plant_ellipse_eccentricity  \n",
       "0                      0.442608  \n",
       "1                      0.711802  \n",
       "2                      0.370421  \n",
       "3                      0.721031  \n",
       "4                      0.580292  \n",
       "..                          ...  \n",
       "160                    0.691160  \n",
       "161                    0.597653  \n",
       "162                    0.511091  \n",
       "163                    0.571464  \n",
       "164                    0.725106  \n",
       "\n",
       "[165 rows x 18 columns]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_filepath = \"./final_harvest_data.csv\"\n",
    "\n",
    "data = pd.read_csv(csv_filepath)\n",
    "DATA = data.drop(['date', 'index', 'plant_id', 'tray_id', 'row', 'column'], axis=1)\n",
    "\n",
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: <torch.utils.data.dataloader.DataLoader object at 0x7fda993e7850>\n",
      " test data: <torch.utils.data.dataloader.DataLoader object at 0x7fda98583b80>\n"
     ]
    }
   ],
   "source": [
    "LFW_train, LFW_test = data_workflow(DATA, 'LFW_g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(17, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            \n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits.double()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = DeepNeuralNetwork()\n",
    "model = model.double()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataset, model, loss_function, optimizer):\n",
    "    model.train()\n",
    "    for (X, y) in dataset:\n",
    "        #X, y = X.to('cuda'), y.to('cuda')\n",
    "        y = y.view(-1,1) \n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        prediction = model(X)\n",
    "        loss = loss_function(prediction, y)\n",
    "        y_loss.append(loss)\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss}\")\n",
    "        #print(f\"X: {X} \\n Y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataset, model, loss_function):\n",
    "    size = len(dataset.dataset)\n",
    "    num_batches = len(dataset)\n",
    "\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (X, y) in dataset:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_function(pred, y).item()\n",
    "            \n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------------\n",
      "Loss: 2128628.6442596056\n",
      "Loss: 3344283.242787921\n",
      "Loss: 1876042.0218082503\n",
      "Loss: 2505528.118704681\n",
      "Loss: 605922.7124002817\n",
      "Epoch 2\n",
      "------------------------------------\n",
      "Loss: 1345445.2202109117\n",
      "Loss: 1320925.017825121\n",
      "Loss: 1292900.220800491\n",
      "Loss: 1224167.7165712984\n",
      "Loss: 14840.256612612891\n",
      "Epoch 3\n",
      "------------------------------------\n",
      "Loss: 561852.2925914266\n",
      "Loss: 780710.6953442498\n",
      "Loss: 318508.4231002876\n",
      "Loss: 516225.93033495604\n",
      "Loss: 280522.3209397014\n",
      "Epoch 4\n",
      "------------------------------------\n",
      "Loss: 124873.41802440376\n",
      "Loss: 108193.2403235146\n",
      "Loss: 378419.2112969244\n",
      "Loss: 36476.37311694876\n",
      "Loss: 20456.736679310536\n",
      "Epoch 5\n",
      "------------------------------------\n",
      "Loss: 82792.22240693751\n",
      "Loss: 8143.474557220041\n",
      "Loss: 40455.37385904088\n",
      "Loss: 1460.624178276316\n",
      "Loss: 3355.8164907013975\n",
      "Epoch 6\n",
      "------------------------------------\n",
      "Loss: 22031.46329721473\n",
      "Loss: 21390.073081576556\n",
      "Loss: 19518.485282023677\n",
      "Loss: 26644.020134839197\n",
      "Loss: 53780.35681658248\n",
      "Epoch 7\n",
      "------------------------------------\n",
      "Loss: 39521.9573680202\n",
      "Loss: 51670.81480936668\n",
      "Loss: 37175.02888243551\n",
      "Loss: 62879.11703079359\n",
      "Loss: 39814.36616251975\n",
      "Epoch 8\n",
      "------------------------------------\n",
      "Loss: 48005.42820614859\n",
      "Loss: 48771.94336106225\n",
      "Loss: 63800.48616838917\n",
      "Loss: 56840.97768378221\n",
      "Loss: 46126.32917083099\n",
      "Epoch 9\n",
      "------------------------------------\n",
      "Loss: 44606.35185779969\n",
      "Loss: 35655.11640077866\n",
      "Loss: 46376.10043732017\n",
      "Loss: 39709.94373135348\n",
      "Loss: 20574.813617985696\n",
      "Epoch 10\n",
      "------------------------------------\n",
      "Loss: 25794.7862990062\n",
      "Loss: 31929.282129472\n",
      "Loss: 29361.926734246903\n",
      "Loss: 8904.9821476799\n",
      "Loss: 10974.54677108615\n",
      "Epoch 11\n",
      "------------------------------------\n",
      "Loss: 34979.38818456836\n",
      "Loss: 9253.835863596176\n",
      "Loss: 6672.647328792029\n",
      "Loss: 5217.96240117313\n",
      "Loss: 2528.20649535381\n",
      "Epoch 12\n",
      "------------------------------------\n",
      "Loss: 2937.4643499789163\n",
      "Loss: 1816.7186041599416\n",
      "Loss: 2130.514100333125\n",
      "Loss: 56206.86981319616\n",
      "Loss: 415.37287555348183\n",
      "Epoch 13\n",
      "------------------------------------\n",
      "Loss: 1439.686131704572\n",
      "Loss: 30617.695164394445\n",
      "Loss: 1229.906927817878\n",
      "Loss: 33388.55897666564\n",
      "Loss: 1582.0561026706673\n",
      "Epoch 14\n",
      "------------------------------------\n",
      "Loss: 1092.346665422989\n",
      "Loss: 1147.5889663619307\n",
      "Loss: 65383.84885344586\n",
      "Loss: 1719.3282131622905\n",
      "Loss: 277.51325208035473\n",
      "Epoch 15\n",
      "------------------------------------\n",
      "Loss: 982.2847317565106\n",
      "Loss: 851.4315536004218\n",
      "Loss: 983.4778219865782\n",
      "Loss: 59875.27753735194\n",
      "Loss: 4184.370646512004\n",
      "Epoch 16\n",
      "------------------------------------\n",
      "Loss: 1984.1537882795087\n",
      "Loss: 53690.266137636776\n",
      "Loss: 2244.5212164473587\n",
      "Loss: 1935.9437329021978\n",
      "Loss: 1235.5359495135476\n",
      "Epoch 17\n",
      "------------------------------------\n",
      "Loss: 2327.091056479863\n",
      "Loss: 46187.561518653\n",
      "Loss: 4007.872418161887\n",
      "Loss: 2735.7764228378865\n",
      "Loss: 2362.3987580481066\n",
      "Epoch 18\n",
      "------------------------------------\n",
      "Loss: 3463.7587059215402\n",
      "Loss: 3856.9512038325324\n",
      "Loss: 22109.97889595194\n",
      "Loss: 23558.103819634\n",
      "Loss: 6491.26134707529\n",
      "Epoch 19\n",
      "------------------------------------\n",
      "Loss: 4634.235019005062\n",
      "Loss: 3716.084290391051\n",
      "Loss: 23392.16246475591\n",
      "Loss: 21563.764374223763\n",
      "Loss: 6390.856358080728\n",
      "Epoch 20\n",
      "------------------------------------\n",
      "Loss: 22504.63011112806\n",
      "Loss: 4613.760660745464\n",
      "Loss: 22933.131733898015\n",
      "Loss: 2992.6936488575675\n",
      "Loss: 392.0176822519391\n",
      "Epoch 21\n",
      "------------------------------------\n",
      "Loss: 3349.846960507444\n",
      "Loss: 23447.576979121146\n",
      "Loss: 22023.429195177498\n",
      "Loss: 3527.384811734126\n",
      "Loss: 6125.181687788311\n",
      "Epoch 22\n",
      "------------------------------------\n",
      "Loss: 22215.427072674658\n",
      "Loss: 2825.84493936826\n",
      "Loss: 1984.9638918476548\n",
      "Loss: 25301.45412422566\n",
      "Loss: 4225.744919535202\n",
      "Epoch 23\n",
      "------------------------------------\n",
      "Loss: 2315.665764239461\n",
      "Loss: 45294.04203051481\n",
      "Loss: 2665.4484860342586\n",
      "Loss: 2891.238403194303\n",
      "Loss: 2847.231490848421\n",
      "Epoch 24\n",
      "------------------------------------\n",
      "Loss: 2203.2616284575333\n",
      "Loss: 2202.564471415518\n",
      "Loss: 1763.8652199287421\n",
      "Loss: 47379.66989790369\n",
      "Loss: 2643.218345420401\n",
      "Epoch 25\n",
      "------------------------------------\n",
      "Loss: 25045.763279003248\n",
      "Loss: 22507.01152491991\n",
      "Loss: 3066.3618422475984\n",
      "Loss: 2666.4668571741977\n",
      "Loss: 1005.6642675893215\n",
      "Epoch 26\n",
      "------------------------------------\n",
      "Loss: 2526.438769886285\n",
      "Loss: 2788.853343486035\n",
      "Loss: 21423.956490438934\n",
      "Loss: 24002.22890238483\n",
      "Loss: 3187.5262505178657\n",
      "Epoch 27\n",
      "------------------------------------\n",
      "Loss: 23414.11308008242\n",
      "Loss: 2699.772393235486\n",
      "Loss: 20175.206952068664\n",
      "Loss: 4493.218439709248\n",
      "Loss: 2748.2055337528955\n",
      "Epoch 28\n",
      "------------------------------------\n",
      "Loss: 4902.085877743186\n",
      "Loss: 3551.743555619201\n",
      "Loss: 39867.37915659399\n",
      "Loss: 1901.2351705096107\n",
      "Loss: 5070.55835721627\n",
      "Epoch 29\n",
      "------------------------------------\n",
      "Loss: 3908.90826367454\n",
      "Loss: 2079.062667918699\n",
      "Loss: 20505.055392377097\n",
      "Loss: 23105.30030722391\n",
      "Loss: 5030.775432387429\n",
      "Epoch 30\n",
      "------------------------------------\n",
      "Loss: 3091.711626728418\n",
      "Loss: 22597.23862965739\n",
      "Loss: 2064.494729417641\n",
      "Loss: 21515.261548512277\n",
      "Loss: 3977.674534761348\n",
      "Epoch 31\n",
      "------------------------------------\n",
      "Loss: 21997.250243686907\n",
      "Loss: 22153.844531281404\n",
      "Loss: 2962.9720150569556\n",
      "Loss: 2065.583672666828\n",
      "Loss: 3676.637489987609\n",
      "Epoch 32\n",
      "------------------------------------\n",
      "Loss: 2817.948278300242\n",
      "Loss: 20512.367887467695\n",
      "Loss: 23360.691556562375\n",
      "Loss: 2217.9538376208297\n",
      "Loss: 3732.9740473118277\n",
      "Epoch 33\n",
      "------------------------------------\n",
      "Loss: 2061.124954151508\n",
      "Loss: 41961.701581907975\n",
      "Loss: 2011.14260195569\n",
      "Loss: 3399.6032030363094\n",
      "Loss: 1635.1974163876625\n",
      "Epoch 34\n",
      "------------------------------------\n",
      "Loss: 2371.50288289322\n",
      "Loss: 20873.444256540053\n",
      "Loss: 22207.24403815493\n",
      "Loss: 2651.552052683652\n",
      "Loss: 2539.359135834593\n",
      "Epoch 35\n",
      "------------------------------------\n",
      "Loss: 21183.90481134754\n",
      "Loss: 3668.5087769733505\n",
      "Loss: 2686.313595404807\n",
      "Loss: 2529.8920778730476\n",
      "Loss: 142001.82988736563\n",
      "Epoch 36\n",
      "------------------------------------\n",
      "Loss: 4311.201563641375\n",
      "Loss: 3583.1457839815093\n",
      "Loss: 20857.26469296967\n",
      "Loss: 16057.147022168703\n",
      "Loss: 14738.300998700444\n",
      "Epoch 37\n",
      "------------------------------------\n",
      "Loss: 17822.56959261899\n",
      "Loss: 19053.155487705993\n",
      "Loss: 9561.996552646791\n",
      "Loss: 10440.230009811094\n",
      "Loss: 9717.564793905212\n",
      "Epoch 38\n",
      "------------------------------------\n",
      "Loss: 17871.879478069943\n",
      "Loss: 17626.04196221051\n",
      "Loss: 8366.908769851685\n",
      "Loss: 7133.730662677081\n",
      "Loss: 8605.919674688781\n",
      "Epoch 39\n",
      "------------------------------------\n",
      "Loss: 17496.654808908166\n",
      "Loss: 18919.77755424645\n",
      "Loss: 4930.359072505176\n",
      "Loss: 2331.3887679784198\n",
      "Loss: 1009.5217925108021\n",
      "Epoch 40\n",
      "------------------------------------\n",
      "Loss: 2583.25171848736\n",
      "Loss: 2277.9599718767195\n",
      "Loss: 20926.29022952753\n",
      "Loss: 23006.581794179932\n",
      "Loss: 382.0012836057086\n",
      "Epoch 41\n",
      "------------------------------------\n",
      "Loss: 1527.1841801413914\n",
      "Loss: 44858.09880644106\n",
      "Loss: 810.8989302782466\n",
      "Loss: 1654.710247434023\n",
      "Loss: 1871.8992137235443\n",
      "Epoch 42\n",
      "------------------------------------\n",
      "Loss: 22263.81455805586\n",
      "Loss: 2153.341030495182\n",
      "Loss: 1801.3778515129475\n",
      "Loss: 19043.80737059005\n",
      "Loss: 1606.6697881011496\n",
      "Epoch 43\n",
      "------------------------------------\n",
      "Loss: 18244.757026922915\n",
      "Loss: 2785.246297174669\n",
      "Loss: 1172.137112841445\n",
      "Loss: 21101.94607385442\n",
      "Loss: 1074.1483021569668\n",
      "Epoch 44\n",
      "------------------------------------\n",
      "Loss: 3527.106993272213\n",
      "Loss: 2947.7502487569227\n",
      "Loss: 16836.157268750696\n",
      "Loss: 18735.265544345613\n",
      "Loss: 2220.511539303898\n",
      "Epoch 45\n",
      "------------------------------------\n",
      "Loss: 17537.38466149599\n",
      "Loss: 3009.868546145518\n",
      "Loss: 18273.93606858782\n",
      "Loss: 3548.3974816953323\n",
      "Loss: 1623.2044643552936\n",
      "Epoch 46\n",
      "------------------------------------\n",
      "Loss: 3459.662017148804\n",
      "Loss: 16652.46659079351\n",
      "Loss: 18356.22942061002\n",
      "Loss: 2800.9231304850155\n",
      "Loss: 5386.319418452939\n",
      "Epoch 47\n",
      "------------------------------------\n",
      "Loss: 2516.9939561468605\n",
      "Loss: 33407.45858132672\n",
      "Loss: 3362.1156535442133\n",
      "Loss: 1971.9332199491823\n",
      "Loss: 2431.1707222798773\n",
      "Epoch 48\n",
      "------------------------------------\n",
      "Loss: 19353.79873106687\n",
      "Loss: 18114.43353664497\n",
      "Loss: 2063.2580915522217\n",
      "Loss: 1733.519549256915\n",
      "Loss: 646.9929001803694\n",
      "Epoch 49\n",
      "------------------------------------\n",
      "Loss: 19895.40523156235\n",
      "Loss: 1782.7360386364126\n",
      "Loss: 1244.8997110554951\n",
      "Loss: 17879.71816633413\n",
      "Loss: 1632.142058292352\n",
      "Epoch 50\n",
      "------------------------------------\n",
      "Loss: 34181.91038363466\n",
      "Loss: 1325.547240228708\n",
      "Loss: 3522.7101995998264\n",
      "Loss: 2767.1008693450094\n",
      "Loss: 375.4316236472949\n",
      "Epoch 51\n",
      "------------------------------------\n",
      "Loss: 2334.683013500274\n",
      "Loss: 18297.786459485404\n",
      "Loss: 2393.3129603151174\n",
      "Loss: 16451.09035393896\n",
      "Loss: 1933.112054081168\n",
      "Epoch 52\n",
      "------------------------------------\n",
      "Loss: 2175.92195911419\n",
      "Loss: 2446.6844812321697\n",
      "Loss: 2009.1458567314294\n",
      "Loss: 33675.358580389104\n",
      "Loss: 1136.451070553651\n",
      "Epoch 53\n",
      "------------------------------------\n",
      "Loss: 2223.319954694904\n",
      "Loss: 16613.656064161958\n",
      "Loss: 1872.9363037602407\n",
      "Loss: 18194.218718274657\n",
      "Loss: 2076.510148356947\n",
      "Epoch 54\n",
      "------------------------------------\n",
      "Loss: 1949.0708216843468\n",
      "Loss: 17760.489278302663\n",
      "Loss: 2441.4668268961054\n",
      "Loss: 2173.3336468000884\n",
      "Loss: 115651.54069558144\n",
      "Epoch 55\n",
      "------------------------------------\n",
      "Loss: 3428.2268932352918\n",
      "Loss: 15835.794784762324\n",
      "Loss: 15723.543326623809\n",
      "Loss: 6088.320271402066\n",
      "Loss: 760.2239911757193\n",
      "Epoch 56\n",
      "------------------------------------\n",
      "Loss: 10970.273698621537\n",
      "Loss: 11960.611073815839\n",
      "Loss: 14370.888435650664\n",
      "Loss: 15450.813052118774\n",
      "Loss: 20040.073951109393\n",
      "Epoch 57\n",
      "------------------------------------\n",
      "Loss: 12323.470674873279\n",
      "Loss: 8820.979053407626\n",
      "Loss: 15348.222047645875\n",
      "Loss: 12110.781478578012\n",
      "Loss: 1463.641025047379\n",
      "Epoch 58\n",
      "------------------------------------\n",
      "Loss: 5128.87341705105\n",
      "Loss: 13982.612698911893\n",
      "Loss: 1749.8430016777716\n",
      "Loss: 16545.415436917476\n",
      "Loss: 2456.0404391250486\n",
      "Epoch 59\n",
      "------------------------------------\n",
      "Loss: 1364.9866046666418\n",
      "Loss: 18405.773669641178\n",
      "Loss: 18005.334388184052\n",
      "Loss: 971.2507337099057\n",
      "Loss: 495.2169028197114\n",
      "Epoch 60\n",
      "------------------------------------\n",
      "Loss: 19365.138379390617\n",
      "Loss: 959.6117293087254\n",
      "Loss: 1055.481930405729\n",
      "Loss: 1069.1221814757482\n",
      "Loss: 129931.27147701393\n",
      "Epoch 61\n",
      "------------------------------------\n",
      "Loss: 1801.5425522782277\n",
      "Loss: 15094.02098505707\n",
      "Loss: 12185.870504485603\n",
      "Loss: 6939.715684442028\n",
      "Loss: 7999.519000197698\n",
      "Epoch 62\n",
      "------------------------------------\n",
      "Loss: 15274.379111720216\n",
      "Loss: 10499.63347558499\n",
      "Loss: 11458.49945746053\n",
      "Loss: 11790.075613035744\n",
      "Loss: 8717.421571842047\n",
      "Epoch 63\n",
      "------------------------------------\n",
      "Loss: 10190.766182980411\n",
      "Loss: 7529.417590902038\n",
      "Loss: 12680.08323508408\n",
      "Loss: 5050.586195961633\n",
      "Loss: 77008.6694789222\n",
      "Epoch 64\n",
      "------------------------------------\n",
      "Loss: 4338.063828639881\n",
      "Loss: 12835.191782447466\n",
      "Loss: 7394.283186604258\n",
      "Loss: 12045.539316511335\n",
      "Loss: 6094.104279910158\n",
      "Epoch 65\n",
      "------------------------------------\n",
      "Loss: 5049.326229540229\n",
      "Loss: 11608.446693759337\n",
      "Loss: 13483.677377335614\n",
      "Loss: 5027.442460680259\n",
      "Loss: 1364.7422831984402\n",
      "Epoch 66\n",
      "------------------------------------\n",
      "Loss: 21229.565523381414\n",
      "Loss: 3258.37289123365\n",
      "Loss: 2471.0972587319407\n",
      "Loss: 2891.236168553554\n",
      "Loss: 4642.229108592013\n",
      "Epoch 67\n",
      "------------------------------------\n",
      "Loss: 1167.7129649984204\n",
      "Loss: 14579.981787362498\n",
      "Loss: 14161.761985104395\n",
      "Loss: 1105.8443198631187\n",
      "Loss: 1338.103103817191\n",
      "Epoch 68\n",
      "------------------------------------\n",
      "Loss: 14757.844409218862\n",
      "Loss: 733.5773575662695\n",
      "Loss: 16355.591553577668\n",
      "Loss: 1034.4897340725156\n",
      "Loss: 218.69254532206534\n",
      "Epoch 69\n",
      "------------------------------------\n",
      "Loss: 1116.3320728301903\n",
      "Loss: 786.068642864416\n",
      "Loss: 14412.012470174022\n",
      "Loss: 15626.683180489428\n",
      "Loss: 1936.3809165356965\n",
      "Epoch 70\n",
      "------------------------------------\n",
      "Loss: 1404.716132544184\n",
      "Loss: 1124.834926838877\n",
      "Loss: 1437.8545858491307\n",
      "Loss: 25002.398224264733\n",
      "Loss: 1274.9307847344985\n",
      "Epoch 71\n",
      "------------------------------------\n",
      "Loss: 985.3384564812624\n",
      "Loss: 1810.9315652306675\n",
      "Loss: 12680.22431603894\n",
      "Loss: 12839.308763743209\n",
      "Loss: 3529.327987430398\n",
      "Epoch 72\n",
      "------------------------------------\n",
      "Loss: 11069.850392771736\n",
      "Loss: 2639.5433683604588\n",
      "Loss: 12029.992408088903\n",
      "Loss: 3000.3845576203685\n",
      "Loss: 3471.267341886015\n",
      "Epoch 73\n",
      "------------------------------------\n",
      "Loss: 2872.13464345721\n",
      "Loss: 12116.739556193183\n",
      "Loss: 12290.493859402566\n",
      "Loss: 1324.365861027944\n",
      "Loss: 463.9100531095765\n",
      "Epoch 74\n",
      "------------------------------------\n",
      "Loss: 23362.903061355202\n",
      "Loss: 1569.3455735863072\n",
      "Loss: 1847.1304630733407\n",
      "Loss: 1629.0559897215503\n",
      "Loss: 427.1816593762589\n",
      "Epoch 75\n",
      "------------------------------------\n",
      "Loss: 1677.793172216949\n",
      "Loss: 1162.0364863597952\n",
      "Loss: 24085.654237495328\n",
      "Loss: 1621.3785093476836\n",
      "Loss: 975.8199481596253\n",
      "Epoch 76\n",
      "------------------------------------\n",
      "Loss: 1340.7750171343087\n",
      "Loss: 1583.3270965898162\n",
      "Loss: 12124.989353562749\n",
      "Loss: 13012.450116434671\n",
      "Loss: 805.2590237907228\n",
      "Epoch 77\n",
      "------------------------------------\n",
      "Loss: 12623.8260128663\n",
      "Loss: 1529.112821057778\n",
      "Loss: 1548.3228747495148\n",
      "Loss: 11196.246594678887\n",
      "Loss: 3340.121019219233\n",
      "Epoch 78\n",
      "------------------------------------\n",
      "Loss: 11920.604185092734\n",
      "Loss: 1129.501566068881\n",
      "Loss: 1755.8374668866402\n",
      "Loss: 11830.931014193142\n",
      "Loss: 427.31862566137767\n",
      "Epoch 79\n",
      "------------------------------------\n",
      "Loss: 1054.4421881669634\n",
      "Loss: 1530.5920390601166\n",
      "Loss: 11978.471257679585\n",
      "Loss: 12027.308009093213\n",
      "Loss: 252.2300005116587\n",
      "Epoch 80\n",
      "------------------------------------\n",
      "Loss: 1462.8852967928556\n",
      "Loss: 1396.2712031384308\n",
      "Loss: 21345.97669277518\n",
      "Loss: 2038.006120055572\n",
      "Loss: 1125.4895428649484\n",
      "Epoch 81\n",
      "------------------------------------\n",
      "Loss: 1762.9813359223544\n",
      "Loss: 1327.3611647323664\n",
      "Loss: 20150.133233882243\n",
      "Loss: 2501.8855055648273\n",
      "Loss: 3052.9677315436074\n",
      "Epoch 82\n",
      "------------------------------------\n",
      "Loss: 10295.08527050489\n",
      "Loss: 1069.357615929996\n",
      "Loss: 2465.808108821727\n",
      "Loss: 11522.235924514413\n",
      "Loss: 305.5515070041599\n",
      "Epoch 83\n",
      "------------------------------------\n",
      "Loss: 919.9448947125117\n",
      "Loss: 11986.55173206971\n",
      "Loss: 10562.12594552944\n",
      "Loss: 1647.7388916950272\n",
      "Loss: 689.432583511186\n",
      "Epoch 84\n",
      "------------------------------------\n",
      "Loss: 1112.675825664719\n",
      "Loss: 1362.4826061464933\n",
      "Loss: 10687.104546349308\n",
      "Loss: 11485.796535116577\n",
      "Loss: 959.976759945302\n",
      "Epoch 85\n",
      "------------------------------------\n",
      "Loss: 1333.5328830288015\n",
      "Loss: 10813.064471015687\n",
      "Loss: 988.8933794764482\n",
      "Loss: 1553.7200953252445\n",
      "Loss: 77638.14963092614\n",
      "Epoch 86\n",
      "------------------------------------\n",
      "Loss: 2376.0615700155195\n",
      "Loss: 4360.50072777111\n",
      "Loss: 10618.673862769703\n",
      "Loss: 8822.844047867658\n",
      "Loss: 1142.4309239788772\n",
      "Epoch 87\n",
      "------------------------------------\n",
      "Loss: 8357.847416662073\n",
      "Loss: 11916.301328483958\n",
      "Loss: 7338.216799516335\n",
      "Loss: 11295.66973357119\n",
      "Loss: 4773.539482814405\n",
      "Epoch 88\n",
      "------------------------------------\n",
      "Loss: 11010.540044693294\n",
      "Loss: 8079.99210698132\n",
      "Loss: 5682.479184475535\n",
      "Loss: 2300.883965307064\n",
      "Loss: 4148.430160211319\n",
      "Epoch 89\n",
      "------------------------------------\n",
      "Loss: 8871.72048982424\n",
      "Loss: 9878.486485162122\n",
      "Loss: 1442.2863839313038\n",
      "Loss: 816.2675347132556\n",
      "Loss: 452.6024419906552\n",
      "Epoch 90\n",
      "------------------------------------\n",
      "Loss: 896.7642486266853\n",
      "Loss: 281.92102847865067\n",
      "Loss: 26055.113765157723\n",
      "Loss: 474.59152859647315\n",
      "Loss: 850.1374944935965\n",
      "Epoch 91\n",
      "------------------------------------\n",
      "Loss: 12981.015901231925\n",
      "Loss: 11751.999141219485\n",
      "Loss: 556.3528699021276\n",
      "Loss: 1154.9653570688627\n",
      "Loss: 882.7114383065539\n",
      "Epoch 92\n",
      "------------------------------------\n",
      "Loss: 10294.227117807763\n",
      "Loss: 1377.6571172228255\n",
      "Loss: 1139.9499650578719\n",
      "Loss: 8807.049352131526\n",
      "Loss: 1855.032363576116\n",
      "Epoch 93\n",
      "------------------------------------\n",
      "Loss: 1395.0659090042882\n",
      "Loss: 8551.168127210005\n",
      "Loss: 9575.687641741026\n",
      "Loss: 1546.7806011912703\n",
      "Loss: 230.55234435864352\n",
      "Epoch 94\n",
      "------------------------------------\n",
      "Loss: 1026.9509564807659\n",
      "Loss: 8451.565855352437\n",
      "Loss: 8767.413133860187\n",
      "Loss: 2578.3698656203087\n",
      "Loss: 1612.7194590472338\n",
      "Epoch 95\n",
      "------------------------------------\n",
      "Loss: 7684.56387472329\n",
      "Loss: 1513.0731095244496\n",
      "Loss: 8718.099147413403\n",
      "Loss: 2511.966952751881\n",
      "Loss: 302.31747233407066\n",
      "Epoch 96\n",
      "------------------------------------\n",
      "Loss: 1418.5542252459416\n",
      "Loss: 8198.610572223051\n",
      "Loss: 1170.043610858655\n",
      "Loss: 9500.050609875572\n",
      "Loss: 1257.2476989479283\n",
      "Epoch 97\n",
      "------------------------------------\n",
      "Loss: 9190.381180063605\n",
      "Loss: 8605.672526955275\n",
      "Loss: 1262.380800182476\n",
      "Loss: 1134.830750723916\n",
      "Loss: 1726.5142041446577\n",
      "Epoch 98\n",
      "------------------------------------\n",
      "Loss: 8611.611475284803\n",
      "Loss: 1129.3011195812874\n",
      "Loss: 994.9372445895729\n",
      "Loss: 9227.10093028139\n",
      "Loss: 962.0905563879064\n",
      "Epoch 99\n",
      "------------------------------------\n",
      "Loss: 17268.22820570814\n",
      "Loss: 669.5446871154578\n",
      "Loss: 1474.2187692775206\n",
      "Loss: 1369.024831873779\n",
      "Loss: 610.8918092642286\n",
      "Epoch 100\n",
      "------------------------------------\n",
      "Loss: 8172.192323962067\n",
      "Loss: 8834.649262795188\n",
      "Loss: 1071.7229473325715\n",
      "Loss: 1192.1897962827018\n",
      "Loss: 278.12163016632707\n",
      "Epoch 101\n",
      "------------------------------------\n",
      "Loss: 1663.8428299119596\n",
      "Loss: 8558.380565311396\n",
      "Loss: 544.3110417958475\n",
      "Loss: 8180.975335386718\n",
      "Loss: 721.8159604064189\n",
      "Epoch 102\n",
      "------------------------------------\n",
      "Loss: 8766.09606243488\n",
      "Loss: 7750.574099502945\n",
      "Loss: 651.818313528132\n",
      "Loss: 1711.2852322159138\n",
      "Loss: 780.5587584856912\n",
      "Epoch 103\n",
      "------------------------------------\n",
      "Loss: 8665.96362184677\n",
      "Loss: 7199.8041704220705\n",
      "Loss: 1371.563853209827\n",
      "Loss: 1112.2558576866213\n",
      "Loss: 241.0162999812343\n",
      "Epoch 104\n",
      "------------------------------------\n",
      "Loss: 1393.1544087236168\n",
      "Loss: 993.1400987248214\n",
      "Loss: 8329.047368754182\n",
      "Loss: 7713.565132167002\n",
      "Loss: 842.6274383657585\n",
      "Epoch 105\n",
      "------------------------------------\n",
      "Loss: 748.8259868900723\n",
      "Loss: 809.2389589734803\n",
      "Loss: 7990.170234914425\n",
      "Loss: 8601.48689252642\n",
      "Loss: 1363.695181918152\n",
      "Epoch 106\n",
      "------------------------------------\n",
      "Loss: 15494.659424058813\n",
      "Loss: 1086.5454794439602\n",
      "Loss: 748.5548307247536\n",
      "Loss: 1100.1219388886245\n",
      "Loss: 714.4734784325713\n",
      "Epoch 107\n",
      "------------------------------------\n",
      "Loss: 658.1474585277758\n",
      "Loss: 1041.7612555725434\n",
      "Loss: 8414.009812167684\n",
      "Loss: 7089.00687229365\n",
      "Loss: 1121.330905975122\n",
      "Epoch 108\n",
      "------------------------------------\n",
      "Loss: 1207.749657273506\n",
      "Loss: 804.0029890182914\n",
      "Loss: 7449.862361661948\n",
      "Loss: 8012.811389860987\n",
      "Loss: 410.62376939673493\n",
      "Epoch 109\n",
      "------------------------------------\n",
      "Loss: 8119.556103463555\n",
      "Loss: 7296.361842804984\n",
      "Loss: 720.0496550909751\n",
      "Loss: 997.2432807692855\n",
      "Loss: 1389.252805041012\n",
      "Epoch 110\n",
      "------------------------------------\n",
      "Loss: 670.1982002585214\n",
      "Loss: 7290.755698221363\n",
      "Loss: 7446.475442224107\n",
      "Loss: 973.0630490243515\n",
      "Loss: 888.142383074516\n",
      "Epoch 111\n",
      "------------------------------------\n",
      "Loss: 6634.540088291459\n",
      "Loss: 7009.01546492269\n",
      "Loss: 1035.5958479658057\n",
      "Loss: 1400.8695753964016\n",
      "Loss: 2894.453429586787\n",
      "Epoch 112\n",
      "------------------------------------\n",
      "Loss: 13161.257268145206\n",
      "Loss: 1277.568976032042\n",
      "Loss: 844.2539762917754\n",
      "Loss: 376.2740295793824\n",
      "Loss: 2408.4610385702144\n",
      "Epoch 113\n",
      "------------------------------------\n",
      "Loss: 568.8063508572224\n",
      "Loss: 14777.980364793508\n",
      "Loss: 572.834857698203\n",
      "Loss: 875.6038680567351\n",
      "Loss: 424.7932422112534\n",
      "Epoch 114\n",
      "------------------------------------\n",
      "Loss: 665.4523725600677\n",
      "Loss: 14545.888547536699\n",
      "Loss: 817.8737109186028\n",
      "Loss: 313.4109312122991\n",
      "Loss: 344.961685681215\n",
      "Epoch 115\n",
      "------------------------------------\n",
      "Loss: 6890.868237590341\n",
      "Loss: 989.2263741196587\n",
      "Loss: 6367.257942569993\n",
      "Loss: 1043.807012502948\n",
      "Loss: 980.4837717902712\n",
      "Epoch 116\n",
      "------------------------------------\n",
      "Loss: 6116.472168020767\n",
      "Loss: 1196.9276432494623\n",
      "Loss: 6070.250644028245\n",
      "Loss: 1506.3365798813272\n",
      "Loss: 1119.6936583398003\n",
      "Epoch 117\n",
      "------------------------------------\n",
      "Loss: 1044.2355048446677\n",
      "Loss: 11746.11574150484\n",
      "Loss: 1183.8091255231825\n",
      "Loss: 717.7777569977603\n",
      "Loss: 940.8236431443673\n",
      "Epoch 118\n",
      "------------------------------------\n",
      "Loss: 1032.403693435428\n",
      "Loss: 6859.532329536711\n",
      "Loss: 6474.157238550137\n",
      "Loss: 563.3794284745974\n",
      "Loss: 175.72621889000396\n",
      "Epoch 119\n",
      "------------------------------------\n",
      "Loss: 895.4628143628031\n",
      "Loss: 6707.879772878805\n",
      "Loss: 779.9404187400603\n",
      "Loss: 6058.679805769572\n",
      "Loss: 1352.9058466701322\n",
      "Epoch 120\n",
      "------------------------------------\n",
      "Loss: 658.4072658455586\n",
      "Loss: 699.1637564616512\n",
      "Loss: 798.679050182577\n",
      "Loss: 12319.457951435063\n",
      "Loss: 461.7692504357424\n",
      "Epoch 121\n",
      "------------------------------------\n",
      "Loss: 5855.899912634101\n",
      "Loss: 808.9344077351208\n",
      "Loss: 1134.8932169429165\n",
      "Loss: 615.7635844723912\n",
      "Loss: 44534.47891838225\n",
      "Epoch 122\n",
      "------------------------------------\n",
      "Loss: 6108.986022747462\n",
      "Loss: 4996.128454444034\n",
      "Loss: 3452.411609644732\n",
      "Loss: 4547.792951412972\n",
      "Loss: 7805.149499544558\n",
      "Epoch 123\n",
      "------------------------------------\n",
      "Loss: 5774.751640023933\n",
      "Loss: 7644.28000115371\n",
      "Loss: 5311.74465990727\n",
      "Loss: 3896.6705915807424\n",
      "Loss: 3434.5268203524993\n",
      "Epoch 124\n",
      "------------------------------------\n",
      "Loss: 2526.399620762076\n",
      "Loss: 1508.9852293384142\n",
      "Loss: 9683.084438813703\n",
      "Loss: 720.5410605341286\n",
      "Loss: 113.82232359767484\n",
      "Epoch 125\n",
      "------------------------------------\n",
      "Loss: 6157.213790218646\n",
      "Loss: 593.863569834065\n",
      "Loss: 7253.07965765284\n",
      "Loss: 239.10546319932698\n",
      "Loss: 334.87615635178565\n",
      "Epoch 126\n",
      "------------------------------------\n",
      "Loss: 202.61460307687707\n",
      "Loss: 358.978419344734\n",
      "Loss: 7207.674353774592\n",
      "Loss: 7645.706720910064\n",
      "Loss: 850.1995289613839\n",
      "Epoch 127\n",
      "------------------------------------\n",
      "Loss: 295.2899012695166\n",
      "Loss: 5768.225237163831\n",
      "Loss: 5845.07711838159\n",
      "Loss: 297.8898045219237\n",
      "Loss: 782.4233073398808\n",
      "Epoch 128\n",
      "------------------------------------\n",
      "Loss: 1092.4207076986222\n",
      "Loss: 7927.050212596112\n",
      "Loss: 1888.2089708554308\n",
      "Loss: 1702.3990138584998\n",
      "Loss: 1550.8585749487281\n",
      "Epoch 129\n",
      "------------------------------------\n",
      "Loss: 4547.84466267151\n",
      "Loss: 2219.7678162076527\n",
      "Loss: 916.2705140795047\n",
      "Loss: 4803.7042029892245\n",
      "Loss: 470.86416154712526\n",
      "Epoch 130\n",
      "------------------------------------\n",
      "Loss: 850.5200065621455\n",
      "Loss: 4505.461693827739\n",
      "Loss: 4996.083651600023\n",
      "Loss: 1029.345082401122\n",
      "Loss: 90.63548683518914\n",
      "Epoch 131\n",
      "------------------------------------\n",
      "Loss: 4804.804651169489\n",
      "Loss: 5194.807484884001\n",
      "Loss: 482.40003974484347\n",
      "Loss: 880.5838562420317\n",
      "Loss: 419.4807426012289\n",
      "Epoch 132\n",
      "------------------------------------\n",
      "Loss: 5390.80798336197\n",
      "Loss: 472.5133543478872\n",
      "Loss: 4823.074190666272\n",
      "Loss: 469.05275771333993\n",
      "Loss: 167.63283557237324\n",
      "Epoch 133\n",
      "------------------------------------\n",
      "Loss: 709.8801352382752\n",
      "Loss: 4652.628144463054\n",
      "Loss: 567.710439431728\n",
      "Loss: 4925.980659653108\n",
      "Loss: 78.90262945658228\n",
      "Epoch 134\n",
      "------------------------------------\n",
      "Loss: 773.287710391359\n",
      "Loss: 673.0632395628795\n",
      "Loss: 605.9280517388677\n",
      "Loss: 8626.360723851532\n",
      "Loss: 603.172373051282\n",
      "Epoch 135\n",
      "------------------------------------\n",
      "Loss: 789.6700495454639\n",
      "Loss: 4906.488975573554\n",
      "Loss: 4089.7045619951937\n",
      "Loss: 488.8772786077228\n",
      "Loss: 1510.955139268137\n",
      "Epoch 136\n",
      "------------------------------------\n",
      "Loss: 4090.396874242595\n",
      "Loss: 715.2214761918646\n",
      "Loss: 4575.720741116148\n",
      "Loss: 622.8650650264972\n",
      "Loss: 1804.7591246999602\n",
      "Epoch 137\n",
      "------------------------------------\n",
      "Loss: 513.7578191018572\n",
      "Loss: 4653.967699692946\n",
      "Loss: 4460.770834113272\n",
      "Loss: 610.44732558357\n",
      "Loss: 210.89012279351894\n",
      "Epoch 138\n",
      "------------------------------------\n",
      "Loss: 436.6943217579943\n",
      "Loss: 225.50580327248682\n",
      "Loss: 4542.724555892332\n",
      "Loss: 5140.955313625898\n",
      "Loss: 739.4434589666987\n",
      "Epoch 139\n",
      "------------------------------------\n",
      "Loss: 276.1870129835646\n",
      "Loss: 8898.125644307422\n",
      "Loss: 525.2079036917708\n",
      "Loss: 538.0439271512063\n",
      "Loss: 91.26053556327102\n",
      "Epoch 140\n",
      "------------------------------------\n",
      "Loss: 729.8031777970306\n",
      "Loss: 7599.370567383733\n",
      "Loss: 451.1288298144845\n",
      "Loss: 900.1082883580498\n",
      "Loss: 1119.4193137751095\n",
      "Epoch 141\n",
      "------------------------------------\n",
      "Loss: 4344.5899773095625\n",
      "Loss: 419.00631067789686\n",
      "Loss: 3592.405044202039\n",
      "Loss: 875.4747320328999\n",
      "Loss: 850.0911410909392\n",
      "Epoch 142\n",
      "------------------------------------\n",
      "Loss: 503.1768300962409\n",
      "Loss: 3788.668959303835\n",
      "Loss: 4594.099369793281\n",
      "Loss: 380.5189279770516\n",
      "Loss: 477.345125320972\n",
      "Epoch 143\n",
      "------------------------------------\n",
      "Loss: 371.7037748380171\n",
      "Loss: 469.8800844025924\n",
      "Loss: 340.09400299461305\n",
      "Loss: 8876.44778418236\n",
      "Loss: 231.58465280952942\n",
      "Epoch 144\n",
      "------------------------------------\n",
      "Loss: 4525.422346944551\n",
      "Loss: 359.1230102540188\n",
      "Loss: 357.7725611006923\n",
      "Loss: 3934.8972148432667\n",
      "Loss: 28.01656527619222\n",
      "Epoch 145\n",
      "------------------------------------\n",
      "Loss: 302.86156655046574\n",
      "Loss: 885.9088352563092\n",
      "Loss: 3811.0029099462186\n",
      "Loss: 3611.645529738281\n",
      "Loss: 104.84124518719679\n",
      "Epoch 146\n",
      "------------------------------------\n",
      "Loss: 725.3890716218201\n",
      "Loss: 3597.589394278663\n",
      "Loss: 774.2122059641205\n",
      "Loss: 3436.1097156862475\n",
      "Loss: 664.4041968676852\n",
      "Epoch 147\n",
      "------------------------------------\n",
      "Loss: 589.0830148495024\n",
      "Loss: 529.1248932843822\n",
      "Loss: 845.5505244318229\n",
      "Loss: 6769.320675651017\n",
      "Loss: 1111.5935752039547\n",
      "Epoch 148\n",
      "------------------------------------\n",
      "Loss: 6682.567846578415\n",
      "Loss: 692.2584278953574\n",
      "Loss: 585.4650083622687\n",
      "Loss: 765.0839164646904\n",
      "Loss: 120.55228203367783\n",
      "Epoch 149\n",
      "------------------------------------\n",
      "Loss: 3296.556888199908\n",
      "Loss: 739.2610629194569\n",
      "Loss: 3639.506634441313\n",
      "Loss: 440.5118249693909\n",
      "Loss: 118.63370925043924\n",
      "Epoch 150\n",
      "------------------------------------\n",
      "Loss: 3507.5413755945815\n",
      "Loss: 3168.7297458288003\n",
      "Loss: 496.4260947041\n",
      "Loss: 982.1959789195796\n",
      "Loss: 550.0982635317163\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "count = 0\n",
    "for t in range(EPOCHS):\n",
    "    count += 1\n",
    "    X_epochs.append(count)\n",
    "    print(f\"Epoch {t+1}\\n------------------------------------\")\n",
    "    train_loop(LFW_train, model, LOSS_FUNCTION, optimizer)\n",
    "    #test_loop(LFW_test, model, LOSS_FUNCTION)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05dbbb7414389032baa654308b5b2368ed4754b5c0531661864b7939633c6eac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

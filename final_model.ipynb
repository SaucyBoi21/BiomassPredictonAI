{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 150\n",
    "LOSS_FUNCTION = nn.MSELoss()\n",
    "LOWEST_LOSS = 150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loss = []\n",
    "X_epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_workflow(input_data, target_value):\n",
    "    X = input_data.drop([target_value], axis=1)\n",
    "    y = input_data[target_value]\n",
    "\n",
    "    #print(X)\n",
    "    #print(y)\n",
    "\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    X = torch.from_numpy(X)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    data = TensorDataset(X, y)\n",
    "\n",
    "    train_ds, test_ds = train_test_split(data, test_size=0.2, random_state=25)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f\"training data: {train_dl}\\n test data: {test_dl}\")\n",
    "    return train_dl, test_dl\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LFW_g</th>\n",
       "      <th>LDW_g</th>\n",
       "      <th>LA_mm2</th>\n",
       "      <th>length_mm</th>\n",
       "      <th>width_mm</th>\n",
       "      <th>height_mm</th>\n",
       "      <th>plant_area</th>\n",
       "      <th>plant_convex_hull_area</th>\n",
       "      <th>plant_solidity</th>\n",
       "      <th>plant_perimeter</th>\n",
       "      <th>plant_width</th>\n",
       "      <th>plant_height</th>\n",
       "      <th>plant_longest_path</th>\n",
       "      <th>plant_convex_hull_vertices</th>\n",
       "      <th>plant_ellipse_major_axis</th>\n",
       "      <th>plant_ellipse_minor_axis</th>\n",
       "      <th>plant_ellipse_angle</th>\n",
       "      <th>plant_ellipse_eccentricity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.30</td>\n",
       "      <td>0.078</td>\n",
       "      <td>31.95</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1211</td>\n",
       "      <td>1456.0</td>\n",
       "      <td>0.831731</td>\n",
       "      <td>189.923880</td>\n",
       "      <td>49</td>\n",
       "      <td>42</td>\n",
       "      <td>346</td>\n",
       "      <td>16</td>\n",
       "      <td>45.684494</td>\n",
       "      <td>40.965988</td>\n",
       "      <td>92.878510</td>\n",
       "      <td>0.442608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.10</td>\n",
       "      <td>0.148</td>\n",
       "      <td>44.10</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1412</td>\n",
       "      <td>1556.5</td>\n",
       "      <td>0.907164</td>\n",
       "      <td>181.338094</td>\n",
       "      <td>56</td>\n",
       "      <td>41</td>\n",
       "      <td>370</td>\n",
       "      <td>20</td>\n",
       "      <td>53.368065</td>\n",
       "      <td>37.484673</td>\n",
       "      <td>96.255425</td>\n",
       "      <td>0.711802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.36</td>\n",
       "      <td>0.196</td>\n",
       "      <td>67.61</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.5</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1303</td>\n",
       "      <td>1766.5</td>\n",
       "      <td>0.737617</td>\n",
       "      <td>247.865005</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>333</td>\n",
       "      <td>19</td>\n",
       "      <td>46.356819</td>\n",
       "      <td>43.059174</td>\n",
       "      <td>2.102176</td>\n",
       "      <td>0.370421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.07</td>\n",
       "      <td>0.184</td>\n",
       "      <td>66.98</td>\n",
       "      <td>5.8</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1601</td>\n",
       "      <td>1787.0</td>\n",
       "      <td>0.895915</td>\n",
       "      <td>203.480229</td>\n",
       "      <td>44</td>\n",
       "      <td>59</td>\n",
       "      <td>395</td>\n",
       "      <td>21</td>\n",
       "      <td>55.633369</td>\n",
       "      <td>38.548523</td>\n",
       "      <td>15.127802</td>\n",
       "      <td>0.721031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.17</td>\n",
       "      <td>0.187</td>\n",
       "      <td>68.74</td>\n",
       "      <td>7.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1919</td>\n",
       "      <td>2372.0</td>\n",
       "      <td>0.809022</td>\n",
       "      <td>263.421354</td>\n",
       "      <td>62</td>\n",
       "      <td>58</td>\n",
       "      <td>454</td>\n",
       "      <td>19</td>\n",
       "      <td>60.012882</td>\n",
       "      <td>48.875011</td>\n",
       "      <td>55.774792</td>\n",
       "      <td>0.580292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>57.96</td>\n",
       "      <td>3.010</td>\n",
       "      <td>726.46</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19084</td>\n",
       "      <td>22974.0</td>\n",
       "      <td>0.830678</td>\n",
       "      <td>767.938160</td>\n",
       "      <td>193</td>\n",
       "      <td>167</td>\n",
       "      <td>1367</td>\n",
       "      <td>33</td>\n",
       "      <td>190.580887</td>\n",
       "      <td>137.733093</td>\n",
       "      <td>62.138748</td>\n",
       "      <td>0.691160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>81.46</td>\n",
       "      <td>3.880</td>\n",
       "      <td>1001.79</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>14.8</td>\n",
       "      <td>18373</td>\n",
       "      <td>21556.5</td>\n",
       "      <td>0.852318</td>\n",
       "      <td>661.612260</td>\n",
       "      <td>164</td>\n",
       "      <td>189</td>\n",
       "      <td>1317</td>\n",
       "      <td>29</td>\n",
       "      <td>176.040924</td>\n",
       "      <td>141.141724</td>\n",
       "      <td>22.056726</td>\n",
       "      <td>0.597653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>140.84</td>\n",
       "      <td>6.380</td>\n",
       "      <td>1707.14</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>16.9</td>\n",
       "      <td>23374</td>\n",
       "      <td>26050.0</td>\n",
       "      <td>0.897274</td>\n",
       "      <td>683.754395</td>\n",
       "      <td>186</td>\n",
       "      <td>194</td>\n",
       "      <td>1427</td>\n",
       "      <td>31</td>\n",
       "      <td>191.379974</td>\n",
       "      <td>164.496170</td>\n",
       "      <td>175.959305</td>\n",
       "      <td>0.511091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>108.17</td>\n",
       "      <td>5.250</td>\n",
       "      <td>1364.18</td>\n",
       "      <td>31.4</td>\n",
       "      <td>20.5</td>\n",
       "      <td>16.6</td>\n",
       "      <td>23457</td>\n",
       "      <td>25678.5</td>\n",
       "      <td>0.913488</td>\n",
       "      <td>671.754395</td>\n",
       "      <td>198</td>\n",
       "      <td>179</td>\n",
       "      <td>1364</td>\n",
       "      <td>31</td>\n",
       "      <td>194.815384</td>\n",
       "      <td>159.870819</td>\n",
       "      <td>122.656914</td>\n",
       "      <td>0.571464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>64.40</td>\n",
       "      <td>3.350</td>\n",
       "      <td>812.24</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>18533</td>\n",
       "      <td>22886.5</td>\n",
       "      <td>0.809779</td>\n",
       "      <td>821.595015</td>\n",
       "      <td>189</td>\n",
       "      <td>168</td>\n",
       "      <td>1376</td>\n",
       "      <td>22</td>\n",
       "      <td>193.579941</td>\n",
       "      <td>133.306320</td>\n",
       "      <td>123.732269</td>\n",
       "      <td>0.725106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LFW_g  LDW_g   LA_mm2  length_mm  width_mm  height_mm  plant_area  \\\n",
       "0      1.30  0.078    31.95        4.3       5.2        5.8        1211   \n",
       "1      2.10  0.148    44.10        5.3       5.7        5.5        1412   \n",
       "2      3.36  0.196    67.61        7.3       6.5        8.9        1303   \n",
       "3      3.07  0.184    66.98        5.8       7.6        7.5        1601   \n",
       "4      3.17  0.187    68.74        7.2       8.0        7.1        1919   \n",
       "..      ...    ...      ...        ...       ...        ...         ...   \n",
       "160   57.96  3.010   726.46       18.5      18.5       13.0       19084   \n",
       "161   81.46  3.880  1001.79       21.0      21.5       14.8       18373   \n",
       "162  140.84  6.380  1707.14       23.0      22.5       16.9       23374   \n",
       "163  108.17  5.250  1364.18       31.4      20.5       16.6       23457   \n",
       "164   64.40  3.350   812.24       21.0      18.0       14.7       18533   \n",
       "\n",
       "     plant_convex_hull_area  plant_solidity  plant_perimeter  plant_width  \\\n",
       "0                    1456.0        0.831731       189.923880           49   \n",
       "1                    1556.5        0.907164       181.338094           56   \n",
       "2                    1766.5        0.737617       247.865005           49   \n",
       "3                    1787.0        0.895915       203.480229           44   \n",
       "4                    2372.0        0.809022       263.421354           62   \n",
       "..                      ...             ...              ...          ...   \n",
       "160                 22974.0        0.830678       767.938160          193   \n",
       "161                 21556.5        0.852318       661.612260          164   \n",
       "162                 26050.0        0.897274       683.754395          186   \n",
       "163                 25678.5        0.913488       671.754395          198   \n",
       "164                 22886.5        0.809779       821.595015          189   \n",
       "\n",
       "     plant_height  plant_longest_path  plant_convex_hull_vertices  \\\n",
       "0              42                 346                          16   \n",
       "1              41                 370                          20   \n",
       "2              54                 333                          19   \n",
       "3              59                 395                          21   \n",
       "4              58                 454                          19   \n",
       "..            ...                 ...                         ...   \n",
       "160           167                1367                          33   \n",
       "161           189                1317                          29   \n",
       "162           194                1427                          31   \n",
       "163           179                1364                          31   \n",
       "164           168                1376                          22   \n",
       "\n",
       "     plant_ellipse_major_axis  plant_ellipse_minor_axis  plant_ellipse_angle  \\\n",
       "0                   45.684494                 40.965988            92.878510   \n",
       "1                   53.368065                 37.484673            96.255425   \n",
       "2                   46.356819                 43.059174             2.102176   \n",
       "3                   55.633369                 38.548523            15.127802   \n",
       "4                   60.012882                 48.875011            55.774792   \n",
       "..                        ...                       ...                  ...   \n",
       "160                190.580887                137.733093            62.138748   \n",
       "161                176.040924                141.141724            22.056726   \n",
       "162                191.379974                164.496170           175.959305   \n",
       "163                194.815384                159.870819           122.656914   \n",
       "164                193.579941                133.306320           123.732269   \n",
       "\n",
       "     plant_ellipse_eccentricity  \n",
       "0                      0.442608  \n",
       "1                      0.711802  \n",
       "2                      0.370421  \n",
       "3                      0.721031  \n",
       "4                      0.580292  \n",
       "..                          ...  \n",
       "160                    0.691160  \n",
       "161                    0.597653  \n",
       "162                    0.511091  \n",
       "163                    0.571464  \n",
       "164                    0.725106  \n",
       "\n",
       "[165 rows x 18 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_filepath = \"./final_harvest_data.csv\"\n",
    "\n",
    "data = pd.read_csv(csv_filepath)\n",
    "DATA = data.drop(['date', 'index', 'plant_id', 'tray_id', 'row', 'column'], axis=1)\n",
    "\n",
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: <torch.utils.data.dataloader.DataLoader object at 0x7faafed172e0>\n",
      " test data: <torch.utils.data.dataloader.DataLoader object at 0x7faafed17610>\n"
     ]
    }
   ],
   "source": [
    "LFW_train, LFW_test = data_workflow(DATA, 'LFW_g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(17, 16),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            \n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits.double()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=16, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = DeepNeuralNetwork()\n",
    "model = model.double()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataset, model, loss_function, optimizer, filler):\n",
    "    model.train()\n",
    "    for (X, y) in dataset:\n",
    "        #X, y = X.to('cuda'), y.to('cuda')\n",
    "        y = y.view(-1,1) \n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prediction = model(X)\n",
    "        loss = loss_function(prediction, y)\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        global LOWEST_LOSS\n",
    "        \n",
    "        print(f\"Loss: {loss}\")\n",
    "        if (loss < LOWEST_LOSS):\n",
    "            LOWEST_LOSS = loss\n",
    "        #print(f\"X: {X} \\n Y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------------\n",
      "Loss: 1385327.6674703239\n",
      "Loss: 720981.4472286827\n",
      "Loss: 518467.7713298213\n",
      "Loss: 662418.5622494407\n",
      "Loss: 366159.35884867405\n",
      "Epoch 2\n",
      "------------------------------------\n",
      "Loss: 90130.26001977903\n",
      "Loss: 51042.68273551899\n",
      "Loss: 17005.960633434\n",
      "Loss: 5697.364153416094\n",
      "Loss: 8536.38425690664\n",
      "Epoch 3\n",
      "------------------------------------\n",
      "Loss: 23065.16531508395\n",
      "Loss: 52657.95272496808\n",
      "Loss: 98541.73887730758\n",
      "Loss: 149364.2619440767\n",
      "Loss: 56667.281774710515\n",
      "Epoch 4\n",
      "------------------------------------\n",
      "Loss: 160750.06584043216\n",
      "Loss: 149736.64447509183\n",
      "Loss: 87493.93567283501\n",
      "Loss: 99964.26953277265\n",
      "Loss: 29075.954927632574\n",
      "Epoch 5\n",
      "------------------------------------\n",
      "Loss: 68198.22409979916\n",
      "Loss: 37988.72854577445\n",
      "Loss: 32187.73419636184\n",
      "Loss: 9486.725563199716\n",
      "Loss: 4084.1734165897124\n",
      "Epoch 6\n",
      "------------------------------------\n",
      "Loss: 1551.7951002323282\n",
      "Loss: 252.33441628959753\n",
      "Loss: 2094.1716617025486\n",
      "Loss: 13646.912143786562\n",
      "Loss: 14774.209408459203\n",
      "Epoch 7\n",
      "------------------------------------\n",
      "Loss: 37959.010870272745\n",
      "Loss: 11636.591640983836\n",
      "Loss: 14330.927619514436\n",
      "Loss: 15527.821977323329\n",
      "Loss: 8895.11932114319\n",
      "Epoch 8\n",
      "------------------------------------\n",
      "Loss: 7756.232088804425\n",
      "Loss: 17343.38486642734\n",
      "Loss: 6190.563529240964\n",
      "Loss: 3873.5096983026765\n",
      "Loss: 39544.818505544354\n",
      "Epoch 9\n",
      "------------------------------------\n",
      "Loss: 4964.116842263028\n",
      "Loss: 748.7936078528545\n",
      "Loss: 1508.3870669458254\n",
      "Loss: 6395.4863181457695\n",
      "Loss: 8452.667252456127\n",
      "Epoch 10\n",
      "------------------------------------\n",
      "Loss: 10979.098945943955\n",
      "Loss: 12594.719537666511\n",
      "Loss: 8331.750089258825\n",
      "Loss: 13512.067113244091\n",
      "Loss: 20218.302797132787\n",
      "Epoch 11\n",
      "------------------------------------\n",
      "Loss: 7477.623106809212\n",
      "Loss: 5182.1113481388675\n",
      "Loss: 3566.300915796185\n",
      "Loss: 1953.9362324979797\n",
      "Loss: 498.9619875210946\n",
      "Epoch 12\n",
      "------------------------------------\n",
      "Loss: 1652.2259955369975\n",
      "Loss: 256.6583126470259\n",
      "Loss: 542.9612132507028\n",
      "Loss: 4216.7048661252065\n",
      "Loss: 1292.9372263040436\n",
      "Epoch 13\n",
      "------------------------------------\n",
      "Loss: 2245.2313177369524\n",
      "Loss: 11887.750044943796\n",
      "Loss: 1898.7220757218786\n",
      "Loss: 2122.3228844356163\n",
      "Loss: 115.81400957742646\n",
      "Epoch 14\n",
      "------------------------------------\n",
      "Loss: 4141.388891038449\n",
      "Loss: 2806.1286352148254\n",
      "Loss: 342.57226576144836\n",
      "Loss: 175.8112095996118\n",
      "Loss: 130.15371471349323\n",
      "Epoch 15\n",
      "------------------------------------\n",
      "Loss: 424.153386111453\n",
      "Loss: 1263.900151915513\n",
      "Loss: 1367.9255781475683\n",
      "Loss: 952.4259510682052\n",
      "Loss: 657.6533158880516\n",
      "Epoch 16\n",
      "------------------------------------\n",
      "Loss: 1403.3951378566596\n",
      "Loss: 1224.4484055418438\n",
      "Loss: 1264.6821902152199\n",
      "Loss: 1005.3859859548257\n",
      "Loss: 1898.7940647097885\n",
      "Epoch 17\n",
      "------------------------------------\n",
      "Loss: 1177.272132404686\n",
      "Loss: 1091.125277801389\n",
      "Loss: 212.3644083857782\n",
      "Loss: 176.97812964056132\n",
      "Loss: 281.7716469345795\n",
      "Epoch 18\n",
      "------------------------------------\n",
      "Loss: 1889.8836911744943\n",
      "Loss: 134.95216147347213\n",
      "Loss: 2000.6238848224677\n",
      "Loss: 184.43880326804495\n",
      "Loss: 231.14288326555706\n",
      "Epoch 19\n",
      "------------------------------------\n",
      "Loss: 139.0117783151955\n",
      "Loss: 211.7827726714165\n",
      "Loss: 1669.9849946574686\n",
      "Loss: 1425.0452152193554\n",
      "Loss: 72.1345963281087\n",
      "Epoch 20\n",
      "------------------------------------\n",
      "Loss: 1248.5715691635703\n",
      "Loss: 392.61974413023233\n",
      "Loss: 464.8344095805343\n",
      "Loss: 856.8102484913466\n",
      "Loss: 127.37791007138361\n",
      "Epoch 21\n",
      "------------------------------------\n",
      "Loss: 594.5886728610132\n",
      "Loss: 519.8902286647644\n",
      "Loss: 1055.5595003375108\n",
      "Loss: 817.3050947805436\n",
      "Loss: 486.0049941609635\n",
      "Epoch 22\n",
      "------------------------------------\n",
      "Loss: 1000.0649682550934\n",
      "Loss: 200.51452651690246\n",
      "Loss: 1087.6737216177924\n",
      "Loss: 304.7020216251725\n",
      "Loss: 275.5737117952535\n",
      "Epoch 23\n",
      "------------------------------------\n",
      "Loss: 261.7708154666004\n",
      "Loss: 198.31180615924615\n",
      "Loss: 1217.3988086119348\n",
      "Loss: 1421.8087726972344\n",
      "Loss: 88.38732862796206\n",
      "Epoch 24\n",
      "------------------------------------\n",
      "Loss: 1383.164148231012\n",
      "Loss: 1137.4663334399206\n",
      "Loss: 229.49217985908658\n",
      "Loss: 196.74774101947608\n",
      "Loss: 268.53148391288926\n",
      "Epoch 25\n",
      "------------------------------------\n",
      "Loss: 293.2965443003375\n",
      "Loss: 273.30786323174516\n",
      "Loss: 1709.0370304140752\n",
      "Loss: 240.16652252942544\n",
      "Loss: 124.41309917299803\n",
      "Epoch 26\n",
      "------------------------------------\n",
      "Loss: 813.9059659611156\n",
      "Loss: 999.49189188409\n",
      "Loss: 381.32083341808413\n",
      "Loss: 305.49741386652613\n",
      "Loss: 216.5915933925237\n",
      "Epoch 27\n",
      "------------------------------------\n",
      "Loss: 1539.9648734548514\n",
      "Loss: 287.90854154475494\n",
      "Loss: 292.29596026427146\n",
      "Loss: 258.4836607365771\n",
      "Loss: 107.28319961190736\n",
      "Epoch 28\n",
      "------------------------------------\n",
      "Loss: 178.95914742014907\n",
      "Loss: 1120.9703978686302\n",
      "Loss: 141.25647160946107\n",
      "Loss: 1004.732066577131\n",
      "Loss: 154.36057413809453\n",
      "Epoch 29\n",
      "------------------------------------\n",
      "Loss: 915.3860699077192\n",
      "Loss: 1010.045315408428\n",
      "Loss: 281.1055698080755\n",
      "Loss: 245.30338638738635\n",
      "Loss: 78.20002117484542\n",
      "Epoch 30\n",
      "------------------------------------\n",
      "Loss: 290.8130609957595\n",
      "Loss: 875.992071322876\n",
      "Loss: 265.07138119208025\n",
      "Loss: 210.26832384597427\n",
      "Loss: 4709.800831454621\n",
      "Epoch 31\n",
      "------------------------------------\n",
      "Loss: 378.54084087240483\n",
      "Loss: 674.0620298182838\n",
      "Loss: 978.8256609946873\n",
      "Loss: 863.0778497215762\n",
      "Loss: 880.9617803744954\n",
      "Epoch 32\n",
      "------------------------------------\n",
      "Loss: 893.1012319394484\n",
      "Loss: 774.256116606029\n",
      "Loss: 379.5073386275034\n",
      "Loss: 610.581696795092\n",
      "Loss: 5191.49704800284\n",
      "Epoch 33\n",
      "------------------------------------\n",
      "Loss: 668.8635453169\n",
      "Loss: 541.0880995089303\n",
      "Loss: 494.3235118759957\n",
      "Loss: 1027.2763030766705\n",
      "Loss: 201.7261181876127\n",
      "Epoch 34\n",
      "------------------------------------\n",
      "Loss: 761.4164963816154\n",
      "Loss: 590.5163996195645\n",
      "Loss: 450.5769887325797\n",
      "Loss: 580.2303871302319\n",
      "Loss: 107.35436882698569\n",
      "Epoch 35\n",
      "------------------------------------\n",
      "Loss: 1157.8162561201143\n",
      "Loss: 180.85214922079956\n",
      "Loss: 212.40307935791063\n",
      "Loss: 115.11862433008147\n",
      "Loss: 54.868478182599034\n",
      "Epoch 36\n",
      "------------------------------------\n",
      "Loss: 893.9492358937637\n",
      "Loss: 722.1333009241403\n",
      "Loss: 184.3782258965428\n",
      "Loss: 218.11692362314307\n",
      "Loss: 81.4275498631178\n",
      "Epoch 37\n",
      "------------------------------------\n",
      "Loss: 593.5303121206362\n",
      "Loss: 692.2511486091266\n",
      "Loss: 259.67575248883185\n",
      "Loss: 193.9053726650639\n",
      "Loss: 164.33900916266327\n",
      "Epoch 38\n",
      "------------------------------------\n",
      "Loss: 410.22418166340736\n",
      "Loss: 270.3027392296224\n",
      "Loss: 693.5064145943963\n",
      "Loss: 178.20649710020285\n",
      "Loss: 270.6090420689351\n",
      "Epoch 39\n",
      "------------------------------------\n",
      "Loss: 553.850734258709\n",
      "Loss: 158.43106136340674\n",
      "Loss: 115.99926774694416\n",
      "Loss: 752.4888732380587\n",
      "Loss: 39.27525120132255\n",
      "Epoch 40\n",
      "------------------------------------\n",
      "Loss: 119.24688340516283\n",
      "Loss: 677.8046418177137\n",
      "Loss: 245.16458917318454\n",
      "Loss: 501.5596668033589\n",
      "Loss: 83.1318551769265\n",
      "Epoch 41\n",
      "------------------------------------\n",
      "Loss: 114.91359824040558\n",
      "Loss: 158.93237517796982\n",
      "Loss: 675.615351731358\n",
      "Loss: 490.7341251388874\n",
      "Loss: 63.17761483373724\n",
      "Epoch 42\n",
      "------------------------------------\n",
      "Loss: 209.52102523469856\n",
      "Loss: 532.1748687901253\n",
      "Loss: 488.1738162376411\n",
      "Loss: 161.9248279978184\n",
      "Loss: 205.68296870815476\n",
      "Epoch 43\n",
      "------------------------------------\n",
      "Loss: 451.00899074551734\n",
      "Loss: 547.681594716707\n",
      "Loss: 131.73421486067804\n",
      "Loss: 201.65649365853022\n",
      "Loss: 109.11323013639603\n",
      "Epoch 44\n",
      "------------------------------------\n",
      "Loss: 99.38521368463572\n",
      "Loss: 531.562411823401\n",
      "Loss: 615.8562130454546\n",
      "Loss: 146.88899726109986\n",
      "Loss: 72.04020302149138\n",
      "Epoch 45\n",
      "------------------------------------\n",
      "Loss: 147.19300338030695\n",
      "Loss: 395.59572506179217\n",
      "Loss: 526.3071636606146\n",
      "Loss: 213.7231880192424\n",
      "Loss: 74.1779085826098\n",
      "Epoch 46\n",
      "------------------------------------\n",
      "Loss: 331.894994193338\n",
      "Loss: 216.94574016886577\n",
      "Loss: 419.4611941066232\n",
      "Loss: 289.09812854593457\n",
      "Loss: 144.61341005292144\n",
      "Epoch 47\n",
      "------------------------------------\n",
      "Loss: 145.18793570603052\n",
      "Loss: 120.37332072995216\n",
      "Loss: 572.7661146236078\n",
      "Loss: 471.31630449351974\n",
      "Loss: 95.42510890715616\n",
      "Epoch 48\n",
      "------------------------------------\n",
      "Loss: 554.6136979651133\n",
      "Loss: 112.76135815046052\n",
      "Loss: 185.41195361540653\n",
      "Loss: 366.54567367966587\n",
      "Loss: 34.712071991988076\n",
      "Epoch 49\n",
      "------------------------------------\n",
      "Loss: 459.92263470019486\n",
      "Loss: 177.1181928828739\n",
      "Loss: 140.32952986081943\n",
      "Loss: 347.44808849017966\n",
      "Loss: 266.9095752992704\n",
      "Epoch 50\n",
      "------------------------------------\n",
      "Loss: 326.20589781804006\n",
      "Loss: 235.2792583330342\n",
      "Loss: 134.98864062111102\n",
      "Loss: 423.3554154247244\n",
      "Loss: 25.94651561568949\n",
      "Epoch 51\n",
      "------------------------------------\n",
      "Loss: 139.16464081245186\n",
      "Loss: 124.02907008060674\n",
      "Loss: 743.0270116397619\n",
      "Loss: 180.42778346318195\n",
      "Loss: 53.386226722307725\n",
      "Epoch 52\n",
      "------------------------------------\n",
      "Loss: 155.75883603291095\n",
      "Loss: 389.0980458137123\n",
      "Loss: 184.94086301419193\n",
      "Loss: 315.45669092687797\n",
      "Loss: 128.29350904281074\n",
      "Epoch 53\n",
      "------------------------------------\n",
      "Loss: 371.89059240795757\n",
      "Loss: 211.46436601564847\n",
      "Loss: 93.62388025274134\n",
      "Loss: 149.00183981401975\n",
      "Loss: 1783.4122766277624\n",
      "Epoch 54\n",
      "------------------------------------\n",
      "Loss: 346.69984477831827\n",
      "Loss: 380.66281267899285\n",
      "Loss: 463.28078531965986\n",
      "Loss: 320.96825299418094\n",
      "Loss: 61.95240012400697\n",
      "Epoch 55\n",
      "------------------------------------\n",
      "Loss: 305.2437786174904\n",
      "Loss: 262.9944149077317\n",
      "Loss: 305.1454824233116\n",
      "Loss: 356.38709854617576\n",
      "Loss: 320.6345029678082\n",
      "Epoch 56\n",
      "------------------------------------\n",
      "Loss: 113.14559516636359\n",
      "Loss: 92.02607020307133\n",
      "Loss: 272.421377379792\n",
      "Loss: 1005.7900049696797\n",
      "Loss: 62.92163981841712\n",
      "Epoch 57\n",
      "------------------------------------\n",
      "Loss: 542.9781559245395\n",
      "Loss: 98.05938009862145\n",
      "Loss: 265.2727363779244\n",
      "Loss: 203.80709165031547\n",
      "Loss: 122.45765777818136\n",
      "Epoch 58\n",
      "------------------------------------\n",
      "Loss: 169.53236884753758\n",
      "Loss: 263.85696698558155\n",
      "Loss: 273.89422583767595\n",
      "Loss: 307.2724150795348\n",
      "Loss: 395.949006429662\n",
      "Epoch 59\n",
      "------------------------------------\n",
      "Loss: 175.98887859335667\n",
      "Loss: 131.74386003924246\n",
      "Loss: 435.3606120246012\n",
      "Loss: 99.74994266917855\n",
      "Loss: 115.37119550219178\n",
      "Epoch 60\n",
      "------------------------------------\n",
      "Loss: 65.59511593789378\n",
      "Loss: 110.73925359570069\n",
      "Loss: 687.6619338052651\n",
      "Loss: 200.49815226253813\n",
      "Loss: 73.89151660387023\n",
      "Epoch 61\n",
      "------------------------------------\n",
      "Loss: 256.0351246907145\n",
      "Loss: 125.64077006481232\n",
      "Loss: 246.63995453806544\n",
      "Loss: 201.0666949387683\n",
      "Loss: 401.32867298590185\n",
      "Epoch 62\n",
      "------------------------------------\n",
      "Loss: 122.48161534484818\n",
      "Loss: 150.9841074785435\n",
      "Loss: 409.35602018649115\n",
      "Loss: 85.44721575902746\n",
      "Loss: 800.8844155828086\n",
      "Epoch 63\n",
      "------------------------------------\n",
      "Loss: 446.55343803537073\n",
      "Loss: 137.91139821959553\n",
      "Loss: 201.45001380689848\n",
      "Loss: 155.2307482588876\n",
      "Loss: 163.76954755965568\n",
      "Epoch 64\n",
      "------------------------------------\n",
      "Loss: 261.5600874931664\n",
      "Loss: 125.53924119059211\n",
      "Loss: 230.40354319773405\n",
      "Loss: 140.44843335843728\n",
      "Loss: 97.06845889170422\n",
      "Epoch 65\n",
      "------------------------------------\n",
      "Loss: 55.74446539164141\n",
      "Loss: 63.061879919699166\n",
      "Loss: 299.8590586386877\n",
      "Loss: 363.0362460770273\n",
      "Loss: 1583.3021919677356\n",
      "Epoch 66\n",
      "------------------------------------\n",
      "Loss: 121.9199801612063\n",
      "Loss: 247.9043090494234\n",
      "Loss: 457.04544872222937\n",
      "Loss: 363.10901947653224\n",
      "Loss: 431.9970846269061\n",
      "Epoch 67\n",
      "------------------------------------\n",
      "Loss: 469.3354038312788\n",
      "Loss: 461.58309818992427\n",
      "Loss: 233.53294251969183\n",
      "Loss: 142.222379531914\n",
      "Loss: 93.42674581518895\n",
      "Epoch 68\n",
      "------------------------------------\n",
      "Loss: 422.6284813476423\n",
      "Loss: 174.08829904420224\n",
      "Loss: 164.29361614930068\n",
      "Loss: 426.74513711546115\n",
      "Loss: 17.083538185932596\n",
      "Epoch 69\n",
      "------------------------------------\n",
      "Loss: 198.8769000182778\n",
      "Loss: 179.66471397002022\n",
      "Loss: 246.06173973267457\n",
      "Loss: 214.7478157049458\n",
      "Loss: 101.1380308780922\n",
      "Epoch 70\n",
      "------------------------------------\n",
      "Loss: 180.48380641284754\n",
      "Loss: 204.917964180529\n",
      "Loss: 185.31135011041644\n",
      "Loss: 187.33437123264258\n",
      "Loss: 85.22426197774321\n",
      "Epoch 71\n",
      "------------------------------------\n",
      "Loss: 214.12755808105774\n",
      "Loss: 190.29034455772114\n",
      "Loss: 201.90989490110758\n",
      "Loss: 101.72167462046596\n",
      "Loss: 14.346633811138895\n",
      "Epoch 72\n",
      "------------------------------------\n",
      "Loss: 140.52435457022784\n",
      "Loss: 229.34391244830923\n",
      "Loss: 106.2810912319312\n",
      "Loss: 77.57456201685643\n",
      "Loss: 30.13405190307037\n",
      "Epoch 73\n",
      "------------------------------------\n",
      "Loss: 59.346863133807766\n",
      "Loss: 323.65197134705187\n",
      "Loss: 52.82584204879639\n",
      "Loss: 166.51872962537462\n",
      "Loss: 56.676689031141635\n",
      "Epoch 74\n",
      "------------------------------------\n",
      "Loss: 145.54010503053914\n",
      "Loss: 110.39658850428937\n",
      "Loss: 127.7580999352744\n",
      "Loss: 159.55426011931138\n",
      "Loss: 98.65201559268783\n",
      "Epoch 75\n",
      "------------------------------------\n",
      "Loss: 163.23138687752322\n",
      "Loss: 92.28767095733227\n",
      "Loss: 120.16160790720721\n",
      "Loss: 145.60105900939192\n",
      "Loss: 92.64465913349316\n",
      "Epoch 76\n",
      "------------------------------------\n",
      "Loss: 180.94600113977586\n",
      "Loss: 215.95440898883237\n",
      "Loss: 62.556343331270014\n",
      "Loss: 58.15960750445794\n",
      "Loss: 124.0534903759823\n",
      "Epoch 77\n",
      "------------------------------------\n",
      "Loss: 70.64427185654851\n",
      "Loss: 133.82901552163185\n",
      "Loss: 169.86055707252495\n",
      "Loss: 129.50112725905672\n",
      "Loss: 37.521639581516695\n",
      "Epoch 78\n",
      "------------------------------------\n",
      "Loss: 66.58977642694177\n",
      "Loss: 141.19500674749338\n",
      "Loss: 120.46726866114646\n",
      "Loss: 154.99036426066675\n",
      "Loss: 33.35155147963898\n",
      "Epoch 79\n",
      "------------------------------------\n",
      "Loss: 118.78152764512933\n",
      "Loss: 50.86243679395409\n",
      "Loss: 82.84127686035404\n",
      "Loss: 159.459208578453\n",
      "Loss: 671.0946393257248\n",
      "Epoch 80\n",
      "------------------------------------\n",
      "Loss: 96.9109376027079\n",
      "Loss: 113.31171900126901\n",
      "Loss: 126.04848682803043\n",
      "Loss: 170.41928203586897\n",
      "Loss: 487.32580297229816\n",
      "Epoch 81\n",
      "------------------------------------\n",
      "Loss: 120.82704331597387\n",
      "Loss: 122.41728575912109\n",
      "Loss: 79.92358305499246\n",
      "Loss: 178.28659965541274\n",
      "Loss: 35.027755585809814\n",
      "Epoch 82\n",
      "------------------------------------\n",
      "Loss: 253.63075858660167\n",
      "Loss: 56.93835775154653\n",
      "Loss: 113.15353877030886\n",
      "Loss: 165.8552264291686\n",
      "Loss: 33.01518145799646\n",
      "Epoch 83\n",
      "------------------------------------\n",
      "Loss: 120.57848821259745\n",
      "Loss: 157.8723589510196\n",
      "Loss: 83.46842444871818\n",
      "Loss: 114.17307094138181\n",
      "Loss: 44.10745251207803\n",
      "Epoch 84\n",
      "------------------------------------\n",
      "Loss: 202.24842014581284\n",
      "Loss: 70.39034669028763\n",
      "Loss: 107.47134098137627\n",
      "Loss: 68.20723492336911\n",
      "Loss: 173.03058936257165\n",
      "Epoch 85\n",
      "------------------------------------\n",
      "Loss: 88.36375240716583\n",
      "Loss: 107.11508456365311\n",
      "Loss: 72.04069732817771\n",
      "Loss: 172.81884339020004\n",
      "Loss: 56.92055814817025\n",
      "Epoch 86\n",
      "------------------------------------\n",
      "Loss: 132.29779355964098\n",
      "Loss: 107.68341717295053\n",
      "Loss: 83.18635218379214\n",
      "Loss: 104.0118600876803\n",
      "Loss: 201.58281936713627\n",
      "Epoch 87\n",
      "------------------------------------\n",
      "Loss: 80.59845837549108\n",
      "Loss: 190.09582728739557\n",
      "Loss: 88.23010022606982\n",
      "Loss: 63.39191963785547\n",
      "Loss: 109.58645871854651\n",
      "Epoch 88\n",
      "------------------------------------\n",
      "Loss: 120.28141624426632\n",
      "Loss: 124.87576412153118\n",
      "Loss: 37.53444766783367\n",
      "Loss: 131.85353389361057\n",
      "Loss: 42.20515140248071\n",
      "Epoch 89\n",
      "------------------------------------\n",
      "Loss: 108.90225365548072\n",
      "Loss: 45.51121195630825\n",
      "Loss: 79.49850230900701\n",
      "Loss: 175.10866215395993\n",
      "Loss: 108.21736383557948\n",
      "Epoch 90\n",
      "------------------------------------\n",
      "Loss: 129.3698550255444\n",
      "Loss: 146.9638775423692\n",
      "Loss: 42.57869651643793\n",
      "Loss: 100.73907371524226\n",
      "Loss: 34.040662011971655\n",
      "Epoch 91\n",
      "------------------------------------\n",
      "Loss: 115.81187346247064\n",
      "Loss: 110.53670791815972\n",
      "Loss: 112.90355028398355\n",
      "Loss: 103.57316672698437\n",
      "Loss: 45.22538514327488\n",
      "Epoch 92\n",
      "------------------------------------\n",
      "Loss: 76.06752199346843\n",
      "Loss: 49.99869510027564\n",
      "Loss: 131.45513409728963\n",
      "Loss: 186.1227648247857\n",
      "Loss: 23.94311339279161\n",
      "Epoch 93\n",
      "------------------------------------\n",
      "Loss: 85.95595925672168\n",
      "Loss: 92.37560297366241\n",
      "Loss: 88.55507022607374\n",
      "Loss: 120.44976842930427\n",
      "Loss: 33.00209350876971\n",
      "Epoch 94\n",
      "------------------------------------\n",
      "Loss: 127.16797095485072\n",
      "Loss: 74.38336966427681\n",
      "Loss: 118.83984307851819\n",
      "Loss: 64.88225343518535\n",
      "Loss: 41.231676677292874\n",
      "Epoch 95\n",
      "------------------------------------\n",
      "Loss: 85.1999281983986\n",
      "Loss: 67.65674880560951\n",
      "Loss: 116.74092485708725\n",
      "Loss: 91.64352947070012\n",
      "Loss: 272.3757069322609\n",
      "Epoch 96\n",
      "------------------------------------\n",
      "Loss: 86.76971811794444\n",
      "Loss: 104.09030603776435\n",
      "Loss: 101.99335427864945\n",
      "Loss: 68.40546776513744\n",
      "Loss: 377.805613306811\n",
      "Epoch 97\n",
      "------------------------------------\n",
      "Loss: 144.88509513664536\n",
      "Loss: 101.65886674048272\n",
      "Loss: 34.87602485840557\n",
      "Loss: 147.48075076849506\n",
      "Loss: 5.8166466460757436\n",
      "Epoch 98\n",
      "------------------------------------\n",
      "Loss: 203.58603646520928\n",
      "Loss: 26.849723853097437\n",
      "Loss: 125.22604403695725\n",
      "Loss: 77.42298147299144\n",
      "Loss: 97.55787246775506\n",
      "Epoch 99\n",
      "------------------------------------\n",
      "Loss: 87.67169084436732\n",
      "Loss: 68.8946417465695\n",
      "Loss: 143.83559526631194\n",
      "Loss: 68.55852055354038\n",
      "Loss: 25.837950213093727\n",
      "Epoch 100\n",
      "------------------------------------\n",
      "Loss: 62.47322094602447\n",
      "Loss: 176.2924159708362\n",
      "Loss: 62.39089469880565\n",
      "Loss: 56.78954264317247\n",
      "Loss: 40.04855091632493\n",
      "Epoch 101\n",
      "------------------------------------\n",
      "Loss: 87.75845868979994\n",
      "Loss: 128.52073270096957\n",
      "Loss: 42.34817357048655\n",
      "Loss: 101.37703479565117\n",
      "Loss: 46.786298856726674\n",
      "Epoch 102\n",
      "------------------------------------\n",
      "Loss: 95.16180078985917\n",
      "Loss: 56.18573642661522\n",
      "Loss: 104.75520710905225\n",
      "Loss: 117.439993903533\n",
      "Loss: 48.056499449456595\n",
      "Epoch 103\n",
      "------------------------------------\n",
      "Loss: 166.88628028336305\n",
      "Loss: 91.51768732362319\n",
      "Loss: 44.77187964937006\n",
      "Loss: 75.61693992759106\n",
      "Loss: 7.405000325486338\n",
      "Epoch 104\n",
      "------------------------------------\n",
      "Loss: 89.9350563839956\n",
      "Loss: 47.996748780395485\n",
      "Loss: 147.43461211533557\n",
      "Loss: 57.24537580112827\n",
      "Loss: 115.0526565142133\n",
      "Epoch 105\n",
      "------------------------------------\n",
      "Loss: 89.52875884458241\n",
      "Loss: 120.20680018079766\n",
      "Loss: 47.71081892612234\n",
      "Loss: 82.8559344403453\n",
      "Loss: 101.19930631567956\n",
      "Epoch 106\n",
      "------------------------------------\n",
      "Loss: 92.3951262524524\n",
      "Loss: 100.3279341890519\n",
      "Loss: 103.8694771913678\n",
      "Loss: 51.38751085892514\n",
      "Loss: 27.022940950318475\n",
      "Epoch 107\n",
      "------------------------------------\n",
      "Loss: 109.12718098238467\n",
      "Loss: 95.18959794208148\n",
      "Loss: 97.44150920011127\n",
      "Loss: 50.059933651394346\n",
      "Loss: 24.116040880166267\n",
      "Epoch 108\n",
      "------------------------------------\n",
      "Loss: 136.35770793420573\n",
      "Loss: 78.62185488804168\n",
      "Loss: 91.58295070888656\n",
      "Loss: 36.817699796808974\n",
      "Loss: 50.0642492256485\n",
      "Epoch 109\n",
      "------------------------------------\n",
      "Loss: 224.83268286217304\n",
      "Loss: 50.543102355558176\n",
      "Loss: 31.92477492020958\n",
      "Loss: 31.64629459807552\n",
      "Loss: 75.72428208011468\n",
      "Epoch 110\n",
      "------------------------------------\n",
      "Loss: 139.55657409143595\n",
      "Loss: 38.84337648107558\n",
      "Loss: 96.99636556119165\n",
      "Loss: 62.94496856058747\n",
      "Loss: 156.0939796619327\n",
      "Epoch 111\n",
      "------------------------------------\n",
      "Loss: 145.46178936562598\n",
      "Loss: 36.107284899039655\n",
      "Loss: 104.19907203372848\n",
      "Loss: 49.708296898313506\n",
      "Loss: 24.130401411061182\n",
      "Epoch 112\n",
      "------------------------------------\n",
      "Loss: 55.42536755271671\n",
      "Loss: 70.94034250718924\n",
      "Loss: 79.23262074316783\n",
      "Loss: 113.64879679180115\n",
      "Loss: 344.0799539806242\n",
      "Epoch 113\n",
      "------------------------------------\n",
      "Loss: 62.05842067527318\n",
      "Loss: 91.16688720568568\n",
      "Loss: 106.75557813285553\n",
      "Loss: 87.17172075168071\n",
      "Loss: 43.65951675173962\n",
      "Epoch 114\n",
      "------------------------------------\n",
      "Loss: 102.06095357150994\n",
      "Loss: 87.53677911935084\n",
      "Loss: 90.25289842042501\n",
      "Loss: 92.98023215655974\n",
      "Loss: 67.16615895851774\n",
      "Epoch 115\n",
      "------------------------------------\n",
      "Loss: 50.748000276049325\n",
      "Loss: 47.05982728891144\n",
      "Loss: 195.56188119735904\n",
      "Loss: 100.49814129750524\n",
      "Loss: 38.47027913991756\n",
      "Epoch 116\n",
      "------------------------------------\n",
      "Loss: 47.805850267694304\n",
      "Loss: 107.29072167735458\n",
      "Loss: 108.96136016310848\n",
      "Loss: 91.49617469602426\n",
      "Loss: 34.698725682778054\n",
      "Epoch 117\n",
      "------------------------------------\n",
      "Loss: 116.67619922309659\n",
      "Loss: 44.25431012794226\n",
      "Loss: 98.99163609477525\n",
      "Loss: 101.26801380050243\n",
      "Loss: 42.72815465839771\n",
      "Epoch 118\n",
      "------------------------------------\n",
      "Loss: 60.35091310581051\n",
      "Loss: 85.10888876183726\n",
      "Loss: 73.53085019570916\n",
      "Loss: 132.02795938361754\n",
      "Loss: 36.614750193659994\n",
      "Epoch 119\n",
      "------------------------------------\n",
      "Loss: 100.34827723465298\n",
      "Loss: 85.67717491821766\n",
      "Loss: 88.47808823534716\n",
      "Loss: 63.04676609754744\n",
      "Loss: 52.60112305331589\n",
      "Epoch 120\n",
      "------------------------------------\n",
      "Loss: 123.52533607094183\n",
      "Loss: 96.38829549660312\n",
      "Loss: 97.17334076090437\n",
      "Loss: 105.48657296694272\n",
      "Loss: 7.685305941986473\n",
      "Epoch 121\n",
      "------------------------------------\n",
      "Loss: 66.27363196416445\n",
      "Loss: 114.37399291505005\n",
      "Loss: 140.68214678256643\n",
      "Loss: 27.119128386743494\n",
      "Loss: 3.083124053801548\n",
      "Epoch 122\n",
      "------------------------------------\n",
      "Loss: 97.1456210775616\n",
      "Loss: 114.48830354699056\n",
      "Loss: 76.61790362162758\n",
      "Loss: 47.3285812344194\n",
      "Loss: 80.10620436438549\n",
      "Epoch 123\n",
      "------------------------------------\n",
      "Loss: 58.701332951917216\n",
      "Loss: 62.155835857395154\n",
      "Loss: 68.3574674962731\n",
      "Loss: 175.54589809008866\n",
      "Loss: 12.262005615818822\n",
      "Epoch 124\n",
      "------------------------------------\n",
      "Loss: 92.22051656766058\n",
      "Loss: 46.0952002729673\n",
      "Loss: 110.29496713706669\n",
      "Loss: 79.78534159026208\n",
      "Loss: 24.214635852108554\n",
      "Epoch 125\n",
      "------------------------------------\n",
      "Loss: 48.472335140261386\n",
      "Loss: 50.397108553492714\n",
      "Loss: 174.16493724174586\n",
      "Loss: 57.09848967129618\n",
      "Loss: 19.795439564568078\n",
      "Epoch 126\n",
      "------------------------------------\n",
      "Loss: 94.62529322769802\n",
      "Loss: 34.385927258122216\n",
      "Loss: 95.75535788489722\n",
      "Loss: 44.8105103360978\n",
      "Loss: 501.03678927272733\n",
      "Epoch 127\n",
      "------------------------------------\n",
      "Loss: 78.03887461783106\n",
      "Loss: 55.90666321234457\n",
      "Loss: 119.00458734843895\n",
      "Loss: 97.40791727003673\n",
      "Loss: 257.80877553555484\n",
      "Epoch 128\n",
      "------------------------------------\n",
      "Loss: 97.59814214580892\n",
      "Loss: 85.08338932025845\n",
      "Loss: 79.56902070769213\n",
      "Loss: 148.28597954095213\n",
      "Loss: 49.64558142558208\n",
      "Epoch 129\n",
      "------------------------------------\n",
      "Loss: 129.02825588619626\n",
      "Loss: 111.86252667784979\n",
      "Loss: 61.990032722126855\n",
      "Loss: 64.5109020408634\n",
      "Loss: 25.4309114694287\n",
      "Epoch 130\n",
      "------------------------------------\n",
      "Loss: 120.59394768574046\n",
      "Loss: 101.87006895058266\n",
      "Loss: 64.88631802058609\n",
      "Loss: 87.4542968008399\n",
      "Loss: 23.273558337819388\n",
      "Epoch 131\n",
      "------------------------------------\n",
      "Loss: 52.5891348897802\n",
      "Loss: 157.6659611578449\n",
      "Loss: 73.69567032046365\n",
      "Loss: 118.14185926541207\n",
      "Loss: 41.3637437000188\n",
      "Epoch 132\n",
      "------------------------------------\n",
      "Loss: 67.50057847534333\n",
      "Loss: 59.7892165362627\n",
      "Loss: 104.967759075597\n",
      "Loss: 75.85150750506294\n",
      "Loss: 40.35781126942473\n",
      "Epoch 133\n",
      "------------------------------------\n",
      "Loss: 78.82394468925145\n",
      "Loss: 78.06446780100823\n",
      "Loss: 124.05010894336186\n",
      "Loss: 51.758542382509745\n",
      "Loss: 65.16090505818022\n",
      "Epoch 134\n",
      "------------------------------------\n",
      "Loss: 53.477273518384436\n",
      "Loss: 84.17270347281801\n",
      "Loss: 50.28637919267498\n",
      "Loss: 137.06587388652244\n",
      "Loss: 226.05114795074883\n",
      "Epoch 135\n",
      "------------------------------------\n",
      "Loss: 55.72693222682389\n",
      "Loss: 90.87813791133358\n",
      "Loss: 78.22905270010385\n",
      "Loss: 146.01833277409673\n",
      "Loss: 53.36963016331308\n",
      "Epoch 136\n",
      "------------------------------------\n",
      "Loss: 76.51661960785925\n",
      "Loss: 67.29313498943839\n",
      "Loss: 136.7371283263358\n",
      "Loss: 81.50501096482786\n",
      "Loss: 48.87878189359413\n",
      "Epoch 137\n",
      "------------------------------------\n",
      "Loss: 55.29888782554658\n",
      "Loss: 135.62491243171326\n",
      "Loss: 62.825351676563116\n",
      "Loss: 89.04490036208433\n",
      "Loss: 304.70525893142064\n",
      "Epoch 138\n",
      "------------------------------------\n",
      "Loss: 76.44285317106153\n",
      "Loss: 171.1952394843294\n",
      "Loss: 100.81295809957746\n",
      "Loss: 72.11137227304495\n",
      "Loss: 237.34539124954514\n",
      "Epoch 139\n",
      "------------------------------------\n",
      "Loss: 85.19258647000208\n",
      "Loss: 68.8488178398611\n",
      "Loss: 68.23631991269802\n",
      "Loss: 94.75084614477547\n",
      "Loss: 235.30796844605965\n",
      "Epoch 140\n",
      "------------------------------------\n",
      "Loss: 63.60817230042973\n",
      "Loss: 101.68527271685227\n",
      "Loss: 49.932838730736115\n",
      "Loss: 80.81431566133351\n",
      "Loss: 185.17074949656313\n",
      "Epoch 141\n",
      "------------------------------------\n",
      "Loss: 52.629435130496844\n",
      "Loss: 106.44254275129464\n",
      "Loss: 64.33135580093013\n",
      "Loss: 105.82732011311018\n",
      "Loss: 45.082189334151934\n",
      "Epoch 142\n",
      "------------------------------------\n",
      "Loss: 43.7452315555755\n",
      "Loss: 60.70133674674121\n",
      "Loss: 155.66568819488242\n",
      "Loss: 94.51111609276103\n",
      "Loss: 6.167264972694058\n",
      "Epoch 143\n",
      "------------------------------------\n",
      "Loss: 92.65331985085841\n",
      "Loss: 82.91978203812889\n",
      "Loss: 45.41912481129472\n",
      "Loss: 73.60394667040268\n",
      "Loss: 265.6382482216616\n",
      "Epoch 144\n",
      "------------------------------------\n",
      "Loss: 63.99651616798222\n",
      "Loss: 82.61702886167065\n",
      "Loss: 57.442255029613804\n",
      "Loss: 120.14488971992628\n",
      "Loss: 75.4307213940833\n",
      "Epoch 145\n",
      "------------------------------------\n",
      "Loss: 83.54805438633278\n",
      "Loss: 140.36564573074577\n",
      "Loss: 31.404686820109426\n",
      "Loss: 61.78033360163065\n",
      "Loss: 15.990437158779514\n",
      "Epoch 146\n",
      "------------------------------------\n",
      "Loss: 61.63339303825971\n",
      "Loss: 99.22569483104729\n",
      "Loss: 113.96622571080641\n",
      "Loss: 49.67895403123016\n",
      "Loss: 10.343141130003435\n",
      "Epoch 147\n",
      "------------------------------------\n",
      "Loss: 47.631487373300146\n",
      "Loss: 135.0802252709282\n",
      "Loss: 87.72680314300086\n",
      "Loss: 35.085741016044224\n",
      "Loss: 56.54313942519056\n",
      "Epoch 148\n",
      "------------------------------------\n",
      "Loss: 81.69527178373696\n",
      "Loss: 112.40673118630474\n",
      "Loss: 68.27291293199667\n",
      "Loss: 57.53525457115546\n",
      "Loss: 24.58542237139438\n",
      "Epoch 149\n",
      "------------------------------------\n",
      "Loss: 61.17184727215777\n",
      "Loss: 104.13233318763304\n",
      "Loss: 69.8402187216893\n",
      "Loss: 67.2332568382294\n",
      "Loss: 17.789304441300146\n",
      "Epoch 150\n",
      "------------------------------------\n",
      "Loss: 52.85813927057579\n",
      "Loss: 93.42698930462288\n",
      "Loss: 88.42642075773719\n",
      "Loss: 65.19722011822853\n",
      "Loss: 36.35409827934789\n",
      "Training complete!\n",
      "tensor(3.0831, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n------------------------------------\")\n",
    "    train_loop(LFW_train, model, LOSS_FUNCTION, optimizer, LOWEST_LOSS)\n",
    "    #test_loop(LFW_test, model, LOSS_FUNCTION)\n",
    "print(\"Training complete!\")\n",
    "print(LOWEST_LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05dbbb7414389032baa654308b5b2368ed4754b5c0531661864b7939633c6eac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
